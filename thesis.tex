%% License 
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %% This is licensed under the terms of the MIT license below.                 %%
  %%                                                                            %%
  %% Written by Luis R.J. Costa.                                                %%
  %% Currently developed at the Learning Services of Aalto University School of %%
  %% Electrical Engineering by Luis R.J. Costa since May 2017.                  %%
  %%                                                                            %%
  %% Copyright 2017-2018, by Luis R.J. Costa, luis.costa@aalto.fi,              %%
  %% Copyright 2017-2018 Swedish translations in aaltothesis.cls by Elisabeth   %%
  %% Nyberg, elisabeth.nyberg@aalto.fi and Henrik Wallén,                       %%
  %% henrik.wallen@aalto.fi.                                                    %%
  %% Copyright 2017-2018 Finnish documentation in the template opinnatepohja.tex%%
  %% by Perttu Puska, perttu.puska@aalto.fi, and Luis R.J. Costa.               %%
  %% Copyright 2018 English template thesistemplate.tex by Luis R.J. Costa.     %%
  %% Copyright 2018 Swedish template kandidatarbetsbotten.tex by Henrik Wallen. %%
  %%                                                                            %%
  %% Permission is hereby granted, free of charge, to any person obtaining a    %%
  %% copy of this software and associated documentation files (the "Software"), %%
  %% to deal in the Software without restriction, including without limitation  %%
  %% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
  %% and/or sell copies of the Software, and to permit persons to whom the      %%
  %% Software is furnished to do so, subject to the following conditions:       %%
  %% The above copyright notice and this permission notice shall be included in %%
  %% all copies or substantial portions of the Software.                        %%
  %% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
  %% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
  %% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
  %% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
  %% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
  %% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
  %% DEALINGS IN THE SOFTWARE.                                                  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%

%% spellcheck-on

%% Configurations, meta information, prefaces etc. 
  \documentclass[english, 12pt, a4paper, elec, utf8, a-1b, hidelinks]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, dvips, online]{aaltothesis}

  %% Use the following options in the \documentclass macro above:
  %% your school: arts, biz, chem, elec, eng, sci
  %% the character encoding scheme used by your editor: utf8, latin1
  %% thesis language: english, finnish, swedish
  %% make an archiveable PDF/A-1b or PDF/A-2b compliant file: a-1b, a-2b
  %%                    (with pdflatex, a normal pdf containing metadata is
  %%                     produced without the a-*b option)
  %% typeset in symmetric layout and blue hypertext for online publication: online
  %%            (no option is the default, resulting in a wide margin on the
  %%             binding side of the page and black hypertext)
  %% two-sided printing: twoside (default is one-sided printing)

  \usepackage{amsfonts, amssymb, amsbsy, amsmath, natbib, graphicx, 
              tabstackengine, tabularx}
  \usepackage{titlesec}
  \setcounter{secnumdepth}{4}
  \titleformat{\paragraph}
  {\normalfont\small\bfseries}{\theparagraph}{1em}{}
  \titlespacing*{\paragraph}
  {0pt}{3.25ex plus 1ex minus .2ex}{0ex plus .2ex}

  \graphicspath{ {./images/} }
  

  \DeclareMathOperator*{\argmax}{argmax}

  % \university{Aalto-yliopisto}
  % \school{Sähkötekniikan korkeakoulu}
  \degreeprogram{Computer, Communication and Information Sciences}
  \major{Signal, Speech and Language Processing}
  \code{ELEC3031}
  \univdegree{MSc}
  \thesisauthor{Anssi Moisio}

  %% A possible "and" in the title should not be the last word in the line, it
  %% begins the next line.
  %% Specify the title again without the linebreak characters in the optional
  %% argument in box brackets. This is done because the title is part of the 
  %% metadata in the pdf/a file, and the metadata cannot contain linebreaks.
  \thesistitle{Automatic recognition of conversational speech} % working title
  %\thesistitle[Title of the thesis]{Title of\\ the thesis}
  \place{Espoo}
  \date{}

  %% Thesis supervisor
  %% Note the "\" character in the title after the period and before the space
  %% and the following character string.
  %% This is because the period is not the end of a sentence after which a
  %% slightly longer space follows, but what is desired is a regular interword
  %% space.
  \supervisor{Prof.\ Mikko Kurimo}

  %% Advisor(s)---two at the most---of the thesis.
  \advisor{Dr.}

  %% Aaltologo: syntax:
  %% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
  %% The logo language is set to be the same as the thesis language.
  \uselogo{aaltoRed}{''}

  %% The English abstract:
  %% All the details (name, title, etc.) on the abstract page appear as specified
  %% above.
  %% Thesis keywords:
  %% Note! The keywords are separated using the \spc macro
  \keywords{For keywords choose\spc concepts that are\spc central to your\spc thesis}

  %% The abstract text. This text is included in the metadata of the pdf file as well
  %% as the abstract page.
  \thesisabstract{abstract here
  }

  %% Copyright text. Copyright of a work is with the creator/author of the work
  %% regardless of whether the copyright mark is explicitly in the work or not.
  %% You may, if you wish, publish your work under a Creative Commons license (see
  %% creaticecommons.org), in which case the license text must be visible in the
  %% work. Write here the copyright text you want. It is written into the metadata
  %% of the pdf file as well.
  %% Syntax:
  %% \copyrigthtext{metadata text}{text visible on the page}
  %% 
  %% In the macro below, the text written in the metadata must have a \noexpand
  %% macro before the \copyright special character, and macros (\copyright and
  %% \year here) must be separated by the \ character (space chacter) from the
  %% text that follows. The macros in the argument of the \copyrighttext macro
  %% automatically insert the year and the author's name. (Note! \ThesisAuthor is
  %% an internal macro of the aaltothesis.cls class file).
  %% Of course, the same text could have simply been written as
  %% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
  %% {Copyright \copyright{} 2018 Eddie Engineer}
  \copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
  {Copyright \copyright{} \number\year{} \ThesisAuthor}

  %% You can prevent LaTeX from writing into the xmpdata file (it contains all the 
  %% metadata to be written into the pdf file) by setting the writexmpdata switch
  %% to 'false'. This allows you to write the metadata in the correct format
  %% directly into the file thesistemplate.xmpdata.
  %\setboolean{writexmpdatafile}{false}

  %% All that is printed on paper starts here
  \begin{document}

  \pagenumbering{roman}

  %% Create the coverpage
  % \makecoverpage

  %% Typeset the copyright text.
  %% If you wish, you may leave out the copyright text from the human-readable
  %% page of the pdf file. This may seem like a attractive idea for the printed
  %% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
  %% on the page. However, the recommendation is to print this copyright text.
  % \makecopyrightpage

  %% Note that when writting your thesis in English, place the English abstract
  %% first followed by the possible Finnish or Swedish abstract.

  %% Abstract text
  %% The text in the \thesisabstract macro is stored in the macro \abstractext, so
  %% you can use the text metadata abstract directly as follows:
  % \begin{abstractpage}[english]
  %   \abstracttext{}
  % \end{abstractpage}

  % \newpage
  % %% Abstract in Finnish.
  % \thesistitle{Opinnäyteen otsikko}
  % \supervisor{Prof.}
  % \advisor{TkT}
  % \degreeprogram{Tieto-, tietoliikenne- ja informaatiotekniikka}
  % \major{Signaalin-, puheen- ja kielenkäsittely}
  % %% The keywords need not be separated by \spc now.
  % \keywords{}
  % %% Abstract text
  % \begin{abstractpage}[finnish]
  %   tiivistelmä tähän
  % \end{abstractpage}

  % %% Preface
  % %% This section is optional. Remove it if you do not want a preface.
  % \mysection{Preface}
  % preface here

  % \vspace{5cm}
  % Otaniemi, 31.8.2018

  % % \vspace{5mm}
  % % {\hfill Eddie E.\ A.\ Engineer \hspace{1cm}}

  % \newpage
  \thesistableofcontents
%%

%% Symbols and abbreviations 
  \mysection{Symbols and abbreviations}

  \subsection*{Symbols}

  \begin{tabular}{ll}
  % examples
  % $\mathbf{B}$  & magnetic flux density  \\
  % $c$              & speed of light in vacuum $\approx 3\times10^8$ [m/s]\\
  % $\omega_{\mathrm{D}}$    & Debye frequency \\
  % $\omega_{\mathrm{latt}}$ & average phonon frequency of lattice \\
  % $\uparrow$       & electron spin direction up\\
  % $\downarrow$     & electron spin direction down \\
  % /examples
  $\boldsymbol{\Delta}$                       & delta feature vector \\
  $\boldsymbol{\mu}$                          & mean vector of a GMM \\
  $\boldsymbol{\Sigma}$                       & covariance matrix of a GMM \\


  $\boldsymbol{o}_t$    & observation, i.e., feature vector, at time $t$ \\ 
  $p(.)$              & probability \\
  $P(.|.)$            & likelihood \\
  
  \end{tabular}

  % \subsection*{Operators}

  % \begin{tabular}{ll}
  % examples
  % $\nabla \times \mathbf{A}$                  & curl of vectorin $\mathbf{A}$\\
  % $\displaystyle\frac{\mbox{d}}{\mbox{d} t}$  & derivative with respect to 
  %                                               variable $t$\\[3mm]
  % $\displaystyle\frac{\partial}{\partial t}$  & partial derivative with respect 
  %                                               to variable $t$ \\[3mm]
  % $\sum_i $                                   & sum over index $i$\\
  % $\mathbf{A} \cdot \mathbf{B}$               & dot product of vectors $\mathbf{A}$ and 
  %                                               $\mathbf{B}$ \\
  % /examples

  % \end{tabular}

  \subsection*{Abbreviations}

  \begin{tabular}{ll}
  AM            & acoustic model \\
  ASR           & automatic speech recognition \\
  CER           & character error rate \\
  CMN           & cepstral mean normalisation \\
  CMVN          & cepstral mean and variance normalisation \\
  DNN           & deep neural network \\
  EM            & expectation maximisation \\
  fMLLR         & feature space maximum likelihood linear regression \\
  FSA           & finite-state acceptor \\
  FST           & finite-state transducer \\
  GMM           & Gaussian mixture model \\
  GRU           & gated recurrent unit \\
  HMM           & hidden Markov model \\
  LDA           & latent Dirichlet allocation \\
  LDA           & linear discriminant analysis \\
  LM            & language model \\
  LSTM          & long short-term memory \\
  MAP           & maximum a posteriori \\
  MCE           & minimum classification error \\
  MFCC          & mel-frequency cepstral coefficient \\
  ML            & maximum likelihood \\
  MLE           & maximum likelihood estimation \\
  MLLR          & maximum likelihood linear regression \\
  MLLT          & maximum likelihood linear transformation \\
  MMI           & maximum mutual information \\
  MPE           & minimum phone error \\
  NN            & neural network \\
  NNLM          & neural network language model \\
  OOV           & out of vocabulary \\
  PDF           & probability density function \\
  PPL           & perplexity \\
  ReLU          & rectified linear unit \\
  RNN           & recurrent neural network \\
  SAT           & speaker adaptive training \\
  SD            & speaker dependent \\
  SI            & speaker independent \\
  TDNN          & time delay neural network \\
  VAD           & voice activity detection, used synonymously with speech activity detection \\
  WER           & word error rate \\
  WFST          & weighted finite-state transducer \\
  VTLN          & vocal tract length normalisation \\
  \end{tabular}

  %% \clearpage is similar to \newpage, but it also flushes the floats (figures
  %% and tables).
  \cleardoublepage
  \pagenumbering{arabic}
  \setcounter{page}{1}
%%

%% Text body begins.
\section{Introduction} \label{sec:introduction}
\thispagestyle{empty} 

\subsection{The ASR task} 

  Automatic speech recognition (ASR) is the task of converting speech into text.
  The difficulty of the task depends on how varied the speech audio signals are. The restricted problem of recognising a few different words pronounced clearly by one speaker recorded in noise-free conditions was solved years ago. Speech recognition becomes more difficult when the speech is continuous and recorded in differing noise conditions from many speakers. Current state-of-the-art ASR systems are nearing the human-level recognition accuracy also in continuous speech recognition tasks if the speech is planned and pronounced clearly, as it is, for example, in broadcast news or parliament discussions. However, spontaneous, informal conversations remain a challenging type of speech to transcribe automatically, and the gap between human and machine accuracy is still very large. This thesis explores methods for improving ASR for conversational Finnish.

\subsection{The basic structure of an ASR system} 

  The conventional ASR system includes two main component systems: the acoustic model (AM) and the language model (LM). The LM generates an \emph{a priori} probability distribution over possible word sequences. For example, the transciption "en minä tiedä" should probably be assigned a larger probability than "en sinä tiedä" even before any speech audio is processed. The AM outputs \emph{a posteriori} probabilities for phoneme sequences based on the speech audio. A dictionary is used to map the phoneme sequences to words, or more accurately, the same grapheme units that the LM uses, which can also be subword units or characters, for instance. The probabilities of the LM and AM are combined to estimate the most likely transcription of the speech audio.

  In the past few years, end-to-end (E2E) speech recognition systems have achieved promising results. An E2E system dispenses with the division to an LM and an AM, and instead learns a mapping  from (preprocessed) audio straight to the transcription. This makes the training procedure simpler since only one model is trained instead of multiple. However, it has been shown that E2E models can still benefit from, for example, incorporating an external language model \citep{toshniwal2018comparison} or speaker embeddings \citep{rouhe2020speaker}, into an E2E system, making it arguably no longer a pure E2E model, depending on how "E2E" is defined. Results such as these indicate that pure E2E systems will not completely supplant conventional ASR systems, or systems that include multiple separately trained models, any time soon although they benefit from the simplified training procedure. The state-of-the-art results are still obtained with the conventional systems in many ASR tasks, and the thesis explores methods in this paradigm.

% 3. What will be your methodology?
\subsection{Related work and methodology}

  The purpose of the thesis is to experiment with some of the latest acoustic and language modelling methods to improve upon the previous best results obtained for an informal, spontaneous Finnish conversation speech data set. Both the speech data set used in this thesis and the previous best results are described by \citet{enarvi2017automatic}. In their work, the acoustic models are trained on 85 hours of speech using the Kaldi toolkit. A first pass of large-vocabulary decoding and word lattice generation is done using an n-gram language model trained on a conversational Finnish text corpus collected by \citet{enarvi2013studies}. A second pass of rescoring the lattices and generating transcripts is done using a recurrent neural network language model trained on the same text corpus. Subword-vocabulary language models based on statistical segmentation of words \citep{creutz2002unsupervised, creutz2007unsupervised} were found to perform better than a word vocabulary.

  The baseline system is the same in this thesis, and the work begun by replicating the previous results. The Kaldi toolkit includes acoustic model training pipelines, called "recipes", that are tuned to achieve optimal results for a particular speech data set. In the past three years after the above mentioned previous best results were achieved, the Kaldi recipes have been developed further, and the latest machine learning algorithms have been implemented in the toolkit. By applying the latest Kaldi recipes for the Finnish speech data used in this thesis, the previous best results can be improved. Other acoustic modelling experiments of this work include modelling the speaker and channel variability using i-vectors \citep{ivector} and x-vectors \citep{snyder2018x}.

  \citet{vaswani2017attention} introduced a neural network architecture for language modelling called the Transformer, which is based solely on (self-)attention mechanisms \citep{bahdanau2014neural}. Since then, the state-of-the-art language models have been of the transformer model type.
  One of the advantages of attention mechanisms is that they are able to exploit parallel computing, unlike the commonly used recurrent neural networks, since their hidden states do not depend on the hidden states of previous time steps.
  In this thesis, a Transformer-XL \citep{dai2019transformer} LM is trained and evaluated in the ASR task, in a similar manner as described by \citet{jain2020finnish}.

  Other language modelling experiments in this work include evaluating word and subword vocabularies, tuning the hyperparameters of the language models (including constant- and variable-order \citep{siivola2007morfessor} n-gram models, RNN LMs as well as Transformer-XLs), and experimenting with topic modelling (see e.g., \citet{xiong2018session, xing2016topic}).



\subsection{Machine learning} \label{sec:ml} 

\subsubsection{Statistical models}

  A \emph{model} is a simplified or abstract representation of a phenomenon, an artefact that aims to hold relevant and distilled information about its real-world counterpart.
  A model can be statistical, conceptual, mathematical ...
  Model of any of these kinds, however, is ...

  Creating models of phenomena in an environment is the basis for being able to understand, predict, or act in the environment. Brains have evolved to build both unconscious and  consciously accessible models of consequential things, such as animals, weather, or phonemes that compose a language.
  % of plants, of the body that contains the brain, of weather, water etc.
  % representations of time, space, temperature etc. 

  The abstraction and simplification can happen by grouping similar things into one. For example, a mental model of a tree integrates many individual leaves into one foliage, and
  % , more fundamentally, 
  a visual model of a leaf integrates many chloroplasts and other small things into one green surface that is perceived when looking at a leaf.

  A model can be a function that generates an output given an input, emulating how the real-world phenomenon behaves. Inputting a knife into a coconut outputs coconut milk, and an accurate mental model of a coconut should be able to predict that a coconut will function in this manner. More generally, a model can be any kind of mapping from a domain to another.
  
  % ; via lowering the resolution: a mental model of a 

% \subsubsection{Estimating models statistically}

  % There are two directions, ways to create a model of a phenomenon: top-down or bottom-up. Top-down design of models is an exclusively human undertaking, and a much rarer way to build a model. A top-down designed model could be, for example, a recipe for a dish, which is then used to cook up the real thing. This way the simplified model is created without the real phenomenon existing yet, or ever. However, top-down design depends on previous models that have ultimately been estimated from bottom up. 
  % A bottom-up generation of a model is the more usual way models come to being. In this approach, the abstract model is estimated from data gathered from the manifest world. In this example, a recipe for the dish can be developed by experimenting with random recipes in some finite search space of possible recipes, tasting the finished product, and iteratively narrowing down the search space, homing in on an optimal experience. Once a cook has done enough bottom-up design, top-down design becomes possible by deducing from the mental model what a never-before-experienced combination of ingredients will taste like. In mathematics, solving a function in a top-down fashion is called an analytical or a symbolic solution, and bottom up a numerical solution. 

  % In computer science, top-down design has been the usual way to develop systems.
  % % If the function to be estimated is simple enough
  % For example, the best computer chess programs were based on rules until the first decade of this century. In recent years, however, the designing work has been more and more often delegated to an algorithm that performs bottom-up estimation of the desired behaviour. This is referred to as machine learning\footnote{More accurately, machine learning is not the only bottom-up estimation approach, an example of another one being optimisation, which is similar to machine learning but does not aim to generalise beyond the data set used. Machine learning involves optimisation algorithms but what separates it from mere optimisation is the desire to build a model that can generalise to data that was not used for building the model.}. The best chess programs no longer do decisions by following human-written if-then clauses; instead they have estimated the mapping that inputs chess board configurations and outputs chess moves by playing a large number of games against themselves and taken notes about which moves lead to winning. As the behaviour to be modelled becomes too complex and non-linear for a human to design it, the only way is to teach it to the machine by brute-force trial and error. 


\subsubsection{Fitness criterion}

  The mapping from input to output is determined by the parameters $\theta$ that are learned. There are different approaches to defining how good a model is, given a training dataset. Using the maximum likelihood estimation (MLE), the parameters are adjusted to maximise how probable the model judges the data set. The maximum likelihood estimate $\theta_{\textup{ML}}$ of the model parameters is the point in the parameter space that maximises the likelihood of the observed data. The confidence of how good the point estimate is is related to its variance. If alternative samplings of the observations are likely to change the estimate, the variance is larger. In contrast to the MLE approach,
  % and frequentist statistics in general,
  the Bayesian approach is to generate a full probability distribution function for $\theta$. When the model is used to estimate the probability of an observation, the distribution is integrated over the parameter space, and the probabilities of the observations given by each parameter point estimate is weighted by how probable the parameter point is. This way the confidence on the parameter estimate is incorporated in the data likelihood. However, using a full probability distribution instead of a point estimate is computationally expensive, and therefore the MLE method is the more commonly used approach in machine learning.
  
  % incorporate the degree of certainty to the model itself

\subsubsection{Optimisation}

  Depending on the criterium for an optimal model, a optimisation method is chosen and applied to tune the free parameters of a model. When the model to be estimated includes latent variables, the maximum likelihood estimate can be computed using the expectation maximisation (EM) algorithm. A latent (i.e., hidden) variable is a variable that is not directly observed, but can affect the observed variables. A general example of a latent variable is some phenomenon in the real world that is not directly observed, but whose properties are inferred from measurements that are observable. Two types of models that include latent variables are GMMs and HMMs, both of which are central to the conventional implementation methods in automatic speech recognition. In GMMs, the identity of the mixture component $c$ of a data point $o$ is a latent variable, and the probability distribution $P(o)$ is determined by the joint distribution $P(o,c)=P(o|c)P(c)$. Hidden Markov models are named after the latent, hidden sequence that generates an observed sequence\footnote{They are also named after the Russian mathematician Andrey Markov.}; in speech recognition the hidden sequence can be a sequence of phones that generate the sequence of processed audio signal chunks referred to as features.
  % 'Maximum likelihood modeling with Gaussian distributions for classification'
  % gupta2011theory



  The EM algorithm consists of iterating the expectation step (E-step) and the maximisation step (M-step). The E-step 


  % bayesing, MAP
  % cross entropy
  % 


\clearpage
\section{Finnish conversations to text} \label{sec:background} 
  The task of converting speech into text, or \emph{automatic speech recognition}, can be divided into
  % four
  subtasks: \emph{feature extraction}, \emph{acoustic modelling}, \emph{phoneme-to-grapheme mapping}, and \emph{language modelling}. The audio signal is first divided into $T$ segments, and the segments converted into feature vectors $\boldsymbol{O}=\boldsymbol{o}_1,...,\boldsymbol{o}_T$, also called observations, which are a compressed representation of the audio signal. 
  The task is then to find $\argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O})$, where $\boldsymbol{w}$ is a word sequence. This probability is not practicable to compute directly, but by Bayes' rule it can be expanded to 
  \begin{equation}\label{eq:asr-bayes}
    \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
    \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
  \end{equation}

  The probability of the observations ${P(\boldsymbol{O})}$ is not relevant in finding the best transcription (the argmax) for the observations, which leaves the product, or sum in logarithmic space,
  \begin{equation}\label{eq:asr-bayes-2}
    \argmax_{\boldsymbol{w}} P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})
    = \argmax_{\boldsymbol{w}}
      \log \{ P(\boldsymbol{w}) \} + \log \{ P(\boldsymbol{O}|\boldsymbol{w}) \}
  \end{equation}
  to be estimated. Here, the former factor is the a priori probabilities of word sequences, modelled by language models, discussed in Section \ref{sec:lm}. The acoustic model determines likelihoods of observations given phoneme sequences that are mapped to grapheme sequences by a lexicon (also called a dictionary), yielding $P(\boldsymbol{O}|\boldsymbol{w})$. Acoustic modelling is discussed in Section \ref{sec:am}.
  % The first
  % % four
  % sections of this chapter respectively describe the
  % % four
  % components of the conventional ASR system.
  Section \ref{sec:conv} discusses the specifics of recognising conversational Finnish.

  %  A language model gives an a priori probability for each word sequence: $P(\boldsymbol{w})$. The estimate $\hat{\boldsymbol{w}}$ of the best transcription given the observations is the most likely sequence 

  % \begin{equation}\label{asr-bayes-argmax}
  %   \hat{\boldsymbol{w}} = \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
  %   \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
  % \end{equation} 



\subsection{Statistical language modelling} \label{sec:lm} 



  A language model defines the probability distribution\footnote{This would be a probability mass function since word sequences are discrete but, since there is no limit to how long the sequences can be, there is an infinite set of possible word sequences.} $P(\boldsymbol{w})$, from Eq. \ref{eq:asr-bayes}, over word sequences $\boldsymbol{w} = w_1,...,w_N$.
  In statistical language modelling, the distribution  is estimated based on statistics drawn from a training corpus. The statistical approach is in contrast with linguistically motivated models of language that take into account, for example, the grammaticality of a string when determining its probability. 
  

  What constitutes the context depends on the method. The constant-order n-gram model uses a simple context of $n-1$ previous tokens. In more involved models, the context can also be of variable length and include subsequent tokens (bi-directional context).


\subsubsection{Choice of vocabulary} \label{sec:vocab} 

  A language model defines a set of units that the output sequence can include, called the vocabulary. One natural choice of vocabulary is to include in it the word types that appear in the training corpus. However, using smaller units has some benefits over a word vocabulary. If the words are segmented into smaller pieces, or all the way to individual characters, the vocabulary is smaller. This decreases computation and memory requirements for neural language models (Sections \ref{sec:rnn} and \ref{ec:attention}),
  % as well as the language model FSA (Section \ref{sec:wfst}),
  for instance. Another benefit of using a \emph{subword} vocabulary is that words absent in the training corpus can possibly be composed of the subword units, which means that the out-of-vocabulary (OOV) rate is smaller. The downside of subword vocabularies is that each sentence includes a larger number of units to be processed.
  % , which increases the computational costs for some systems, for example when 

  When segmenting words into subword units it is beneficial to 


\subsubsection{n-gram language models} \label{sec:ngram} 


  % An n-gram language model bases the prediction of the last token in a sequence of $n$ tokens on statistics gathered from a training corpus. 
  The probability of a word sequence is the product of the probabilities of the constituent words, and the probability of each word is conditional on $n-1$ previous words (i.e., the context):
  \begin{equation}\label{eq:lm}
    P(w_1,...,w_N) = \prod_{i=1}^N P(w_i|w_{i-(n-1)},...,w_{i-1})
  \end{equation}
  

  A simple 
  % (the simplest of the ones that make any sense)
  method to define the probability of an n-gram is to let the probability of each word be its normalised frequency in the context:
  \begin{equation}\label{eq:ngram}
    P(w_i|w_{i-(n-1)},...,w_{i-1}) = \frac{\textup{count}(w_{i-(n-1)},...,w_{i-1}, w_i)}{\textup{count}(w_{i-(n-1)},...,w_{i-1})}
  \end{equation}
  This is called the maximum likelihood estimate of $P(w_i|w_{i-(n-1)},...,w_{i-1})$ since it maximises the likelihood of 
  % the language model for 
  this specific training data set\citep{chen1998empirical}. As in machine learning in general, however, the aim is to use the training data to distil from it a generalisable model, which predicts patterns also in previously unseen data, instead of building a model that maximally accounts for the training data set. 
  
  % problems aimed to be fixed by smoothing

  To improve the predictions, \emph{smoothing} can be applied to the simple occurrence counts by redistributing probability mass from the most common n-grams to the less common ones, i.e., \emph{discounting} the most frequent n-grams. Low occurrence counts become a problem especially when $n$ is high, since the number of possible n-grams increases exponentially as $n$ increases. If there is an insufficient number of examples of the n-grams of the desired order, the information of lower-order n-grams (unigrams,...,(n-1)-grams) can be used; the model can \emph{backoff} to the lower orders to make the probability distribution smoother. The lower-order n-gram scores can also be \emph{interpolated} with the scores of the n-grams of the nominal order $n$. 
  
  Backing off to lower orders revives % better  word ?
  problems that motivated using higher-order n-grams in the first place. One of them is that of two words that are equally frequent, and thus have the same unigram probability, one may have a very specific kind of context in which it almost always appears whereas the other appears in various contexts. Given a novel context, which is why backing off to unigrams is necessary, the former is statistically less probable to appear in it than the latter, but this is not captured by unigram statistics.
  \citet{kneser1995improved} introduced a smoothing method, which has become commonly used, where the number of different bigram contexts of a word correlate with the unigram backoff probability. The number of seen bigram types where the word $w$ is the latter word, i.e., the number of types of the previous token $w'$ that at least once precede $w$ in the corpus, can be expressed as $| \{ {w' : \textup{count}(w',w)>0} \} |$. This count is normalised by the number of all word types that are seen as the first word of a bigram to get the KN unigram probability:
  \begin{equation}\label{eq:continuation}
    P_{\textup{KN}}(w) = \frac{| \{ {w' : \textup{count}(w',w)>0} \} |}
                              { \sum_{w''} |\{ {w' : \count(w'w'')>0}\}|}
  \end{equation}
  For a Kneser-Ney smoothed bigram model, the unigram and bigram statistics are then interpolated:
  \begin{equation}\label{eq:kn_bigram}
    P_{\textup{KN}}(w_i|w_{i-1}) = \frac{\max(\textup{count}(w_{i-1}w_i)-d, 0)}
                                        {\textup{count}(w_{i-1})} 
                                  \lambda(w_{i-1})
                                  P_{\textup{KN}}(w_i)
  \end{equation}
  where $d$ is a discount constant, usually $0<d<1$, and $\lambda$ is a normalising factor that defines how the discounted probability mass is redistributed:
  \begin{equation}\label{eq:kn_lambda}
      \lambda(w_{i-1}) = \frac{d}
                          {\sum_{w'} \textup{count}(w_{i-1},w')  }
                          | \{ {w' : \textup{count}(w_{i-1},w')>0} \} |
  \end{equation}
  The discount is normalised by the sum of the counts of all bigrams where the $w_{i-1}$ is the first word, and the normalised discount is multiplied by the number of word types that have followed $w_{i-1}$, i.e., the number of word types that have been discounted, so that $P_{\textup{KN}}(w_i|w_{i-1})$ over all $w_i$ equals to one.

  The bigram formulation can be generalised to higher-order n-grams:
  \begin{equation}\label{eq:kn}
    P_{\textup{KN}}(w_i|w_{i-n+1}^{i-1}) = 
        \frac{\max(\textup{count}_{\textup{KN}}(w_{i-n+1}^{i-1},w_{i})
            - d, 0)}{\sum_{w'} \textup{count}_{\textup{KN}}(w_{i-n+1}^{i-1},w')} + 
        \lambda(w_{i-n+1}^{i-1})
        P_{\textup{KN}}(w_i|w_{i-n+2}^{i-1})
  \end{equation}
  where $w_{i-n+1}^{i-1}$ is the $n-1$ words before $w_i$ and $\textup{count}_{\textup{KN}}$ is the count for the highest order and the number of different words that precede the n-gram for the lower orders \citep{Jurafsky2019}.

  In the modified Kneser-Ney smoothing, introduced by \citet{chen1998empirical}, the discount constants are different for n-grams that have one, two, or more than two occurrences, changing also $\lambda$ for the distribution to still sum to one. This is motivated by empirical results suggesting that the optimal $d$ depends on the frequency.

  A common approach to making an n-gram model more efficient is to \emph{prune} n-grams that do not contribute to the 
  \citep{siivola2007growing}

  The ARPA (named after DARPA which was previously named ARPA) backoff n-gram format is used ... \footnote{\url{http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html}}

\subsubsection{Recurrent and convolutional neural language models} \label{sec:rnn} 

\subsubsection{Attentive neural language models} \label{sec:attention} 

  % \citet{dai2019transformer}


\subsection{Automatic speech recognition using the Kaldi toolkit} \label{sec:am}

  Kaldi is a toolkit for automatic speech recognition, used in the ASR experiments in this study. This chapter aims to give an overview of the most relevant theoretical underpinnings of Kaldi ASR systems, as well as some of the practical details of implementing the systems.


\subsubsection{Feature extraction} \label{sec:feats} 

  A speech audio signal contains a lot of information that is irrelevant for converting the signal to text. The first step of ASR is to find the features of the signal that contain the information about what is being said. An assumption is made that the speech signal does not change meaningfully in a time frame of about 10 milliseconds so that the signal can be divided into frames with this time resolution. The frames overlap so that each frame is about 20 or 25 milliseconds, and a tapered window function, such as Hamming, is applied to (i.e., multiplied by) each frame. This window function removes the discontinuities that occur on the borders of frames, and the overlapping compensates for the tapering of the window function so that the distorting effect on the signal statistics is minimised \citet{}. % https://wiki.aalto.fi/display/ITSP/Windowing

  \paragraph*{Mel-frequency cepstral coefficients}
    The stationary frames' frequency components can be then computed with the Fourier transform. A commonly used method is to extract the MFCCs by applying a logarithmic mel-scale filterbank to the frequency spectrum, and lastly computing the DCT. The log mel-scale emphasises the lower frequencies emulating the way humans perceive sound, i.e., the way the human inner ear recognises lower frequencies with higher frequency resolution. The DCT decorrelates the coefficients so that the use of diagonal covariance matrices is possible in the subsequent stages of the modelling, namely when using GMMs to model the HMM state emissions (Section \ref{sec:phoneme_hmm}). The use of diagonal covariance matrices greatly reduces the number of free parameters, but the trade-off is that correlation between feature vector elements is not modelled.
  

  \paragraph*{Delta and delta-delta features} 
    % ?
    % The MFCC method thus assumes that coefficients adjacent in time are independent of each other, which is a false assumption in the case of speech signals.
    % \?
    An MFCC vector encodes only the stationary frequency features of a frame. However, a speech signal varies in time, and this variation carries meaning about which phones are uttered.
    Thus, it is useful to add information to the feature vectors about how the signal changes in time. 
    Information about temporal change and about change of temporal change is extracted from the MFCCs by calculating the differences and second-order differences of adjacent coefficients. These features are called the delta ($\boldsymbol{\Delta}$) and delta-delta ($\boldsymbol{\Delta\Delta}$), or acceleration, features. 
    % The $\boldsymbol{\Delta}$s and $\boldsymbol{\Delta\Delta}$s are concatenated with the MFCC vectors, increasing the feature vector length threefold.
    The delta feature vector $\boldsymbol{\Delta}_t$ corresponding to the MFCC vector $\boldsymbol{c}_t$ (or the time step if that vector) is calculated by subtracting the weighted previous vector(s) from the weighted subsequent vector(s) and normalising the sum:
    \begin{equation}
    \boldsymbol{\Delta}_t = \frac{\sum_{\theta=1}^{\Theta}
    \theta(\boldsymbol{c}_{t+\theta}-\boldsymbol{c}_{t-\theta})}{2\sum_{\theta=1}^{\Theta}\theta^2}
    \end{equation}
    In Kaldi, the default window length $\boldsymbol{\Theta}$ is 2, so the $\boldsymbol{\Delta}$s are computed by multiplying the MFCCs with a sliding window of values $[-2,-1,0,1,2]$ and then normalising by dividing by $2*(1^2 + 2^2)= 10$ \citep{kaldi}. The $\boldsymbol{\Delta\Delta}$s are computed by applying the same method to the $\boldsymbol{\Delta}$ features. The first and last MFCCs are replicated to fill the window \citep{htkbook}.


  \paragraph*{Cepstral mean and variance normalisation}

    The cepstral mean normalisation (CMN) \citep{rosenberg1994cepstral} and cepstral mean and variance normalisation (CMVN) \citep{viikki1998cepstral} are methods to make the features more useful in noisy conditions. In these techniques, the MFCC feature vectors are normalised to have a zero mean, and in CMVN unit variance, over a sliding finite segment. After the normalisation, clean and noisy MFCCs are more similar, which mitigates the performance reduction caused by noisy environments. The variance of the MFCCs of a noisy speech signal is generally lower than the variance of those of a clean signal. By requiring the variance be constantly unity, noisy and clean speech MFCCs resemble each other more closely. Similarly, when noise is added to a signal, the mean changes, and by requiring the mean to be zero the characteristics of clean and noisy signals become more alike. Normalising variability between the speech signals is in general important in training an ASR system that ought to recognise different types of speech by different speakers in different recording conditions. The topic of handling meaningless variability (i.e., variability that does not contribute to the phoneme content of the utterance)  between the utterances is revisited in Section \ref{sec:adaptive_training} which describes speaker adaptive training.



  \paragraph*{Dimensionality reduction and feature-space transforms}

    Features can be compressed by a dimensionality reduction method. Two common methods for this are the principal component analysis (PCA) \citep{pearson1901liii} and linear discriminant analysis (LDA) also called Fisher discriminant analysis \citep{martinez2001pca}.
    % The general idea behind PCA is that a feature vector contains interdependencies between its elements, which means there is redundant information that can be lessened by making the elements (i.e., dimensions) more independent. 
    The general idea of these methods is to find a linear combination of variables that best explain the observations.
    PCA achieves this by performing an orthogonal transformation that transforms the data matrix into a space where the dimensions, called the principal components, are ordered by their variances in decreasing order. After this, dimensionality reduction is achieved by pruning the dimensions with the lowest variance since they contribute the least to the information content of the features. LDA, on the other hand, searches for those dimensions that best discriminate between classes. For LDA, labelled training data is needed. 


    % MLLT and LDA \citep{somervuo2003feature, pylkkonen2006lda} transforms 
    
    MLLT is a feature orthogonalizing transform that makes the features more accurately modeled by diagonal-covariance Gaussians ->Improved feature processing for Deep Neural Networks

    The intuition behind LDA is that defining new axes that maximise variance between speakers makes discriminating easier.

  \paragraph*{Differences between GMM and DNN input}

    GMMs are sensitive to the number of dimensions of the feature vectors: even a small increase in the vector length will increase the number of GMM parameters substantially. The usual number of dimensions used with GMMs is about 40. With DNNs, however, the input vector determines only the width of the input layer--widths of the other layers are not constrained by the dimensionality of the input features. Therefore, DNNs can learn to use longer feature vectors, and often the used number of dimensions is a few hundred \citep{rath2013improved}.

    % frame splicing


\subsubsection{Modelling phonemes with hidden Markov models} \label{sec:phoneme_hmm} 

  Estimating the likelihoods of observations given phonemes is achieved by creating a HMM for each phoneme. 
  The phoneme-specific HMMs
  generate likelihoods of the observed sequences which can be used to map observations to phonemes. This way the task becomes to estimate the parameters of the HMMs so that each of them models the associated phoneme as accurately as possible.
  % In the conventional ASR system, used also in this thesis, phonemes are modelled by HMMs, which are then concatenated to model utterances.

  A hidden Markov model consists of a hidden Markov chain, also called regime, and the observation sequence, i.e., feature vectors. Each observation $\boldsymbol{o}_t$ has a probability $b_i(\boldsymbol{o}_t)$ of being generated when a hidden state $i$ is entered. In other words, the observation is a probabilistic function of the hidden state. 
  % Gaussian mixtures
  A state's emission probabilities are represented by a PDF, typically a mixture of multivariate Gaussian densities
  \begin{equation}\label{}
    b_i(\boldsymbol{o}_t) = \sum^{M_j}_{m=1}c_{jm}
      \mathcal{N}(\boldsymbol{o}_t ; \boldsymbol{\mu}_{jm}, \boldsymbol{\Sigma}_{jm})
  \end{equation}
  where $\boldsymbol{\mu}_{jm}$ is the mean vector, $\boldsymbol{\Sigma}_{jm}$ is the covariance matrix and $c_{jm}$ is mixture weight for mixture component $m$ in state $j$. The Gaussian mixture density is
  \begin{equation}\label{}
    \mathcal{N}(\boldsymbol{o} ; \boldsymbol{\mu}, \boldsymbol{\Sigma})
       = \frac{1}{\sqrt{(2 \pi)^n |\boldsymbol{\Sigma}|}} 
      e^{-\frac{1}{2}(\boldsymbol{o} - \boldsymbol{\mu})^\top 
      \boldsymbol{\Sigma}^{-1} 
      (\boldsymbol{o} - \boldsymbol{\mu})}
  \end{equation}

  Because a mixture of Gaussians can assume arbitrary shapes, they can model non-Gaussian phenomena. 

  HMMs are used to model sequences, but observations are independent of past observations. Instead, the regime has a memory, although the shortest possible: the probability of being a certain state in the next time step depends only on the current state and not the previous states. This independence of the previous transitions is called the Markov assumption. Each hidden state pair (an arc from a state to another) is associated with a transition probability $a_{ij}$ that describes how probable it is to move from state $i$ to state $j$. 

  A HMM is defined by ...

  

  % in Kaldi 
  % http://kaldi-asr.org/doc/hmm.html
  The typical HMM topology for a phoneme is a left-to-right model, also called the Bakis model, with three emitting states that each have a transition to the next state and a self-loop. The model also includes a fourth non-emitting final state that has no outbound transitions. However, in the Kaldi chain models (see Section \ref{sec:dnn_am}), the topology is reduced to have only one emitting state due to a lower time resolution used.
  In Kaldi, the phoneme topology is defined in the lang/topo file.
  
  
  After initialising a HMM for each phoneme, the parameters, i.e. means, covariances and mixture weights need to be estimated in the process referred to as "training" of the model.


\subsubsection{Finding the best hidden state sequence and training HMMs} \label{sec:hmm_est} 

  The speech recognition system is trained in a supervised manner, meaning that the training data consists of a parallel corpus of speech audio and corresponding correct transcription. However, the task of the acoustic model is not as straightforward as finding a label for an input vector\footnote{In End-to-end ASR the whole ASR task is simplified to outputting an arbitrary length grapheme sequence given the sequence of observed features}. The reference transcription can be of arbitrary length but the AM is required to map each observation to a HMM state, generating an equal-length \emph{alignment} of observations and states. The states correspond to phonemes, so an alignment can be mapped to a transcription of the audio.

  ML is one criterium of finding the best HMM parameters; others include the MMI (see Section \ref{sec:mmi}) and the MAP criteria.

  % Baum-Welch algorithm
  The Baum-Welch algorithm is an expectation-maximisation algorithm for estimating the HMM parameters. Maximising the expectedness of the data can be turned around and seen as minimising how surprising the training data are to the model by modifying the model. Expectation maximisation is an iterative method to finding a local maximum for the likelihood of a set of data given a model.  Here, the task is to maximise the likelihood $P(\boldsymbol{O}|M)$ of the observations $\boldsymbol{O}$  given the parameters of
  the HMM $M$. % here all HMMs, viterbi: only the best ?
  If the HMM had only one state $j$, the maximum likelihood estimate $\hat{\boldsymbol{\mu}}_j$ would simply be the average of the observations, and $\hat{\boldsymbol{\Sigma}}_j$, too, could be determined directly, using the covariance definition.
  In practice, there are many states which is why the parameters need to be estimated numerically, iteratively. 
  However, the initial parameter values can be taken from simple statistics of the observations. The observations are divided equally between the states and the means and variances of the states are taken from the average values.

  The maximum likelihood estimates for the parameters are 
  \begin{equation}\label{eq:mu_hat}
    \hat{\boldsymbol{\mu}}_j = \frac{\sum^T_{t=1}L_j(t)\boldsymbol{o}_t}
      {\sum^T_{t=1}L_j(t)}
  \end{equation}
  and
  \begin{equation}\label{eq:sigma_hat}
    \hat{\boldsymbol{\Sigma}}_j = \frac{\sum^T_{t=1}L_j(t)
      (\boldsymbol{o}_t - \boldsymbol{\mu}_j)
      (\boldsymbol{o}_t - \boldsymbol{\mu}_j)^\top }
      {\sum^T_{t=1}L_j(t)}
  \end{equation}
  The numerator and denominator sums are \emph{accumulated} from the observations.

  % kaldi: gmm-init, HTK: Hinit

    % 
  
  % The training starts with some trial values of the parameters, for which

  % , for which the likelihood is calculated, given each training example.
  
  The state occupation probability
  \begin{equation}\label{eq:occ}
    L_j(t) = P(x(t)=j|\boldsymbol{O},M),
  \end{equation}
  i.e., the probability of being in state $j$ at time $t$, is calculated using using the forward-backward algorithm. The forward probability $\alpha_j(t)$ and backward probability $\beta_j(t)$ are defined as 
  \begin{align}
    \alpha_j(t) &= P(\boldsymbol{o}_1,...,\boldsymbol{o}_t,x(t)=j|M)  \label{eq:forward} \\
    \beta_j(t) &= P(\boldsymbol{o}_{t+1},...,\boldsymbol{o}_T|x(t)=j,M) \label{eq:back}
  \end{align}
  Spelled out, $\alpha_j(t)$ is the probability of the partial observation sequence up to time $t$ and that $M$ is in state $j$ at the time step. The backward probability is the probability of the partial observation sequence at the subsequent time steps up to the last vector, given that at the current time the model state is $j$. 
  The forward probability is a joint probability of the observations and the state, whereas the backward probability of the observations is conditional on the state. This allows for the state occupation probability to be determined by the product of the forward and backward probabilities (from Eqs. \ref{eq:occ} \ref{eq:forward} and \ref{eq:back})
  \begin{align}\label{}
    L_j(t) = \frac{ \alpha_j(t) \beta_j(t) }{P(\boldsymbol{O}|M)}
  \end{align}
  
  $\alpha$ and $\beta$ are calculated respectively using the recursions
  \begin{align}
    \alpha_j(t) &= \bigg[ \sum^{N-1}_{i=2}\alpha_i(t-1)a_{ij} \bigg] b_j(\boldsymbol{o}_t)
    \label{eq:alpha_recurs} \\
    \beta_i(t) &= \sum^{N-1}_{j=2} \beta_j(t+1) a_{ij} b_j(\boldsymbol{o}_{t+1}) 
    \label{eq:beta_recurs}
  \end{align}
  and the initial conditions,
  \begin{align}
    \alpha_1(1) &= 1, \; \; \; \; \alpha_j(1) = a_{ij} b_j(\boldsymbol{o}_1) 
    \label{eq:alpha_init} \\
    \beta_i(T) &= a_{iN} \label{eq:beta_init}
  \end{align}
  for $1<j<N$ and the final conditions
  \begin{align}
    \alpha_N(T) &= \sum^{N-1}_{i=2}\alpha_i(T)a_{iN} \label{eq:afinal} \\
    \beta_1(1) &= \sum^{N-1}_{j=2} a_{1j} b_j(\boldsymbol{o}_1) \beta_j(1) \label{eq:bfinal}
  \end{align}
  where the limits of the sums exclude the states $1$ and $N$ because they are non-emitting. The recursion \ref{eq:alpha_recurs} calculates the forward probabilities (of seeing the specified observations and being at the state $j$) by summing all possible forward probabilities for all possible predecessor states $i$ weighted by the transition probability $a_{ij}$.

  From \ref{eq:forward}, \ref{eq:afinal} and \ref{eq:bfinal} it follows that calculating the forward probability also yields the total likelihood $P(\boldsymbol{O}|M)=\alpha_N(T)$.

  % Viterbi training
  An alternative approach to the B-W algorithm is an iterative procedure called Viterbi training, also called Viterbi extraction or Baum-Viterbi algorithm since it involves the Baum re-estimation (Eqs. \label{eq:mu_hat} and \label{eq:sigma_hat}) and the Viterbi algorithm \citep{lember2008adjusted}. Instead of maximising the likelihood of all the data as in Eq. \ref{eq:alpha_recurs}, in VT the probability of only the most likely hidden sequence is maximised
  \begin{equation}
    \phi_N(T) = \max_i \{ \phi_i(T)a_{iN} \}
  \end{equation}
  for $1<i<N$ where
  \begin{equation}
    \phi_j(t) = \max_i \{ \phi_i(t-1)a_{ij} \} b_j(\boldsymbol{o}_t)
  \end{equation}
  and initially
  \begin{align}
    &\phi_1(1) = 1 \\
    &\phi_j(1) = a_{1j}b_j(\boldsymbol{o}_t).
  \end{align}
  for $1<j<N$. This alignment process finds an arc $ij$ for each observation $\boldsymbol{o}_t$ so that the last

  This results in an approximation of the maximum likelihood estimate (which was computed in the B-W algorithm)
  % ?
  .
  The Viterbi training is computationally less expensive than the B-W algorithm.
  In Viterbi training, the HMM parameters and the most probable hidden state sequence, both unknown, are estimated alternately. After updating the HMM parameters, the training data observations are aligned with the states. The new alignment is then used for estimating the HMM parameters again, and so on. 
  % Within each state, a further alignment is made to align observations with mixture components. 
  % Using the Viterbi algorithm, the states are aligned with the observation sequence by maximising



  % AS stated in the Kaldi documentation, computed alignment is a sequence of transitions that corresponds to an utterance. 


\subsubsection{Phones in context} \label{sec:triphone} 
  Phones of the same phoneme sound different when flanked by different phonemes. For this reason, contextual information is modelled, too, by assigning each triphone a HMM. \citep{schwartz1985context}

  The monophone models are first trained
  % (with single-component Gaussians?)
  and the triphone models are initialised with the monophone model set parameters. The number of Gaussians in the triphone models is increased gradually and the parameters are re-estimated.

  % transition modelling
  % The transition probabilities from a state to another are essentially the counts of the transitions seen in the training corpus.


\subsubsection{State tying and phonetic decision trees} 
  % tying
  The states of the phoneme HMMs are can be tied together so that the parameters of the output distributions of those states are shared. This makes the estimation of the parameters more robust because there are more training data occurrences, and also makes the total system more compact with fewer parameters \citep{young1992general}. States are clustered based on some metric of similarity.  

  % http://kaldi-asr.org/doc/tree_externals.html
  In tree-based clustering as described by \citet{young1994tree}, the states are divided into branches in a top-down optimisation procedure. Starting from the root node, the question that maximises the likelihood is selected for the node, with the data on each side of the divide being modelled by a single Gaussian.
  In a phonetic decision trees the questions are about the context of the phone, e.g. "Is the phone on the left of the current phone a fricative?". 

  After the procedure, the leaves of the tree are the state clusters in which the states are tied. In the final stage, leaves can be merged if the likelihood does not decrease more than a threshold value.

  After a tree has been constructed for the states of the triphone models, also previously unseen triphones can be synthesised by traversing the tree to the appropriate leaf node, i.e. cluster, by answering the questions about that triphone's context and using the tied states of that cluster.


\subsubsection{Speaker adaptive training} \label{sec:adaptive_training} 
  % alignments % lda + mllt
  % https://kaldi-asr.org/doc/transform.html

  % acoustic normalisation = feature space

  Variability between speakers poses a challenge to an ASR system. Each speaker may have idiosyncratic voice, distinct style of pronunciation as well as distinct recording conditions, which can degrade the ASR performance. This section describes some of the methods to account for inter-speaker variability by adapting either the model or the features to a particular speaker. An exhaustive overview of all the used methods is beyond the scope of this thesis.

  The adaptation can be done either in testing or training, usually respectively referred to as \emph{speaker adaptation} and \emph{speaker adaptive training} (SAT). Speaker adaptation can be done by modifying a speaker-independent (SI) model to create personal, speaker-dependent (SD) models for each speaker.  This can be done by taking a small number of speech data from the speaker and using this adapt a SI model to the specific speaker \citep{shinoda2011speaker}. In SAT, speaker-specific information is incorporated in the training of the model. When a model is trained using SAT, it naturally benefits to use an adaptation scheme also in testing. Both GMM- and DNN-based AMs have been shown to benefit from speaker adaptation as well as SAT. A commonly used SAT method for GMMs is using the feature-space MLLR. For DNNs

  % Cepstral mean and variance normalisation, mentioned in Section \ref{sec:feats}, mitigates the problem of different noise conditions of speech signals. 
  
  % One solution to the problem is \emph{speaker adaptation}, which can be done by modifying a speaker-independent (SI) model to create personal, speaker-dependent (SD) models for each speaker.  This can be done by taking a small number of speech data from the speaker and using this adapt a SI model to the specific speaker. However, this requires data from the speaker and re-estimating the parameters of the model to conform to the speaker.
  % Instead  of adapting the model parameters, speaker adaptation can be done also transforming the features of the test speaker \citep{}.
  
  % Often the the aim is to build a  system that recognises speech also from completely new speakers.
  % Speaker-dependent systems can be adapted from speaker-independent systems by a transformation  
  % The solution to speaker variability discussed in this section is to learn an explicit representation for each speaker's idiosyncratic qualities which is then used in addition to the speaker-independent (SI) model in the training. Because the speaker variabilities are learned by separate models, the SI model encodes, in theory, only the information needed for discriminating between phonemes, and is therefore better able to generalise to new speakers.
  % This is called speaker-adaptive training, or SAT for short.  
  SAT requires identifying each speaker from the metadata that indicates the speaker ID. Adaptation can be useful even if the speaker-identifying metadata is absent, in which case a separate adaptation is learned for each utterance, as if each utterance were spoken by a different person.

  With a HMM/GMM acoustic model, speaker adaptive training can be done by representing each speaker's distinct qualities as a transform in the feature space or in the model space. A feature space transform is applied on the observation vectors, and model space transform on the mean and variance of the GMM. Furthermore, a model space transform can be unconstrained or constrained, the former being separate transforms for the means and variances and the latter using the same transform for both \citep{gales1998maximum}.

  Using the model space transform, the aim is to learn the optimal model $\boldsymbol{M}$ and adaptation $\boldsymbol{G}^{(r)}$ for speaker $r$
  \begin{equation}
    (\hat{\boldsymbol{M}}, \hat{\boldsymbol{G}}) =
        \argmax_{(\boldsymbol{M},\boldsymbol{G})}} \prod^R_{r=1}
        P(\boldsymbol{O}^{(r)}; \boldsymbol{G}^{(r)}(\boldsymbol{M}))
  \end{equation}
  where $\hat{\boldsymbol{M}}$ are the model parameters \citep{anastasakos1996compact}.
  The feature space transform is analogous, only transforming the observations instead of the model.

  A common SAT transform is the maximum likelihood linear regression where $\boldsymbol{G}(\boldsymbol{M})$ is an affine\footnote{\emph{Linear} regression is thus a slight misnomer.} transformation defined by the matrix $\boldsymbol{A}$ and bias $\boldsymbol{b}$ 
  \begin{equation}
    \boldsymbol{\mu}^{(r)} = \boldsymbol{A}^{(r)}\boldsymbol{\mu}+\boldsymbol{b}^{(r)}
  \end{equation}
  Whether an adaptation modifies the model or the features is in some cases only a question of interpretation when describing the method, and a question of choosing among two equivalent implementations. The constrained MLLR (CMLLR) can be represented as a feature space transform \citep{gales1998maximum}, and is therefore also called feature-space MLLR or fMLLR. The transformation projects the features from the speaker-specific space to the speaker-normalised space.

  In practice (in Kaldi), affine transform is applied by appending a 1 to the feature vector, and multiplying it with the linear transform $\boldsymbol{A}$ concatenated with the constant offset (bias) $\boldsymbol{b}$: $\bracketVectorstack{  \boldsymbol{A} ; \boldsymbol{b}} \bracketVectorstack{\boldsymbol{o} \\ 1}$. The Kaldi program \texttt{transform-feats} is used to multiply the feature vectors with transform matrices.
  % \citep{povey2008fast}
  \newline

  As well as GMM parameters, also DNN acoustic models (see Section \ref{sec:dnn_am}) can be estimated speaker-adaptively. Adapting can be performed in the feature-space by appending or transforming the observations or in the model-space by modifying the DNN AM parameters.
  
  A common method is to extract i-vectors from speakers \citep{ivector}, optionally perform a transformation of the i-vectors using a control network, and append them to the features that are fed to the DNN \citep{miao2015speaker}. i-vectors have usually a hundred or a few hundred dimensions that encode properties of a speaker as well as the environment, enabling the AM to generalise more robustly to speech in different conditions. The original paper by \citet{ivector} referred to the i-vector as the \emph{total factors} $\boldsymbol{w}$ in the \emph{total variability space} $\boldsymbol{T}$ because it models both speaker and channel variability in contrast with joint factor analysis \citep{kenny2005joint} which makes a distinction between the two sources of variability. 
  
  The total variability space $\boldsymbol{T}$ models the variability between utterances, and it is also referred to as the i-vector extractor \citep{alam2014use}. i-vector extractor training starts by estimating a speaker-independent GMM called an \emph{universal background model}, or UBM. The purpose of the UBM is to represent the general, or universal, characteristics of speech, i.e., it is the speaker-independent model. Baum-Welch statistics are obtained from the UBM at the frame level. The i-vector is then defined as the mean vector of the posterior Gaussian distribution conditioned on the Baum-Welch statistics for a given utterance \citep{ivector}. In practice it is a MAP estimate \citep{kenny2005eigenvoice}. The mean vector (i-vector) is actually a concatenation of the mean vectors of the mixture components, called a supervector \citep{campbell2006svm}. A speaker-specific utterance supervector $\boldsymbol{M}$ is composed of the factors
  \begin{equation}
    \boldsymbol{M} = \boldsymbol{m} + \boldsymbol{T} \boldsymbol{w}
  \end{equation}
  where $\boldsymbol{m}$ is the UBM supervector.  The extractor compresses the high-dimensional statistics from the UBM into the dense i-vector representation for a given utterance. In decoding, i-vectors can be extracted, for example, every 10 frames, and an utterance-level i-vector can be obtained by averaging the frame-level i-vectors across the utterance.

  Another approach to modelling speaker characteristics is to train a feedforward deep neural network to project speakers into an embedding space that models the speaker variance. \citet{snyder2017deep} introduced a \emph{speaker embedding} method  where the DNN is trained to classify speakers from variable-length segments. The DNN learns to assign each speaker an embedding space vector which can be then used in the AM training similarly to an i-vector.
  \citet{snyder2018x} describe the use of speaker embeddings, which they call x-vectors, in speaker  recognition. In contrast with i-vectors, x-vectors model only the speaker characteristics since the DNN is trained to identify speakers. 
  
  

  %%%%  
    % A speaker-independent (SI)!!!!!!!!!!! ASR system can be trained on and transcribe the speech of different speakers, in different recording conditions, 
    % % about different topics,
    % and with other distinct attributes. When these speech audio attributes are known, and in some way consistent, they can be taken into account in the training of and decoding with the system.
    
    % This section discusses how an ASR system training can be improved by  and learning to adapt the general acoustic model to each speaker.
    
    % The attributes that are distinct but consistent for each speaker include obviously the qualities of the speaker's voice, but also the recording conditions since it can be assumed that these stay constant per speaker. Alternatively, adaptation can be performed for each environment \citep{}.
    
    % The idea can also be generalised to any other attributes of the speech which modulates the speech in some consistent manner, for example the gender of the speaker. 

    % For each speaker, each feature vector is transformed by multiplying it by a matrix that encodes information about how this speaker differs from the general speaker.
    
    % The transform can be done when testing the final model (speaker adaptation, discussed in Section \ref{sec:adaptation}) and in the training process. The latter is called adaptive training.
    
    % First, a transform is generated for each speaker. 



\subsubsection{Discriminative training} \label{sec:mmi} 

  % The maximum likelihood method aims to  
  % Typically?
  % Discriminative training is based on an objective function which is minimised or maximised using an optimisation algorithm.

  The MLE method described in Section \ref{sec:hmm_est} aims to maximise the likelihood of the observed sequence given the most probable HMM in Viterbi training, or all the possible HMMs in Baum-Welch.
  % This method results in a model that predicts the training data by determining a probability distribution over the feature space, i.e., over all possible observations. This is called a generative model since the distribution can be sampled for predictions. However, the ability to produce examples of the modelled data is redundant in classification tasks.
  % Furthermore, t
  The MLE method maximises the likelihood of the observations for all of the competing HMMs independently of each other. However, the underlying task is to find the HMM that most accurately models the observation sequence, so it makes sense to also try to find the meaningful differences between HMMs.
  % The HMMs competing for being the most accurate model of the observations are not compared to each other.
  In discriminative training, instead of maximising the likelihood of the data given a generative model, a model is trained to discriminate between the classes, which in this case are different phoneme sequences. This way more capacity is used to model the boundaries between different HMMs, instead of using it to model just the relations between individual HMMs and alignments. 

  The objective function can be simply the difference between the correct classifications (e.g., a phoneme sequence) for a set of examples and the classifications assigned to them by the model. This is called the minimum classification error (MCE) criterion \citep{juang1997minimum}.
  Another type of objective function is the maximum mutual information (MMI) \citep{bahl1986maximum} criterion
  \begin{equation} \label{eq:mmi}
    \mathcal{F}_{\textup{MMI}}(M) = \sum_{r=1}^R \log 
      \frac{P(\boldsymbol{w}_r)P(\boldsymbol{O}_r|\boldsymbol{w}_r)}
        {\sum_{\boldsymbol{w}} P(\boldsymbol{w})P(\boldsymbol{O}_r|\boldsymbol{w})}
  \end{equation}
  where $\boldsymbol{w}_r$ is the correct transcription for the $r$'th speech file \citep{povey2005discriminative}. The numerator is the log-probability of the output sequence, and the denominator is the log-probability of all possible output sequences. This way the probability of a particular sequence is normalised by the probability of all sequences.
  % equivalent to conditional maximum likelihood 
  % extended b-w
  
  % vesely2013sequence

  % Traditionally this has been done by training a cross-entropy system, generating word lattices with a weak language model, and using these lattices as an approx- imation for all possible word sequences in the discriminative objective function– as was done when Gaussian Mixture Mod- els were the state of the art povey2016purely


\subsubsection{Mapping words to phoneme sequences} \label{sec:lexicon} 

  The hidden state sequence decoded from a HMM corresponds to a phoneme sequence, but the ultimate aim is to generate a word sequence for a given observation sequence. For mapping words to phoneme sequences, the ASR system uses a \emph{lexicon}, also referred to as \emph{pronouncing dictionary}, or just \emph{dictionary}. 

  In Finnish there is generally a one-to-one mapping from graphemes to phonemes. This makes creating a lexicon very simple.

  A lexicon for a language like English has to be constructed largely by hand, as most of the mapping from letters to phonemes does not have a sound\footnote{Ignore the pun, please.} logic behind it. This is due to historical change in the pronunciations of words that was not translated into corresponding change in their written forms (e.g., during the Great Vowel Shift) as well as loan words from other languages which have different rules of orthography \citep{english}.
  
  Similar change can be seen in Finnish as words are imported from other languages, mainly from English. Loan words often do not follow the Finnish phonetic orthography, which means that the one-to-one mapping from letters to phonemes is no longer accurate for these words. For example, "googlata" is pronounced "G U U G L A T A" instead of following the Finnish orthographic rules to pronounce it "G O O G L A T A", or alternatively spelling it "guuglata". In this study these inaccuracies are ignored(?), and the Finnish lexicon is generated automatically using the simple letter-to-phoneme correspondence, where letters correspond to a single phoneme, usually denoted by itself (with a few exceptions: e.g., "c"->"K", "q"->"K V").

  \begin{table}[htb]
    \centering
      \begin{tabular}{|ll|}
      \hline
      {[oov]}   & SPN \\
      !SIL    & SIL \\
      {[laugh]} & SPN \\
      {[reject]} & NSN \\
      +i+     & I \\
      +loma   & L O M A \\
      +ssa    & S S A \\
      avoim+  & A V O I M \\ 
      zoom+    & T S O O M   \\
      äitiys+ & AE I T I Y S \\
      överi   & OE V E R I  \\
      über    & Y Y B E R  \\
      \hline 
    \end{tabular}
    \caption{An example of a lexicon.
    The first entries have special phonemes: spoken noise (SPN), silence (SIL) and non-spoken noise (NSN). "oov" refers to out-of-vocabulary words. 
    "+" is the intra-word boundary marker in subword vocabularies.
    Note also the inaccurate pronunciation of "zoom": "T S U U M" would be more accurate.
    }
    \label{}
  \end{table}

  

\subsubsection{Weighted finite-state transducers in Kaldi} \label{sec:wfst} 

  Weighted finite-state transducers are a type of automaton in which a transition has an input label, an output label, and a weight. A special case of FST is a finite-state acceptor (FSA) where the input and output labels of a transition are equal. FSTs, having both input and output labels, map an input sequence to an output sequence when a path is taken through it. Figure \ref{fig:wfst} displays a simple example of an FST.
  \begin{figure}[htb]
    \centering
    \includegraphics[width=9cm]{wfst}
    \caption{A weighted finite-state transducer \citep{openfst}.}
    \label{fig:wfst}
  \end{figure}
  The example transducer has three states, the initial state (0) denoted with a bold circle and final state (2) with a double circle. The arcs between states have input labels \{a, b, c\}, output labels \{x, y, z\}, and real number (floating point) weights associated with them. The final state has also a weight. This transducer would map the input sequence "ac" to the output sequence "xz" with the weight calculated by summing the individual arc weights, in this case adding up to 6.5.

  FSTs are used in Kaldi ... \textbf{training L, decoding hclg}

  Kaldi composes the $HCLG$ decoding graph from four component transducers: HMM $H$, context $C$, lexicon $L$, and LM (or grammar) $G$ \citep{povey2012wfst}. 

  The n-gram language model that is used in decoding
  % (in training, only a lexicon is needed)
  is converted from the ARPA format, described in \ref{tab:arpa}, into the $G$ transducer using the \texttt{arpa2fst} program. The purpose of $G$ is not to transduce a sequence from one domain to another, but to assign weights to the possible word sequences. For this reason, the input and output labels are equal, making it a finite-state acceptor. The ARPA model lists the conditional base-10 logarithmic probabilities for each n-gram. These give weights to the arcs of the corresponding paths in the acceptor, the arc weights having an inverse relation to the probabilities (i.e., they are negated) and using natural logarithm instead of 10-base.  Referring to the weight of decoding graph FST as "cost" is more intuitive, so in this text this term is used, too. A complete path through the acceptor corresponds to a word sequence and the cost of the path indicates how \emph{un}likely the sequence is.
  If a probability of the highest-order n-gram has not been explicitly specified  in the n-gram model, a path is taken through the backoff node which corresponds to the lower-order backoff n-grams and the costs are determined by the associated backoff probabilities. Since the model can recursively back off  all the way to unigrams, any word sequence is accepted and given a cost. The backoff arcs do not have a word label, which raises a problem there are multiple paths for a single input sequence, making processing the graph inefficient\footnote{more disadvantages?}. The backoff arcs are therefore assigned a \emph{disambiguation symbol} "\texttt{\#0}" as the input label. The disambiguation symbol allows for an operation (in practice, an algorithm) called \emph{determinisation}. A deterministic transducer has the property that one input string matches at most one path \citep{mohri2008speech}.
  The acceptor also dismisses sentence start and end tokens of the LM  (\texttt{<s>} and \texttt{</s>} or whatever they may be) since these are not wanted in the speech transcripts. The number of word types used in speech recognition can also be limited to make the decoding graph of feasible size, in which case some types are omitted from the LM acceptor.

  The lexicon transducer $L$ maps a phoneme sequence input to an output consisting of one word. If the lexicon has multiple words with the same pronunciation, or when a phoneme sequence is a part of multiple word pronunciations, word disambiguation symbols \{\texttt{\#1}, \texttt{\#2},...\} are needed to ensure each phoneme sequence has only one possible word output. Ambiguity in the transducer means that there could be multiple paths matching one input string.  
  See Section
  \ref{sec:lexicon} for discussion about orthography and Section \ref{sec:sil_prob} for discussion about the case where one word has multiple pronunciations. 

  The grammar acceptor $G$ and lexicon transducer $L$ are combined to compose a new transducer $LG$ that maps a phoneme sequence input to a word sequence output. The composition is done in the program \texttt{fsttablecompose}, and the new transducer is determinised in the program \texttt{fstdeterminizestar}.

  $C$ is the transducer from context-dependent phonemes to context-independent phonemes. 


  % , for example, in the lexicon FST (the file L.fst) to transduce a phoneme sequence to a (sub)word token sequence. A grammar or a language model is encoded in a finite-state acceptor (G.fst) since its purpose is to define the possible sequences without transducing sequences from a domain to another. 

  % Figure \ref{fig:fst} is an example of a grammar-lexicon (LG.fst) transducer. 

  % \begin{figure}
  % \caption{} \label{fig:fst}
  % \end{figure}


\subsubsection{Silence and pronunciation probability modelling} \label{sec:sil_prob} 

  The Kaldi toolkit allows also for modelling the probability of silence between specific words, and the probability of different pronunciations of a word. This has found to improve WER results by a small but consistent margin \citep{chen2015pronunciation}. 

  Differences of pronunciation are significant in many languages. However, in Finnish, which has a phonemic orthography (see Section \ref{sec:lexicon}), pronunciation probabilities are not applicable. If the mapping from phonemes to graphemes is one-to-one, there are no alternative pronunciations for a word.

  The probability of a silence phone between two words is estimated from statistics collected from alignments. 

  When using a subword vocabulary, the it is important to indicate which boundaries are actual word boundaries \citep{smit2017improved}, otherwise the silences might 


\subsubsection{Deep neural networks for acoustic modelling} \label{sec:dnn_am}

  In the previous decade, deep neural networks achieved state-of-the-art results in acoustic modelling, supplanting the Gaussian mixture models as the most accurate method to classify observations into phoneme classes. In the HMM/DNN hybrid approach, DNNs provide \emph{pseudo-likelihoods} of the observations for each HMM state. This is due to DNNs being discriminative instead of generative like GMMs.
  The approach defined in Eq. \ref{eq:asr-bayes-2} works well for traditional ASR systems that use GMMs for acoustic modelling. However, if the generative model is replaced with a discriminative neural network model, it does not produce an acoustic likelihood $P(\boldsymbol{o}_t|\boldsymbol{s}(t))$ but a state level posterior probability $P(\boldsymbol{s}(t)|\boldsymbol{o}_t)$, which is a problem because the decoding (Eq. \ref{eq:asr-bayes-2}) relies on the likelihoods. This mismatch can be bypassed by applying the Bayes' rule to produce pseudo-likelihoods:
  \begin{equation}
    P(\boldsymbol{x}_t|\boldsymbol{s}(t)) =
    \frac{P(\boldsymbol{s}(t)|\boldsymbol{x}_t)P(\boldsymbol{x}_t)} {P(\boldsymbol{s}(t))} \propto	 
    \frac{P(\boldsymbol{s}(t)|\boldsymbol{x}_t)} {P(\boldsymbol{s}(t))}
  \end{equation}
  The state priors $P(\boldsymbol{s}(t))$ can be gathered from corpus frequencies \citep{Bourlard1994}. 



  Two common types of DNNs used for acoustic modelling are RNNs and time delay neural networks (TDNN). Both RNNs and TDNNs are inherently suited to model time-series data where the previous time steps are relevant when producing the output for the current time step. (explaining RNNs and TDNNs in previous sections?)

  Currently, the state-of-the-art implementations of DNNs in Kaldi are trained using lattice-free MMI (LF-MMI) training criterion, as described by \citet{povey2016purely}. These are called "chain" models in Kaldi. LF-MMI is a sequence discriminative criterion, which means that the aim is to maximise the conditional log-likelihood (Eq. \ref{eq:mmi}) of the correct transcript on the sequence level. In the traditional MMI approach, a cross-entropy system is trained to  generate lattices for a weak language model. The lattices are used to approximate the possible word sequences for the discriminative objective function denominator. In LF-MMI, the possible word sequences are not approximated with a lattice, but a phone-level language model is computed so that the sum in the denominator is not approximated, but computed exactly. The phone LM is represented as an FST, created in a similar manner as the normal decoding FST described in Section \ref{sec:wfst}. In this case there is no lexicon and grammar FSTs but a phone grammar FST, so the graph consists of the component FSTs $H$, $C$, and $P$. The numerator FST uses a lattice to represent the utterance. The numerator FST is composed with the denominator FST so that the phoneme LM of the denominator removes illegal output sequences. The composition also ensures that the objective function value is negative. 
  % https://desh2608.github.io/2019-05-21-chain/




% \subsubsection{Speaker recognition} \label{sec:spk_id}

%   Section \ref{sec:adaptive_training} discussed the differences between speakers and how the differences are modelled in adaptive training. 



\subsection{Spoken and written conversations in Finnish} \label{sec:conv}



% \clearpage
% \section{Methods}




\clearpage
\section{Experiments}

% \subsection{Baseline} 

%   The baseline language models and the ASR system is based on the systems developed by \citet{enarvi2017automatic}.

\subsection{Acoustic modelling experiments}
\subsubsection{Speech corpora}

  The acoustic model training, development and test sets are the same as in \citep{enarvi2017automatic}. The training speech data are 85 hours of read and spontaneous speech from three different sources. The SPEECON corpus consists of 550 speakers reading 30 sentences and 30 single words as well as speaking 10 spontaneous sentences \citep {iskra2002speecon}. The DSPCON\footnote{\url{http://urn.fi/urn:nbn:fi:lb-201708251}} corpus consists of 5281 spontaneous sentences from 218 different male students and 24 female students, totalling 9.8 hours \citep{enarvi2018modeling}. The third source is the FinDialogue part of the FinINTAS corpus \citep{lennes2009segmental}. The development and test sets contain additionally spontaneous utterances from radio shows, referred to as RadioCon. Table \ref{tab:am_data} lists the acoustic modelling data sets.

  \begin{table}[ht!]
      \begin{tabularx}{\columnwidth}{|X|l|l|l|}
          \hline
          \textbf{AM corpora} & \# of speakers &\# of utterances & \# of hours \\ \hline
          DSPCON                                & 242 &     & 9.8  \\ \hline
          SPEECON                               & 550 &     &     \\ \hline
          FinDialogue                           &     &     &     \\ \hline
          total AM training data                &     &     & 85   \\ \hline
          RadioCon (devel, test sets)           &     &     &     \\ \hline
          VoxCeleb (speaker embeddings)         &     &     &     \\ \hline
      \end{tabularx}
      \caption{}
      \label{tab:am_data}
  \end{table}

  \emph{Speed perturbation} is a data augmentation method in which the speed of the audio is increased or decreased \citep{ko2015audio}. This method is used for the DNN AM training data, augmenting the speech data by changing its speed by a factor of 0.9 and 1.1, increasing the amount of data by a factor of three. As the quality, or domain, of the three different data sets is different, it could be useful to augment only the in-domain data, i.e., DSPCON. A system is trained with the whole dataset augmented, as well as one system augmenting only DSPCON. The difference


\subsubsection{Acoustic model architecture and training}

  

\subsubsection{Speaker embedding experiments}

  In the speaker embedding experiments, pretrained extractors are used as well as extractors trained on the AM training data. 
  
  \paragraph*{Pretrained VoxCeleb i-vector and x-vector extractors} 
    Pretrained i-vector and x-vector extractors trained on the VoxCeleb data \citep{nagrani2017voxceleb, chung2018voxceleb2} are used in the experiments. The VoxCeleb1 data contains about 100k utterances from 1251 celebrities and the VoxCeleb2 data contains about 1M utterances from over 6000 speakers. The code that was used for training the extractors is in the Kaldi repository\footnote{i-vectors: \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/voxceleb/v1}}\footnote{x-vectors: \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/voxceleb/v2}} and the pretrained extractors are downloaded online\footnote{\url{https://kaldi-asr.org/models/m7}}.
    
    The VoxCeleb i-vector extractor was trained on 24 MFCCs and their delta and delta-delta coefficients. An energy-based VAD system is used to select the voiced frames for both i-vector and x-vector systems. The i-vector system UBM is a GMM that has 2048 full-covariance component Gaussians. The i-vectors have 400 dimensions, and are subsequently reduced to 200 dimensions using an LDA model.

    The VoxCeleb x-vector extractor is trained on the VoxCeleb speech data that has been augmented in various ways. The MUSAN corpus \citep{snyder2015musan} of music, noise and speech as well as simulated room impulse response \citep{ko2017study} are used to generate noise for the speech data. The noise is added to the speech data and a subset of the noisy audio files is randomly selected and pooled with the clean audio files. This increases the amount of data roughly twofold. 
    The x-vector extractor was trained on 30 MFCCs with their deltas and delta-deltas. The DNN is a TDNN with ReLU non-linearities where the five first layers use frame-level training with a temporal context of a few adjacent frames. The architecure is described by \citet{snyder2017deep}. After the frame-level layers is a pooling layer that aggregates the frame-level outputs, calculating the mean and standard deviation of the whole segment. The last two layers before softmax operate on the segment level statistics. The x-vector is extracted from the penultimate layer (the 6th layer), with 512 dimensions. An LDA model reduces the x-vector dimensionality to 200.


  \paragraph*{Pretrained i-vector extractor trained on Yle and Parliament data}
    A pretrained i-vector extractor was used as a comparison to the models trained in this work.

  \paragraph*{i-vector extractor trained on the conversational Finnish data}



  Both online and offline i-vectors are extracted 


\subsection{Language modelling experiments}

\subsubsection{Text corpora}

  \begin{table}[ht!]
      \begin{tabularx}{\columnwidth}{|X|l|l|}
          \hline
          \textbf{LM training corpora} & \# of word tokens & 4-gram interpolation weight \\ \hline
          DSP conversational speech corpus transcriptions & 61k  & 0.41 \\ \hline
          WEB corpus, conversational written text corpus & 75.9M & 0.59 \\ \hline
      \end{tabularx}
      \caption{}
      \label{}
  \end{table}



\clearpage
\section{Results}

\clearpage
\section{Conclusion} 


\clearpage
\small
% for counting pages
\thesisbibliography

\bibliographystyle{apalike}
\bibliography{references}


% \begin{thebibliography}{99}

%% Alla pilkun j\"alkeen on pakotettu oikea v\"ali \<v\"alily\"onti>-merkeill\"a.
% \bibitem{Kauranen} Kauranen,\ I., Mustakallio,\ M. ja Palmgren,\ V.
%   \textit{Tutkimusraportin kirjoittamisen opas opinn\"aytety\"on
%     tekij\"oille.}  Espoo, Teknillinen korkeakoulu, 2006.

% \end{thebibliography}

%% Appendices
%% If you don't have appendices, remove \clearpage and \thesisappendix below.
% \clearpage
% \thesisappendix
% \section{}

\end{document}