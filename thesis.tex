%% License 
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %% This is licensed under the terms of the MIT license below.                 %%
  %%                                                                            %%
  %% Written by Luis R.J. Costa.                                                %%
  %% Currently developed at the Learning Services of Aalto University School of %%
  %% Electrical Engineering by Luis R.J. Costa since May 2017.                  %%
  %%                                                                            %%
  %% Copyright 2017-2018, by Luis R.J. Costa, luis.costa@aalto.fi,              %%
  %% Copyright 2017-2018 Swedish translations in aaltothesis.cls by Elisabeth   %%
  %% Nyberg, elisabeth.nyberg@aalto.fi and Henrik Wallén,                       %%
  %% henrik.wallen@aalto.fi.                                                    %%
  %% Copyright 2017-2018 Finnish documentation in the template opinnatepohja.tex%%
  %% by Perttu Puska, perttu.puska@aalto.fi, and Luis R.J. Costa.               %%
  %% Copyright 2018 English template thesistemplate.tex by Luis R.J. Costa.     %%
  %% Copyright 2018 Swedish template kandidatarbetsbotten.tex by Henrik Wallen. %%
  %%                                                                            %%
  %% Permission is hereby granted, free of charge, to any person obtaining a    %%
  %% copy of this software and associated documentation files (the "Software"), %%
  %% to deal in the Software without restriction, including without limitation  %%
  %% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
  %% and/or sell copies of the Software, and to permit persons to whom the      %%
  %% Software is furnished to do so, subject to the following conditions:       %%
  %% The above copyright notice and this permission notice shall be included in %%
  %% all copies or substantial portions of the Software.                        %%
  %% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
  %% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
  %% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
  %% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
  %% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
  %% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
  %% DEALINGS IN THE SOFTWARE.                                                  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%

%% spellcheck-on

%% Configurations, meta information, prefaces etc. 
  \documentclass[english, 12pt, a4paper, elec, utf8, a-1b, hidelinks]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, dvips, online]{aaltothesis}

  %% Use the following options in the \documentclass macro above:
  %% your school: arts, biz, chem, elec, eng, sci
  %% the character encoding scheme used by your editor: utf8, latin1
  %% thesis language: english, finnish, swedish
  %% make an archiveable PDF/A-1b or PDF/A-2b compliant file: a-1b, a-2b
  %%                    (with pdflatex, a normal pdf containing metadata is
  %%                     produced without the a-*b option)
  %% typeset in symmetric layout and blue hypertext for online publication: online
  %%            (no option is the default, resulting in a wide margin on the
  %%             binding side of the page and black hypertext)
  %% two-sided printing: twoside (default is one-sided printing)

  \usepackage{amsfonts, amssymb, amsbsy, amsmath, natbib, graphicx, 
              tabstackengine, tabularx}
  \usepackage{titlesec}
  \setcounter{secnumdepth}{4}
  \titleformat{\paragraph}
  {\normalfont\small\bfseries}{\theparagraph}{1em}{}
  \titlespacing*{\paragraph}
  {0pt}{3.25ex plus 1ex minus .2ex}{0ex plus .2ex}

  \graphicspath{ {./images/} }
  

  \DeclareMathOperator*{\argmax}{argmax}
  \DeclareMathOperator*{\tanh}{tanh}

  % \university{Aalto-yliopisto}
  % \school{Sähkötekniikan korkeakoulu}
  \degreeprogram{Computer, Communication and Information Sciences}
  \major{Signal, Speech and Language Processing}
  \code{ELEC3031}
  \univdegree{MSc}
  \thesisauthor{Anssi Moisio}

  %% A possible "and" in the title should not be the last word in the line, it
  %% begins the next line.
  %% Specify the title again without the linebreak characters in the optional
  %% argument in box brackets. This is done because the title is part of the 
  %% metadata in the pdf/a file, and the metadata cannot contain linebreaks.
  \thesistitle{Automatic recognition of conversational speech} % working title
  %\thesistitle[Title of the thesis]{Title of\\ the thesis}
  \place{Espoo}
  \date{}

  %% Thesis supervisor
  %% Note the "\" character in the title after the period and before the space
  %% and the following character string.
  %% This is because the period is not the end of a sentence after which a
  %% slightly longer space follows, but what is desired is a regular interword
  %% space.
  \supervisor{Prof.\ Mikko Kurimo}

  %% Advisor(s)---two at the most---of the thesis.
  \advisor{Dr.}

  %% Aaltologo: syntax:
  %% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
  %% The logo language is set to be the same as the thesis language.
  \uselogo{aaltoRed}{''}

  %% The English abstract:
  %% All the details (name, title, etc.) on the abstract page appear as specified
  %% above.
  %% Thesis keywords:
  %% Note! The keywords are separated using the \spc macro
  \keywords{For keywords choose\spc concepts that are\spc central to your\spc thesis}

  %% The abstract text. This text is included in the metadata of the pdf file as well
  %% as the abstract page.
  \thesisabstract{abstract here
  }

  %% Copyright text. Copyright of a work is with the creator/author of the work
  %% regardless of whether the copyright mark is explicitly in the work or not.
  %% You may, if you wish, publish your work under a Creative Commons license (see
  %% creaticecommons.org), in which case the license text must be visible in the
  %% work. Write here the copyright text you want. It is written into the metadata
  %% of the pdf file as well.
  %% Syntax:
  %% \copyrigthtext{metadata text}{text visible on the page}
  %% 
  %% In the macro below, the text written in the metadata must have a \noexpand
  %% macro before the \copyright special character, and macros (\copyright and
  %% \year here) must be separated by the \ character (space chacter) from the
  %% text that follows. The macros in the argument of the \copyrighttext macro
  %% automatically insert the year and the author's name. (Note! \ThesisAuthor is
  %% an internal macro of the aaltothesis.cls class file).
  %% Of course, the same text could have simply been written as
  %% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
  %% {Copyright \copyright{} 2018 Eddie Engineer}
  \copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
  {Copyright \copyright{} \number\year{} \ThesisAuthor}

  %% You can prevent LaTeX from writing into the xmpdata file (it contains all the 
  %% metadata to be written into the pdf file) by setting the writexmpdata switch
  %% to 'false'. This allows you to write the metadata in the correct format
  %% directly into the file thesistemplate.xmpdata.
  %\setboolean{writexmpdatafile}{false}

  %% All that is printed on paper starts here
  \begin{document}

  \pagenumbering{roman}

  %% Create the coverpage
  % \makecoverpage

  %% Typeset the copyright text.
  %% If you wish, you may leave out the copyright text from the human-readable
  %% page of the pdf file. This may seem like a attractive idea for the printed
  %% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
  %% on the page. However, the recommendation is to print this copyright text.
  % \makecopyrightpage

  %% Note that when writting your thesis in English, place the English abstract
  %% first followed by the possible Finnish or Swedish abstract.

  %% Abstract text
  %% The text in the \thesisabstract macro is stored in the macro \abstractext, so
  %% you can use the text metadata abstract directly as follows:
  % \begin{abstractpage}[english]
  %   \abstracttext{}
  % \end{abstractpage}

  % \newpage
  % %% Abstract in Finnish.
  % \thesistitle{Opinnäyteen otsikko}
  % \supervisor{Prof.}
  % \advisor{TkT}
  % \degreeprogram{Tieto-, tietoliikenne- ja informaatiotekniikka}
  % \major{Signaalin-, puheen- ja kielenkäsittely}
  % %% The keywords need not be separated by \spc now.
  % \keywords{}
  % %% Abstract text
  % \begin{abstractpage}[finnish]
  %   tiivistelmä tähän
  % \end{abstractpage}

  % %% Preface
  % %% This section is optional. Remove it if you do not want a preface.
  % \mysection{Preface}
  % preface here

  % \vspace{5cm}
  % Otaniemi, 31.8.2018

  % % \vspace{5mm}
  % % {\hfill Eddie E.\ A.\ Engineer \hspace{1cm}}

  % \newpage
  \thesistableofcontents
%%

%% Symbols and abbreviations 
  \mysection{Symbols and abbreviations}

  \subsection*{Symbols}
  \begin{tabular}{ll}
  % examples
  % $\mathbf{B}$  & magnetic flux density  \\
  % $c$              & speed of light in vacuum $\approx 3\times10^8$ [m/s]\\
  % $\omega_{\mathrm{D}}$    & Debye frequency \\
  % $\omega_{\mathrm{latt}}$ & average phonon frequency of lattice \\
  % $\uparrow$       & electron spin direction up\\
  % $\downarrow$     & electron spin direction down \\
  % /examples
  $\boldsymbol{\Delta}$                       & delta feature vector \\
  $\boldsymbol{\mu}$                          & mean vector of a GMM \\
  $\boldsymbol{\Sigma}$                       & covariance matrix of a GMM \\


  $\boldsymbol{o}_t$    & observation, i.e., feature vector, at time $t$ \\ 
  $p(.)$              & probability \\
  $P(.|.)$            & likelihood \\
  
  \end{tabular}

  \subsection*{Abbreviations}
  \begin{tabular}{ll}
  AM            & acoustic model \\
  ANN           & artificial neural network \\
  ASR           & automatic speech recognition \\
  CER           & character error rate \\
  CMN           & cepstral mean normalisation \\
  CMVN          & cepstral mean and variance normalisation \\
  DNN           & deep neural network \\
  EM            & expectation maximisation \\
  fMLLR         & feature space maximum likelihood linear regression \\
  FSA           & finite-state acceptor \\
  FST           & finite-state transducer \\
  GMM           & Gaussian mixture model \\
  GRU           & gated recurrent unit \\
  HMM           & hidden Markov model \\
  LDA           & latent Dirichlet allocation \\
  LDA           & linear discriminant analysis \\
  LF-MMI        & lattice-free MMI \\
  LM            & language model \\
  LSTM          & long short-term memory \\
  MAP           & maximum a posteriori \\
  MCE           & minimum classification error \\
  MFCC          & mel-frequency cepstral coefficient \\
  ML            & maximum likelihood \\
  MLE           & maximum likelihood estimation \\
  MLLR          & maximum likelihood linear regression \\
  MLLT          & maximum likelihood linear transformation \\
  MLP           & multilayer perceptron \\
  MMI           & maximum mutual information \\
  MPE           & minimum phone error \\
  NN            & neural network \\
  NNLM          & neural network language model \\
  OOV           & out of vocabulary \\
  PDF           & probability density function \\
  PPL           & perplexity \\
  ReLU          & rectified linear unit \\
  RNN           & recurrent neural network \\
  SAT           & speaker adaptive training \\
  SD            & speaker dependent \\
  SI            & speaker independent \\
  TDNN          & time delay neural network \\
  VAD           & voice activity detection, here synonymous with speech activity detection \\
  VT            & Viterbi training \\
  VTLN          & vocal tract length normalisation \\
  WER           & word error rate \\
  WFST          & weighted finite-state transducer \\
  \end{tabular}

  %% \clearpage is similar to \newpage, but it also flushes the floats (figures
  %% and tables).
  \cleardoublepage
  \pagenumbering{arabic}
  \setcounter{page}{1}
%%

%% Text body begins.
\section{Introduction} \label{sec:introduction}
\thispagestyle{empty} 

\subsection{The ASR task} 

  Automatic speech recognition (ASR) is the task of converting speech into text.
  The difficulty of the task depends on how varied the speech audio signals are. The restricted problem of recognising a few different words pronounced clearly by one speaker recorded in noise-free conditions was solved years ago. Speech recognition becomes more difficult when the speech is continuous and recorded in differing noise conditions from many speakers. Current state-of-the-art ASR systems are nearing the human-level recognition accuracy also in continuous speech recognition tasks if the speech is planned and pronounced clearly, as it is, for example, in broadcast news or parliament discussions. However, spontaneous, informal conversations remain a challenging type of speech to transcribe automatically, and the gap between human and machine accuracy is still very large. This thesis explores methods for improving ASR for conversational Finnish.

  % \subsection{The basic structure of an ASR system} 

  The conventional ASR system includes two main component systems: the acoustic model (AM) and the language model (LM). The LM generates an \emph{a priori} probability distribution over possible word sequences. For example, the transciption "en minä tiedä" should probably be assigned a larger probability than "en sinä tiedä" even before any speech audio is processed. The AM outputs \emph{a posteriori} probabilities for phoneme sequences based on the speech audio. A dictionary is used to map the phoneme sequences to words, or more accurately, the same grapheme units that the LM uses, which can also be subword units or characters, for instance. The probabilities of the LM and AM are combined to estimate the most likely transcription of the speech audio.

  In the past few years, end-to-end (E2E) speech recognition systems have achieved promising results. An E2E system dispenses with the division to an LM and an AM, and instead learns a mapping  from (preprocessed) audio straight to the transcription. This makes the training procedure simpler since only one model is trained instead of multiple. However, it has been shown that E2E models can still benefit from, for example, incorporating an external language model \citep{toshniwal2018comparison} or speaker embeddings \citep{rouhe2020speaker}, into an E2E system, making it arguably no longer a pure E2E model, depending on how "E2E" is defined. Results such as these indicate that pure E2E systems will not completely supplant conventional ASR systems, or systems that include multiple separately trained models, any time soon although they benefit from the simplified training procedure. The state-of-the-art results are still obtained with the conventional systems in many ASR tasks, and the thesis explores methods in this paradigm.

% 3. What will be your methodology?
\subsection{Related work and methodology}

  The purpose of the thesis is to experiment with some of the latest acoustic and language modelling methods to improve upon the previous best results obtained for an informal, spontaneous Finnish conversation speech data set. Both the speech data set used in this thesis and the previous best results are described by \citet{enarvi2017automatic}. In their work, the acoustic models are trained on 85 hours of speech using the Kaldi toolkit. A first pass of large-vocabulary decoding and word lattice generation is done using an n-gram language model trained on a conversational Finnish text corpus collected by \citet{enarvi2013studies}. A second pass of rescoring the lattices and generating transcripts is done using a recurrent neural network language model trained on the same text corpus. Subword-vocabulary language models based on statistical segmentation of words \citep{creutz2002unsupervised, creutz2007unsupervised} were found to perform better than a word vocabulary.

  The baseline system is the same in this thesis, and the work begun by replicating the previous results. The Kaldi toolkit includes acoustic model training pipelines, called "recipes", that are tuned to achieve optimal results for a particular speech data set. In the past three years after the above mentioned previous best results were achieved, the Kaldi recipes have been developed further, and the latest machine learning algorithms have been implemented in the toolkit. By applying the latest Kaldi recipes for the Finnish speech data used in this thesis, the previous best results can be improved. Other acoustic modelling experiments of this work include modelling the speaker and channel variability using i-vectors \citep{ivector} and x-vectors \citep{snyder2018x}.

  \citet{vaswani2017attention} introduced a neural network architecture for language modelling called the Transformer, which is based solely on (self-)attention mechanisms \citep{bahdanau2014neural}. Since then, the state-of-the-art language models have been of the transformer model type.
  One of the advantages of attention mechanisms is that they are able to exploit parallel computing, unlike the commonly used recurrent neural networks, since their hidden states do not depend on the hidden states of previous time steps.
  In this thesis, a Transformer-XL \citep{dai2019transformer} LM is trained and evaluated in the ASR task, in a similar manner as described by \citet{jain2020finnish}.

  Other language modelling experiments in this work include evaluating word and subword vocabularies, tuning the hyperparameters of the language models (including constant- and variable-order \citep{siivola2007morfessor} n-gram models, RNN LMs as well as Transformer-XLs), and experimenting with topic modelling (see e.g., \citet{xiong2018session, xing2016topic}).


\clearpage
\section{Machine learning} \label{sec:ml} 

  % \subsection{Statistical models} 

  %   A \emph{model} is a simplified or abstract representation of a phenomenon, an artefact that aims to hold relevant and distilled information about its real-world counterpart.
  %   A model can be statistical, conceptual, mathematical ...
  %   Model of any of these kinds, however, is ...

  %   Creating models of phenomena in an environment is the basis for being able to understand, predict, or act in the environment. Brains have evolved to build both unconscious and  consciously accessible models of consequential things, such as animals, weather, or phonemes that compose a language.
  %   % of plants, of the body that contains the brain, of weather, water etc.
  %   % representations of time, space, temperature etc. 

  %   The abstraction and simplification can happen by grouping similar things into one. For example, a mental model of a tree integrates many individual leaves into one foliage, and
  %   % , more fundamentally, 
  %   a visual model of a leaf integrates many chloroplasts and other small things into one green surface that is perceived when looking at a leaf.

  %   A model can be a function that generates an output given an input, emulating how the real-world phenomenon behaves. Inputting a knife into a coconut outputs coconut milk, and an accurate mental model of a coconut should be able to predict that a coconut will function in this manner. More generally, a model can be any kind of mapping from a domain to another.
    
  %   % ; via lowering the resolution: a mental model of a 


% \subsubsection{Estimating models statistically}

  % There are two directions, ways to create a model of a phenomenon: top-down or bottom-up. Top-down design of models is an exclusively human undertaking, and a much rarer way to build a model. A top-down designed model could be, for example, a recipe for a dish, which is then used to cook up the real thing. This way the simplified model is created without the real phenomenon existing yet, or ever. However, top-down design depends on previous models that have ultimately been estimated from bottom up. 
  % A bottom-up generation of a model is the more usual way models come to being. In this approach, the abstract model is estimated from data gathered from the manifest world. In this example, a recipe for the dish can be developed by experimenting with random recipes in some finite search space of possible recipes, tasting the finished product, and iteratively narrowing down the search space, homing in on an optimal experience. Once a cook has done enough bottom-up design, top-down design becomes possible by deducing from the mental model what a never-before-experienced combination of ingredients will taste like. In mathematics, solving a function in a top-down fashion is called an analytical or a symbolic solution, and bottom up a numerical solution. 

  % In computer science, top-down design has been the usual way to develop systems.
  % % If the function to be estimated is simple enough
  % For example, the best computer chess programs were based on rules until the first decade of this century. In recent years, however, the designing work has been more and more often delegated to an algorithm that performs bottom-up estimation of the desired behaviour. This is referred to as machine learning\footnote{More accurately, machine learning is not the only bottom-up estimation approach, an example of another one being optimisation, which is similar to machine learning but does not aim to generalise beyond the data set used. Machine learning involves optimisation algorithms but what separates it from mere optimisation is the desire to build a model that can generalise to data that was not used for building the model.}. The best chess programs no longer do decisions by following human-written if-then clauses; instead they have estimated the mapping that inputs chess board configurations and outputs chess moves by playing a large number of games against themselves and taken notes about which moves lead to winning. As the behaviour to be modelled becomes too complex and non-linear for a human to design it, the only way is to teach it to the machine by brute-force trial and error. 


\subsection{Fitness criterion} 

  The mapping from input to output is determined by the parameters $\theta$ that are learned. There are different approaches to defining how good a model is, given a training dataset. Using the maximum likelihood estimation (MLE), the parameters are adjusted to maximise how probable the model judges the data set. The maximum likelihood estimate $\theta_{\textup{ML}}$ of the model parameters is the point in the parameter space that maximises the likelihood of the observed data. The confidence of how good the point estimate is is related to its variance. If alternative samplings of the observations are likely to change the estimate, the variance is larger. In contrast to the MLE approach,
  % and frequentist statistics in general,
  the Bayesian approach is to generate a full probability distribution function for $\theta$. When the model is used to estimate the probability of an observation, the distribution is integrated over the parameter space, and the probabilities of the observations given by each parameter point estimate is weighted by how probable the parameter point is. This way the confidence on the parameter estimate is incorporated in the data likelihood. However, using a full probability distribution instead of a point estimate is computationally expensive, and therefore the MLE method is the more commonly used approach in machine learning.
  
  % incorporate the degree of certainty to the model itself

\subsection{Optimisation} 

  Depending on the criterion for an optimal model, a optimisation method is chosen and applied to tune the free parameters of a model. When the model to be estimated includes latent variables, the maximum likelihood estimate can be computed using the expectation maximisation (EM) algorithm. A latent (i.e., hidden) variable is a variable that is not directly observed, but can affect the observed variables. A general example of a latent variable is some phenomenon in the real world that is not directly observed, but whose properties are inferred from measurements that are observable. Two types of models that include latent variables are GMMs and HMMs, both of which are central to the conventional implementation methods in automatic speech recognition. In GMMs, the identity of the mixture component $c$ of a data point $o$ is a latent variable, and the probability distribution $P(o)$ is determined by the joint distribution $P(o,c)=P(o|c)P(c)$. Hidden Markov models are named after the latent, hidden sequence that generates an observed sequence\footnote{They are also named after the Russian mathematician Andrey Markov.}; in speech recognition the hidden sequence can be a sequence of phones that generate the sequence of processed audio signal chunks referred to as features.
  % 'Maximum likelihood modeling with Gaussian distributions for classification'
  % gupta2011theory

  Maximising the expectedness of the data can be turned around and seen as minimising how surprising the training data are to the model by modifying the model. Expectation maximisation is an iterative method to finding a local maximum for the likelihood of a set of data given a model.


  The EM algorithm consists of iterating the expectation step (E-step) and the maximisation step (M-step). The E-step 


  % bayesing, MAP
  % cross entropy
  % 

\subsection{Artificial neural networks} \label{sec:dnn_intro}

  Artificial neural networks (ANN) are a family of model types, historically based on the concept of a perceptron \citep{rosenblatt1957perceptron}, which functions similarly to a biological neuron, and is therefore called an artificial neuron, or just neuron.
  The core functioning, common to the perceptron and the biological neuron, is that an input either activates the output or not based on an activation function. In the case of a perceptron, the activation function is a unit step function: if the input is above a scalar threshold the output is activated. Perceptron learns a binary classifier by tuning this threshold based on information from training examples. The idea of a perceptron has since been generalised and extended into the wide range of different kinds of artificial neural networks, but the core idea of a neuron remains.
  
  A multilayer perceptron (MLP) stacks neurons in multiple layers that are connected in the forward direction from the input to the output layer. An MLP includes an input layer of neurons with linear or non-linear activation functions, one or more hidden layers, and an output layer. The layered network learns the weights of the connections through backpropagation. 
  
  An important class of ANNs are deep neural networks (DNN), called "deep" because they include many hidden layers. MLPs are a subclass of DNNs; other subclasses include, for instance, convolutional NNs (CNN) and recurrent NNs (RNN), which are particularly useful in speech recognition since they are capable of modelling long-term time dependencies in data.
  

\subsubsection{Recurrence} \label{sec:rnn} 

  Feedforward networks feed the output of a layer to the next layer until the output layer. The network output at time $t$ is therefore dependent only on the input at $t$. In natural languages, each unit of speech or text is strongly connected to the previous units. Both language models and acoustic models ought to capture dependencies across many time steps, and RNNs have been successfully applied to these tasks. In a simple RNN, the output of a layer is fed back to the same layer in the next time step, creating a type of memory of previous states of the network. Other recurrence methods are also possible, but all RNNs are based on some type of cycle between the network connections, creating a dependence of the current output on the previous outputs. This enables modelling dependencies between time steps.

  \paragraph*{Long short-term memory}
  A widely used type of recurrent NN is called a long short-term memory (LSTM) network \citep{hochreiter1997long}. The term "short-term" alludes to the activations of recurrent connections as type of memory, in contrast with "long-term" memory in the form of the weights of the connections that change by learning. The short-term memory is made longer and more expressive than in the usual RNN memory by utilising special gate layers that comprise a memory cell unit. The LSTM cell contains in total four neural network layers that perform different gating functions and interact to jointly learn which information should be memorised, i.e., which of the processed previous inputs contain the relevant \emph{context} that the next output should depend on. The gate layers are called the forget gate, the add gate, the input gate, and the output gate.

  The forget gate inputs the previous hidden state $h_{t-1}$ and the current input $x_t$ and outputs a vector that decides which elements in the previous context vector (aka cell state) $c_{t-1}$ should be kept in the memory and how completely, on a scale from 0 to 1.
  \begin{align}
    f_t &= \sigma(U_f h_{t-1} + W_f x_t) \\
    k_t &=  f_t \circ c_{t-1}
  \end{align}
  where $\circ$ represents the element-wise (also called Hadamard) product of the vectors.

  The add gate extracts from  $h_{t-1}$ and $x_t$ a new candidate context vector $\hat{c}_t$ that could be added to the context in lieu of the forgot information. The hyperbolic tangent squishes the values between -1 and 1. The input gate $i_t$ selects from $\hat{c}_t$ the new context that is added to the context.
  \begin{align}
    \hat{c}_t &= \tanh(U_c h_{t-1} + W_c x_t) \\
    i_t &= \sigma(U_i h_{t-1} + W_i x_t) \\
    j_t &= \hat{c}_t \circ i_t \\
    c_t &= k_t + j_t
  \end{align}

  The output gate $o_t$ generates the current hidden state $h_t$ from the newly computed current context $c_t$:
  \begin{align}
    o_t &= \sigma(U_o h_{t-1} + W_o x_t) \\
    h_t &= o_t \circ \tanh(c_t)
  \end{align}

  Figure \ref{fig:lstm} illustrates the LSTM cell with its four gate layers.
  \begin{figure}[htb]
    \centering
    \includegraphics[width=14cm]{lstm-cell}
    \caption{A LSTM cell.}
    \label{fig:lstm}
  \end{figure}


% \subsubsection{Convolution}

\subsubsection{Attention} \label{sec:attention}

  \citet{bahdanau2014neural} introduced a mechanism that has been widely adopted in speech and language processing systems, called \emph{attention}. 




\clearpage
\section{Finnish conversations to text} \label{sec:background} 
  The task of converting speech into text, or \emph{automatic speech recognition}, can be divided into
  % four
  subtasks: \emph{feature extraction}, \emph{acoustic modelling}, \emph{phoneme-to-grapheme mapping}, and \emph{language modelling}. The audio signal is first divided into $T$ segments, and the segments converted into feature vectors $\boldsymbol{O}=\boldsymbol{o}_1,...,\boldsymbol{o}_T$, also called observations, which are a compressed representation of the audio signal. 
  The task is then to find $\argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O})$, where $\boldsymbol{w}$ is a word sequence. This probability is not practicable to compute directly, but by Bayes' rule it can be expanded to 
  \begin{equation}\label{eq:asr-bayes}
    \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
    \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
  \end{equation}

  The probability of the observations ${P(\boldsymbol{O})}$ is not relevant in finding the best transcription (the argmax) for the observations, which leaves the product, or sum in logarithmic space,
  \begin{equation}\label{eq:asr-bayes-2}
    \argmax_{\boldsymbol{w}} P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})
    = \argmax_{\boldsymbol{w}}
      \log \{ P(\boldsymbol{w}) \} + \log \{ P(\boldsymbol{O}|\boldsymbol{w}) \}
  \end{equation}
  to be estimated. Here, the former factor is the a priori probabilities of word sequences, modelled by language models, discussed in Section \ref{sec:lm}. The acoustic model determines likelihoods of observations given phoneme sequences that are mapped to grapheme sequences by a lexicon (also called a dictionary), yielding $P(\boldsymbol{O}|\boldsymbol{w})$. Acoustic modelling is discussed in Section \ref{sec:am}.
  % The first
  % % four
  % sections of this chapter respectively describe the
  % % four
  % components of the conventional ASR system.
  Section \ref{sec:conv} discusses the specifics of recognising conversational Finnish.

  %  A language model gives an a priori probability for each word sequence: $P(\boldsymbol{w})$. The estimate $\hat{\boldsymbol{w}}$ of the best transcription given the observations is the most likely sequence 

  % \begin{equation}\label{asr-bayes-argmax}
  %   \hat{\boldsymbol{w}} = \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
  %   \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
  % \end{equation} 


\subsection{Statistical language modelling} \label{sec:lm} 



  A language model defines the probability distribution\footnote{This would be a probability mass function since word sequences are discrete but, since there is no limit to how long the sequences can be, there is an infinite set of possible word sequences.} $P(\boldsymbol{w})$, from Eq. \ref{eq:asr-bayes}, over word sequences $\boldsymbol{w} = w_1,...,w_N$.
  In statistical language modelling, the distribution  is estimated based on statistics drawn from a training corpus. The statistical approach is in contrast with linguistically motivated models of language that take into account, for example, the grammaticality of a string when determining its probability. 
  

  What constitutes the context depends on the method. The constant-order n-gram model uses a simple context of $n-1$ previous tokens. In more involved models, the context can also be of variable length and include subsequent tokens (bi-directional context).


\subsubsection{Choice of vocabulary} \label{sec:vocab} 

  A language model defines a set of units that the output sequence can include, called the vocabulary. One natural choice of vocabulary is to include in it the word types that appear in the training corpus. However, using smaller units has some benefits over a word vocabulary. If the words are segmented into smaller pieces, or all the way to individual characters, the vocabulary is smaller. This decreases computation and memory requirements for neural language models (Sections \ref{sec:rnn} and \ref{ec:attention}),
  % as well as the language model FSA (Section \ref{sec:wfst}),
  for instance. Another benefit of using a \emph{subword} vocabulary is that words absent in the training corpus can possibly be composed of the subword units, which means that the out-of-vocabulary (OOV) rate is smaller. The downside of subword vocabularies is that each sentence includes a larger number of units to be processed.
  % , which increases the computational costs for some systems, for example when 

  When segmenting words into subword units it is beneficial to 


\subsubsection{n-gram language models} \label{sec:ngram} 


  % An n-gram language model bases the prediction of the last token in a sequence of $n$ tokens on statistics gathered from a training corpus. 
  The probability of a word sequence is the product of the probabilities of the constituent words, and the probability of each word is conditional on $n-1$ previous words (i.e., the context):
  \begin{equation}\label{eq:lm}
    P(w_1,...,w_N) = \prod_{i=1}^N P(w_i|w_{i-(n-1)},...,w_{i-1})
  \end{equation}
  

  A simple 
  % (the simplest of the ones that make any sense)
  method to define the probability of an n-gram is to let the probability of each word be its normalised frequency in the context:
  \begin{equation}\label{eq:ngram}
    P(w_i|w_{i-(n-1)},...,w_{i-1}) = \frac{\textup{count}(w_{i-(n-1)},...,w_{i-1}, w_i)}{\textup{count}(w_{i-(n-1)},...,w_{i-1})}
  \end{equation}
  This is called the maximum likelihood estimate of $P(w_i|w_{i-(n-1)},...,w_{i-1})$ since it maximises the likelihood of 
  % the language model for 
  this specific training data set\citep{chen1998empirical}. As in machine learning in general, however, the aim is to use the training data to distil from it a generalising model which predicts patterns also in previously unseen data instead of building a model that maximally accounts for the training data set. 
  
  % problems aimed to be fixed by smoothing

  To improve the predictions, \emph{smoothing} can be applied to the simple occurrence counts by redistributing probability mass from the most common n-grams to the less common ones, i.e., \emph{discounting} the most frequent n-grams. Low occurrence counts become a problem especially when $n$ is high, since the number of possible n-grams increases exponentially as $n$ increases. If there is an insufficient number of examples of the n-grams of the desired order, the information of lower-order n-grams (unigrams,...,(n-1)-grams) can be used; the model can \emph{backoff} to the lower orders to make the probability distribution smoother. The lower-order n-gram scores can also be \emph{interpolated} with the scores of the n-grams of the nominal order $n$. 
  
  Backing off to lower orders revives % better  word ?
  problems that motivated using higher-order n-grams in the first place. One of them is that of two words that are equally frequent, and thus have the same unigram probability, one may have a very specific kind of context in which it almost always appears whereas the other appears in various contexts. Given a novel context, which is why backing off to unigrams is necessary, the former is statistically less probable to appear in it than the latter, but this is not captured by unigram statistics.
  \citet{kneser1995improved} introduced a smoothing method, which has become commonly used, where the number of different bigram contexts of a word correlate with the unigram backoff probability. The number of seen bigram types where the word $w$ is the latter word, i.e., the number of types of the previous token $w'$ that at least once precede $w$ in the corpus, can be expressed as $| \{ {w' : \textup{count}(w',w)>0} \} |$. This count is normalised by the number of all word types that are seen as the first word of a bigram to get the KN unigram probability:
  \begin{equation}\label{eq:continuation}
    P_{\textup{KN}}(w) = \frac{| \{ {w' : \textup{count}(w',w)>0} \} |}
                              { \sum_{w''} |\{ {w' : \count(w'w'')>0}\}|}
  \end{equation}
  For a Kneser-Ney smoothed bigram model, the unigram and bigram statistics are then interpolated:
  \begin{equation}\label{eq:kn_bigram}
    P_{\textup{KN}}(w_i|w_{i-1}) = \frac{\max(\textup{count}(w_{i-1}w_i)-d, 0)}
                                        {\textup{count}(w_{i-1})} 
                                  \lambda(w_{i-1})
                                  P_{\textup{KN}}(w_i)
  \end{equation}
  where $d$ is a discount constant, usually $0<d<1$, and $\lambda$ is a normalising factor that defines how the discounted probability mass is redistributed:
  \begin{equation}\label{eq:kn_lambda}
      \lambda(w_{i-1}) = \frac{d}
                          {\sum_{w'} \textup{count}(w_{i-1},w')  }
                          | \{ {w' : \textup{count}(w_{i-1},w')>0} \} |
  \end{equation}
  The discount is normalised by the sum of the counts of all bigrams where the $w_{i-1}$ is the first word, and the normalised discount is multiplied by the number of word types that have followed $w_{i-1}$, i.e., the number of word types that have been discounted, so that $P_{\textup{KN}}(w_i|w_{i-1})$ over all $w_i$ equals to one.

  The bigram formulation can be generalised to higher-order n-grams:
  \begin{equation}\label{eq:kn}
    P_{\textup{KN}}(w_i|w_{i-n+1}^{i-1}) = 
        \frac{\max(\textup{count}_{\textup{KN}}(w_{i-n+1}^{i-1},w_{i})
            - d, 0)}{\sum_{w'} \textup{count}_{\textup{KN}}(w_{i-n+1}^{i-1},w')} + 
        \lambda(w_{i-n+1}^{i-1})
        P_{\textup{KN}}(w_i|w_{i-n+2}^{i-1})
  \end{equation}
  where $w_{i-n+1}^{i-1}$ is the $n-1$ words before $w_i$ and $\textup{count}_{\textup{KN}}$ is the count for the highest order and the number of different words that precede the n-gram for the lower orders \citep{Jurafsky2019}.

  In the modified Kneser-Ney smoothing, introduced by \citet{chen1998empirical}, the discount constants are different for n-grams that have one, two, or more than two occurrences, changing also $\lambda$ for the distribution to still sum to one. This is motivated by empirical results suggesting that the optimal $d$ depends on the frequency.

  A common approach to making an n-gram model more efficient is to \emph{prune} n-grams that do not contribute to the 
  \citep{siivola2007growing}

  The ARPA (named after DARPA which was previously named ARPA) backoff n-gram format is used ... \footnote{\url{http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html}}


\subsubsection{Neural language models}

  n-gram language models 

  % The LSTM \ref{sec:rnn}
    

  \paragraph*{Transformers} \label{sec:attention} 

    % \citet{dai2019transformer}


\subsection{Automatic speech recognition using the Kaldi toolkit} \label{sec:am}

  Kaldi is a toolkit for automatic speech recognition, used in the ASR experiments in this study. This chapter aims to give an overview of the most relevant theoretical underpinnings of Kaldi ASR systems, as well as some of the practical details of implementing the systems.


\subsubsection{Feature extraction} \label{sec:feats} 

  A speech audio signal contains a lot of information that is irrelevant for converting the signal to text. The first step of ASR is to find the features of the signal that contain the information about what is being said. An assumption is made that the speech signal does not change meaningfully in a time frame of about 10 milliseconds so that the signal can be divided into frames with this time resolution. The frames overlap so that each frame is about 20 or 25 milliseconds, and a tapered window function, such as Hamming, is applied to (i.e., multiplied by) each frame. This window function removes the discontinuities that occur on the borders of frames, and the overlapping compensates for the tapering of the window function so that the distorting effect on the signal statistics is minimised \citet{}. % https://wiki.aalto.fi/display/ITSP/Windowing

  \paragraph*{Mel-frequency cepstral coefficients}
    The stationary frames' frequency components can be then computed with the Fourier transform. A commonly used method is to extract the MFCCs by applying a logarithmic mel-scale filterbank to the frequency spectrum, and lastly computing the DCT. The log mel-scale emphasises the lower frequencies emulating the way humans perceive sound, i.e., the way the human inner ear recognises lower frequencies with higher frequency resolution. The DCT decorrelates the coefficients so that the use of diagonal covariance matrices is possible in the subsequent stages of the modelling, namely when using GMMs to model the HMM state emissions (Section \ref{sec:phoneme_hmm}). The use of diagonal covariance matrices greatly reduces the number of free parameters, but the trade-off is that correlation between feature vector elements is not modelled.
  

  \paragraph*{Delta and delta-delta features} 
    % ?
    % The MFCC method thus assumes that coefficients adjacent in time are independent of each other, which is a false assumption in the case of speech signals.
    % \?
    An MFCC vector encodes only the stationary frequency features of a frame. However, a speech signal varies in time, and this variation carries meaning about which phones are uttered.
    Thus, it is useful to add information to the feature vectors about how the signal changes in time. 
    Information about temporal change and about change of temporal change is extracted from the MFCCs by calculating the differences and second-order differences of adjacent coefficients. These features are called the delta ($\boldsymbol{\Delta}$) and delta-delta ($\boldsymbol{\Delta\Delta}$), or acceleration, features. 
    % The $\boldsymbol{\Delta}$s and $\boldsymbol{\Delta\Delta}$s are concatenated with the MFCC vectors, increasing the feature vector length threefold.
    The delta feature vector $\boldsymbol{\Delta}_t$ corresponding to the MFCC vector $\boldsymbol{c}_t$ (or the time step of that vector) is calculated by subtracting the weighted previous vector(s) from the weighted subsequent vector(s) and normalising the sum:
    \begin{equation}
    \boldsymbol{\Delta}_t = \frac{\sum_{\theta=1}^{\Theta}
    \theta(\boldsymbol{c}_{t+\theta}-\boldsymbol{c}_{t-\theta})}{2\sum_{\theta=1}^{\Theta}\theta^2}
    \end{equation}
    In Kaldi, the default window length $\boldsymbol{\Theta}$ is 2, so the $\boldsymbol{\Delta}$s are computed by multiplying the MFCCs with a sliding window of values $[-2,-1,0,1,2]$ and then normalising by dividing by $2*(1^2 + 2^2)= 10$ \citep{kaldi}. The $\boldsymbol{\Delta\Delta}$s are computed by applying the same method to the $\boldsymbol{\Delta}$ features. The first and last MFCCs are replicated to fill the window \citep{htkbook}.


  \paragraph*{Cepstral mean and variance normalisation}

    The cepstral mean normalisation (CMN) \citep{rosenberg1994cepstral} and cepstral mean and variance normalisation (CMVN) \citep{viikki1998cepstral} are methods to make the features more useful in noisy conditions. In these techniques, the MFCC feature vectors are normalised to have a zero mean, and in CMVN unit variance, over a sliding finite segment. After the normalisation, clean and noisy MFCCs are more similar, which mitigates the performance reduction caused by noisy environments.
    
    The variance of the MFCCs of a noisy speech signal is generally lower than the variance of those of a clean signal. By requiring the variance be constantly unity, noisy and clean speech MFCCs resemble each other more closely. Similarly, when noise is added to a signal, the mean changes, and by requiring the mean to be zero the characteristics of clean and noisy signals become more alike. Normalising variability between the speech signals is in general important in training an ASR system that ought to recognise different types of speech by different speakers in different recording conditions.
    
    The topic of handling meaningless variability, i.e., variability that does not contribute to the phoneme content of the utterance, between the utterances is revisited in Section \ref{sec:adaptive_training} which describes speaker adaptive training.



  \paragraph*{Dimensionality reduction and feature-space transforms}

    Features can be compressed by a dimensionality reduction method. Two common methods for this are the principal component analysis (PCA) \citep{pearson1901liii} and linear discriminant analysis (LDA), also called Fisher discriminant analysis \citep{martinez2001pca}.
    % The general idea behind PCA is that a feature vector contains interdependencies between its elements, which means there is redundant information that can be lessened by making the elements (i.e., dimensions) more independent. 
    The general idea of these methods is to find a linear combination of variables that best explain the observations.
    PCA achieves this by performing an orthogonal transformation that transforms the data matrix into a space where the dimensions, called the principal components, are ordered by their variances in decreasing order. After this, dimensionality reduction is achieved by pruning the dimensions with the lowest variance since they contribute the least to the information content of the features. LDA, on the other hand, searches for those dimensions that best discriminate between classes. For LDA, labelled training data is needed. 


    % MLLT and LDA \citep{somervuo2003feature, pylkkonen2006lda} transforms 
    
    MLLT is a feature orthogonalizing transform that makes the features more accurately modeled by diagonal-covariance Gaussians
    % ->Improved feature processing for Deep Neural Networks

    % The intuition behind LDA is that defining new axes that maximise variance between speakers makes discriminating easier.

  \paragraph*{Differences between GMM and DNN input}

    GMMs are sensitive to the number of dimensions of the feature vectors: even a small increase in the vector length will increase the number of GMM parameters substantially. The usual number of dimensions used with GMMs is about 40. With DNNs, however, the input vector determines only the width of the input layer--widths of the other layers are not constrained by the dimensionality of the input features. Therefore, DNNs can learn to use longer feature vectors, and often the used number of dimensions is a few hundred \citep{rath2013improved}.

    % frame splicing


\subsubsection{Modelling phonemes with hidden Markov models} \label{sec:phoneme_hmm} 

  Estimating the likelihoods of observations given phonemes is achieved by creating a HMM for each phoneme. 
  The phoneme-specific HMMs
  generate likelihoods of the observed sequences which can be used to map observations to phonemes. This way the task becomes to estimate the parameters of the HMMs so that each of them models the associated phoneme as accurately as possible.
  % In the conventional ASR system, used also in this thesis, phonemes are modelled by HMMs, which are then concatenated to model utterances.

  A hidden Markov model consists of a hidden Markov chain, also called regime, and the observation sequence, i.e., feature vectors. Each observation $\boldsymbol{o}_t$ has a probability $b_i(\boldsymbol{o}_t)$ of being generated when a hidden state $i$ is entered. In other words, the observation is a probabilistic function of the hidden state. 
  % Gaussian mixtures
  A state's emission probabilities are represented by a PDF, typically a mixture of multivariate Gaussian densities
  \begin{equation}\label{}
    b_i(\boldsymbol{o}_t) = \sum^{M_j}_{m=1}c_{jm}
      \mathcal{N}(\boldsymbol{o}_t ; \boldsymbol{\mu}_{jm}, \boldsymbol{\Sigma}_{jm})
  \end{equation}
  where $\boldsymbol{\mu}_{jm}$ is the mean vector, $\boldsymbol{\Sigma}_{jm}$ is the covariance matrix and $c_{jm}$ is mixture weight for mixture component $m$ in state $j$. The Gaussian mixture density is
  \begin{equation}\label{}
    \mathcal{N}(\boldsymbol{o} ; \boldsymbol{\mu}, \boldsymbol{\Sigma})
       = \frac{1}{\sqrt{(2 \pi)^n |\boldsymbol{\Sigma}|}} 
      e^{-\frac{1}{2}(\boldsymbol{o} - \boldsymbol{\mu})^\top 
      \boldsymbol{\Sigma}^{-1} 
      (\boldsymbol{o} - \boldsymbol{\mu})}
  \end{equation}
  Because a mixture of Gaussians can assume arbitrary shapes, they can model non-Gaussian phenomena; no restricting assumption is made about the shape of the PDF when a GMM is used.

  HMMs are used to model sequences, but an observation is independent of past observations. Instead, the regime has a memory, although the shortest possible: the probability of being a certain state in the next time step depends only on the current state and not the previous states. This independence of the previous transitions is called the Markov assumption. Each hidden state pair, represented in Kaldi by an arc from a state to another, is associated with a transition probability $a_{ij}$ that describes how probable it is to move from state $i$ to state $j$. 

  All in all, a HMM is defined by the set of states $S=s_1,s_2,...s_N$, the transition probability matrix $A=a_{11},...,a_{ij},...,a_{NN}$, the emission probabilities $B=b_i(\boldsymbol{o}_t)$, and the initial probability distribution $\pi=\pi_1,\pi_2,...\pi_N$ that models the probability of a state being the first state in the hidden sequence.
  
  % The observations $\boldsymbol{O}=\boldsymbol{o}_1,\boldsymbol{o}_2,...,\boldsymbol{o}_T$ 

  % in Kaldi 
  % http://kaldi-asr.org/doc/hmm.html
  The typical HMM topology for a phoneme is a left-to-right model, also called the Bakis model, with three emitting states that each have a transition to the next state and a self-loop. The model also includes a fourth non-emitting final state that has no outbound transitions. However, in the Kaldi chain models (see Section \ref{sec:dnn_am}), the topology is reduced to have only one emitting state due to a lower time resolution used.
  In Kaldi, the phoneme topology is defined in the lang/topo file.
  
  
  After initialising a HMM for each phoneme, the parameters, i.e. means, covariances and mixture weights need to be estimated in the process referred to as "training" of the model.


\subsubsection{Finding the maximum likelihood hidden state sequence and training HMM/GMM AMs} \label{sec:hmm_est} 

  The speech recognition system is trained in a supervised manner, meaning that the training data consists of a parallel corpus of speech audio and corresponding correct transcription. However, the task of the acoustic model is not as straightforward as finding a label for an input vector\footnote{In End-to-end ASR the whole ASR task is simplified to outputting an arbitrary length grapheme sequence given the sequence of observed features}. The reference transcription can be of arbitrary length but the AM is required to map each observation to a HMM state, generating an equal-length \emph{alignment} of observations and states. The states correspond to phonemes, so an alignment can be mapped to a transcription of the audio, given a lexicon that maps the phoneme sequences to word sequences.

  ML is one criterion of determining the best HMM parameters; others include the MMI criterion which is discussed in Section \ref{sec:mmi}. Section \ref{sec:ml} was a brief discussion about different fitness criteria and optimisation algorithms. It was noted there that for finding the maximum likelihood estimate of a model with latent variables, expectation maximisation is a commonly used optimisation algorithm.

  \paragraph*{The Baum-Welch algorithm}
  The Baum-Welch algorithm is an expectation maximisation algorithm for estimating the HMM parameters.  Here, the task is to maximise the likelihood $P(\boldsymbol{O}|M)$ of the observations $\boldsymbol{O}$  given the parameters of
  the HMM $M$. % here all HMMs, viterbi: only the best ?
  If the HMM had only one state $j$, the maximum likelihood estimate $\hat{\boldsymbol{\mu}}_j$ would simply be the average of the observations, and $\hat{\boldsymbol{\Sigma}}_j$, too, could be determined directly, using the covariance definition.
  In practice, there are many states which is why the parameters need to be estimated numerically, iteratively. 
  However, the initial parameter values can be taken from simple statistics of the observations. Initially, the observations are divided equally between the states and the means and variances of the states are taken from the average values.

  The maximum likelihood estimates for the parameters are 
  \begin{equation}\label{eq:mu_hat}
    \hat{\boldsymbol{\mu}}_j = \frac{\sum^T_{t=1}L_j(t)\boldsymbol{o}_t}
      {\sum^T_{t=1}L_j(t)}
  \end{equation}
  and
  \begin{equation}\label{eq:sigma_hat}
    \hat{\boldsymbol{\Sigma}}_j = \frac{\sum^T_{t=1}L_j(t)
      (\boldsymbol{o}_t - \boldsymbol{\mu}_j)
      (\boldsymbol{o}_t - \boldsymbol{\mu}_j)^\top }
      {\sum^T_{t=1}L_j(t)}
  \end{equation}
  The numerator and denominator sums for both parameter groups are \emph{accumulated} from the observations. This is the M-step in this expectation maximisation algorithm. The E-step includes finding the optimal alignment given the current HMM parameters. This is achieved by the forward-backward algorithm.

  % kaldi: gmm-init, HTK: Hinit

    % 
  
  % The training starts with some trial values of the parameters, for which

  % , for which the likelihood is calculated, given each training example.
  
  The state occupation probability
  \begin{equation}\label{eq:occ}
    L_j(t) = P(x(t)=j|\boldsymbol{O},M),
  \end{equation}
  i.e., the probability of being in state $j$ at time $t$, is calculated using using the forward-backward algorithm. The forward probability $\alpha_j(t)$ and backward probability $\beta_j(t)$ are defined as 
  \begin{align}
    \alpha_j(t) &= P(\boldsymbol{o}_1,...,\boldsymbol{o}_t,x(t)=j|M)  \label{eq:forward} \\
    \beta_j(t) &= P(\boldsymbol{o}_{t+1},...,\boldsymbol{o}_T|x(t)=j,M) \label{eq:back}
  \end{align}
  Spelled out, $\alpha_j(t)$ is the probability of the partial observation sequence up to time $t$ and that $M$ is in state $j$ at the time step. The backward probability is the probability of the partial observation sequence at the subsequent time steps up to the last vector, given that at the current time the model state is $j$. 
  The forward probability is a joint probability of the observations and the state, whereas the backward probability of the observations is conditional on the state. This allows for the state occupation probability to be determined by the product of the forward and backward probabilities (from Eqs. \ref{eq:occ}, \ref{eq:forward}, and \ref{eq:back})
  \begin{align}\label{}
    L_j(t) = \frac{ \alpha_j(t) \beta_j(t) }{P(\boldsymbol{O}|M)}
  \end{align}
  
  $\alpha$ and $\beta$ are calculated respectively using the recursions
  \begin{align}
    \alpha_j(t) &= \bigg[ \sum^{N-1}_{i=2}\alpha_i(t-1)a_{ij} \bigg] b_j(\boldsymbol{o}_t)
    \label{eq:alpha_recurs} \\
    \beta_i(t) &= \sum^{N-1}_{j=2} \beta_j(t+1) a_{ij} b_j(\boldsymbol{o}_{t+1}) 
    \label{eq:beta_recurs}
  \end{align}
  and the initial conditions,
  \begin{align}
    \alpha_1(1) &= 1, \; \; \; \; \alpha_j(1) = a_{ij} b_j(\boldsymbol{o}_1) 
    \label{eq:alpha_init} \\
    \beta_i(T) &= a_{iN} \label{eq:beta_init}
  \end{align}
  for $1<j<N$ and the final conditions
  \begin{align}
    \alpha_N(T) &= \sum^{N-1}_{i=2}\alpha_i(T)a_{iN} \label{eq:afinal} \\
    \beta_1(1) &= \sum^{N-1}_{j=2} a_{1j} b_j(\boldsymbol{o}_1) \beta_j(1) \label{eq:bfinal}
  \end{align}
  where the limits of the sums exclude the states $1$ and $N$ because they are non-emitting. The recursion of Eq. \ref{eq:alpha_recurs} calculates the forward probabilities (of seeing the specified observations and being at the state $j$) by summing all possible forward probabilities for all possible predecessor states $i$ weighted by the transition probability $a_{ij}$.

  From Eqs. \ref{eq:forward}, \ref{eq:afinal}, and \ref{eq:bfinal} it follows that calculating the forward probability also yields the total likelihood $P(\boldsymbol{O}|M)=\alpha_N(T)$.
  % By calculating the forward probability the best hidden path can be determined. The forward-backward algorithm is therefore the E-step that finds the optimal alignment given the HMM parameters.
  % The M-step consists of accumulating the averages from training examples

  \paragraph*{Viterbi training}
  An alternative approach to the Baum-Welch algorithm is an iterative procedure called Viterbi training (VT), also called Viterbi extraction or Baum-Viterbi algorithm since it involves the Baum re-estimation (Eqs. \ref{eq:mu_hat} and \ref{eq:sigma_hat}) and the Viterbi algorithm \citep{lember2008adjusted}. Instead of maximising the likelihood of all the data as in Eq. \ref{eq:alpha_recurs}, in VT the probability of only the most likely hidden sequence is maximised
  \begin{equation}
    \phi_N(T) = \max_i \{ \phi_i(T)a_{iN} \}
  \end{equation}
  for $1<i<N$ where
  \begin{equation}
    \phi_j(t) = \max_i \{ \phi_i(t-1)a_{ij} \} b_j(\boldsymbol{o}_t)
  \end{equation}
  and initially
  \begin{align}
    &\phi_1(1) = 1 \\
    &\phi_j(1) = a_{1j}b_j(\boldsymbol{o}_t).
  \end{align}
  for $1<j<N$. This alignment process finds an arc $ij$ for each observation $\boldsymbol{o}_t$.
  % so that the last 

  This results in an approximation of the maximum likelihood estimate, which was computed in the Baum-Welch algorithm. The approximation is convenient, since using only the best hidden sequence for updating the HMM parameters makes the Viterbi training is computationally less expensive than the Baum-Welch algorithm. Viterbi training is used in the Kaldi toolkit for estimating the HMM/GMM acoustic models in the standard recipes.

  % In Viterbi training, the HMM parameters and the most probable hidden state sequence, both unknown, are estimated alternately. After updating the HMM parameters, the training data observations are aligned with the states. The new alignment is then used for estimating the HMM parameters again, and so on. 
  % Within each state, a further alignment is made to align observations with mixture components. 
  % Using the Viterbi algorithm, the states are aligned with the observation sequence by maximising



  % AS stated in the Kaldi documentation, computed alignment is a sequence of transitions that corresponds to an utterance. 


\subsubsection{A discriminative training criterion: MMI} \label{sec:mmi} 

  % The maximum likelihood method aims to  
  % Typically?
  % Discriminative training is based on an objective function which is minimised or maximised using an optimisation algorithm.

  The MLE method described in Section \ref{sec:hmm_est} aims to maximise the likelihood of the observed sequence given the most probable HMM in Viterbi training, or all the possible HMMs in Baum-Welch.
  % This method results in a model that predicts the training data by determining a probability distribution over the feature space, i.e., over all possible observations. This is called a generative model since the distribution can be sampled for predictions. However, the ability to produce examples of the modelled data is redundant in classification tasks.
  % Furthermore, t
  The MLE method maximises the likelihood of the observations for all of the competing HMMs independently of each other. However, the ultimate aim is to find the HMM that most accurately models the observation sequence, so it would make sense to also try to find meaningful differences between HMMs.
  % The HMMs competing for being the most accurate model of the observations are not compared to each other.
  In discriminative training, instead of maximising the likelihood of the data given the
  % generative
  model, a model is trained to discriminate between the classes, which in this case are different phoneme sequences, corresponding to the HMMs. This way, in principle, more of the model capacity is used to model the boundaries between different HMMs, instead of using it to model just the relations between individual HMMs and alignments. 

  The discriminative objective function can be simply the difference between the correct classifications (e.g., a phoneme sequence) for a set of examples and the classifications assigned to them by the model. This is called the minimum classification error (MCE) criterion \citep{juang1997minimum}.
  Another type of objective function is the maximum mutual information (MMI) \citep{bahl1986maximum} criterion
  \begin{equation} \label{eq:mmi}
    \mathcal{F}_{\textup{MMI}}(M) = \sum_{r=1}^R \log 
      \frac{P(\boldsymbol{w}_r)P(\boldsymbol{O}_r|\boldsymbol{w}_r)}
        {\sum_{\boldsymbol{w}} P(\boldsymbol{w})P(\boldsymbol{O}_r|\boldsymbol{w})}
  \end{equation}
  where $\boldsymbol{w}_r$ is the correct transcription for the $r$'th speech file \citep{povey2005discriminative}. The numerator is the log-probability of the output sequence, and the denominator is the log-probability of all possible output sequences. This way the probability of a particular sequence is normalised by the probability of all sequences. In other words, the probability of all possible sequences is \emph{minimised}, while maximising the probability of the correct output sequence. Since the correct sequence is included also in the denominator, the maximum value of the objective function is zero.
  % equivalent to conditional maximum likelihood 
  % extended b-w

  Optimisation w.r.t the MMI criterion is achieved by the extended Baum-Welch (EBW) algorithm. The Gaussian parameter updating formulas are reminiscent of the Baum-Welch updating formulas (Eqs. \ref{eq:mu_hat} and \ref{eq:sigma_hat}), whence the name. The EBW is described for example by \citet{jiang2010discriminative}.
  % woodland2002large

  % pylkkonen2012analysis
  
  % vesely2013sequence

  % Traditionally this has been done by training a cross-entropy system, generating word lattices with a weak language model, and using these lattices as an approx- imation for all possible word sequences in the discriminative objective function– as was done when Gaussian Mixture Mod- els were the state of the art povey2016purely


\subsubsection{Phone context, state tying, and phonetic decision trees} \label{sec:triphone}
  Phones of the same phoneme sound different when flanked by different phonemes. For this reason, contextual information is modelled, too, by taking into account the preceding and subsequent phonemes of the modelled phoneme. These phonemes with left and right context of one are called triphones, and each can be assigned a HMM \citep{schwartz1985context}.

  % The monophone models are first trained
  % (with single-component Gaussians?)
  % and
  % The triphone models are initialised with the monophone model set parameters. The number of Gaussians in the triphone models is increased gradually and the parameters are re-estimated.

  % transition modelling
  % The transition probabilities from a state to another are essentially the counts of the transitions seen in the training corpus.

  % tying
  When considering triphones instead of monophones, the number of possible phonemes increases to the power of three. This means each class of triphone will include fewer instances in the training data, which brings difficulties in estimating the state output PDFs. To alleviate the data sparseness, the states of the phoneme HMMs are can be tied together so that the parameters of the output distributions of those states are shared. This makes the estimation of the parameters more robust because there are more training data occurrences, and also makes the total system more compact with fewer parameters \citep{young1992general}. States are clustered based on a chosen metric of similarity.  

  % http://kaldi-asr.org/doc/tree_externals.html
  In tree-based clustering as described by \citet{young1994tree}, the states are divided into branches in a top-down optimisation procedure. Starting from the root node, the question that maximises the likelihood is selected for the node, with the data on each side of the divide being modelled by a single Gaussian.
  In phonetic decision trees the questions are about the context of the phone, e.g. "Is the phone on the left of the current phone a fricative?". 
  After the procedure, the leaves of the tree are the state clusters in which the states are tied. In the final stage, leaves can be merged if the likelihood does not decrease more than a threshold value.

  After a tree has been constructed for the states of the triphone models, also previously unseen triphones can be synthesised by traversing the tree to the appropriate leaf node, i.e. cluster, by answering the questions about that triphone's context and using the tied states of that cluster.


\subsubsection{Speaker adaptive training} \label{sec:adaptive_training} 
  % alignments % lda + mllt
  % https://kaldi-asr.org/doc/transform.html

  % acoustic normalisation = feature space

  Variability between speakers poses a challenge to an ASR system. Each speaker may have an idiosyncratic voice, distinct style of pronunciation as well as distinct recording conditions, which can degrade the ASR performance, as the training speech set and test speech set differ from each other. This section describes some of the methods to account for inter-speaker variability by adapting either the model or the features to a particular speaker. An exhaustive overview of all the used methods is beyond the scope of this thesis.

  The adaptation can be done either in testing or training, usually respectively referred to as \emph{speaker adaptation} and \emph{speaker adaptive training} (SAT). Speaker adaptation can be done by modifying a speaker-independent (SI) model to create personal, speaker-dependent (SD) models for each speaker.  This can be done by taking a small number of speech data from the speaker and using this adapt a SI model to the specific speaker \citep{shinoda2011speaker}. In SAT, speaker-specific information is incorporated in the training of the model. When a model is trained using SAT, it naturally benefits to use an adaptation scheme also in testing. Both GMM- and DNN-based AMs have been shown to benefit from speaker adaptation as well as SAT. A commonly used SAT method for GMMs is the feature-space MLLR. For DNNs, speaker embeddings can be appended to the speech feature vectors. These methods are described in this section.

  % Cepstral mean and variance normalisation, mentioned in Section \ref{sec:feats}, mitigates the problem of different noise conditions of speech signals. 
  
  % One solution to the problem is \emph{speaker adaptation}, which can be done by modifying a speaker-independent (SI) model to create personal, speaker-dependent (SD) models for each speaker.  This can be done by taking a small number of speech data from the speaker and using this adapt a SI model to the specific speaker. However, this requires data from the speaker and re-estimating the parameters of the model to conform to the speaker.
  % Instead  of adapting the model parameters, speaker adaptation can be done also transforming the features of the test speaker \citep{}.
  
  % Often the the aim is to build a  system that recognises speech also from completely new speakers.
  % Speaker-dependent systems can be adapted from speaker-independent systems by a transformation  
  % The solution to speaker variability discussed in this section is to learn an explicit representation for each speaker's idiosyncratic qualities which is then used in addition to the speaker-independent (SI) model in the training. Because the speaker variabilities are learned by separate models, the SI model encodes, in theory, only the information needed for discriminating between phonemes, and is therefore better able to generalise to new speakers.
  % This is called speaker-adaptive training, or SAT for short.  
  SAT requires identifying each speaker from the metadata that indicates the speaker ID. Adaptation can be useful even if the speaker-identifying metadata is absent, in which case a separate adaptation is learned for each utterance, as if each utterance were spoken by a different person. In this case, the adaptation is for inter-utterance variability in general, so speaker-adaptive training becomes a misnomer. Furthermore, the adaptation can, of course, be done w.r.t any attribute of the data that has been labelled in the metadata, and thus these methods could be termed more generally \emph{attribute-aware training} \citep{rownicka2019embeddings}.

  With a HMM/GMM acoustic model, speaker adaptive training can be done by representing each speaker's distinct qualities as a transform in the feature space or in the model space. A feature space transform is applied on the observation vectors, and model space transform on the mean and variance of the GMM. Furthermore, a model space transform can be unconstrained or constrained, the former being separate transforms for the means and variances and the latter using the same transform for both \citep{gales1998maximum}.

  Using the model space transform, the aim is to learn the optimal model $\boldsymbol{M}$ and adaptation $\boldsymbol{G}^{(r)}$ for speaker $r$
  \begin{equation}
    (\hat{\boldsymbol{M}}, \hat{\boldsymbol{G}}) =
        \argmax_{(\boldsymbol{M},\boldsymbol{G})}} \prod^R_{r=1}
        P(\boldsymbol{O}^{(r)}; \boldsymbol{G}^{(r)}(\boldsymbol{M}))
  \end{equation}
  where $\hat{\boldsymbol{M}}$ are the model parameters \citep{anastasakos1996compact}.
  The feature space transform is analogous, only transforming the observations instead of the model.

  A common SAT transform is the maximum likelihood linear regression where $\boldsymbol{G}(\boldsymbol{M})$ is an affine\footnote{\emph{Linear} regression is thus a slight misnomer.} transformation defined by the matrix $\boldsymbol{A}$ and bias $\boldsymbol{b}$ 
  \begin{equation}
    \boldsymbol{\mu}^{(r)} = \boldsymbol{A}^{(r)}\boldsymbol{\mu}+\boldsymbol{b}^{(r)}
  \end{equation}
  Whether an adaptation modifies the model or the features is in some cases only a question of interpretation when describing the method, and a question of choosing among two equivalent implementations. The constrained MLLR (CMLLR) can be represented as a feature space transform \citep{gales1998maximum}, and is therefore also called feature-space MLLR or fMLLR. The transformation projects the features from the speaker-specific space to the speaker-normalised space.

  In practice, in the Kaldi implementation, affine transform is applied by appending a 1 to the feature vector, and multiplying it with the linear transform $\boldsymbol{A}$ concatenated with the constant offset (bias) $\boldsymbol{b}$: $\bracketVectorstack{  \boldsymbol{A} ; \boldsymbol{b}} \bracketVectorstack{\boldsymbol{o} \\ 1}$. The Kaldi program \texttt{transform-feats} is used to multiply the feature vectors with transform matrices.
  % \citep{povey2008fast}
  \newline

  As well as GMM parameters, also DNN acoustic models (see Section \ref{sec:dnn_am}) can be estimated speaker-adaptively. Adapting can be performed in the feature-space by appending or transforming the observations or in the model-space by modifying the DNN AM parameters.
  
  A common method is to extract i-vectors from speakers \citep{ivector}, optionally perform a transformation of the i-vectors using a control network, and append them to the features that are fed to the DNN \citep{miao2015speaker}. i-vectors have usually a hundred or a few hundred dimensions that encode properties of a speaker as well as the environment, enabling the AM to generalise more robustly to speech in different conditions. The original paper by \citet{ivector} referred to the i-vector as the \emph{total factors} $\boldsymbol{w}$ in the \emph{total variability space} $\boldsymbol{T}$ because it models both speaker and channel variability in contrast with joint factor analysis \citep{kenny2005joint} which makes a distinction between the two sources of variability. 
  
  The total variability space $\boldsymbol{T}$ models the variability between utterances, and it is also referred to as the i-vector extractor \citep{alam2014use}. i-vector extractor training starts by estimating a speaker-independent GMM called an \emph{universal background model}, or UBM. The purpose of the UBM is to represent the general, or universal, characteristics of speech, i.e., it is the speaker-independent model. Baum-Welch statistics are obtained from the UBM at the frame level. The i-vector is then defined as the mean vector of the posterior Gaussian distribution conditioned on the Baum-Welch statistics for a given utterance \citep{ivector}. In practice it is a MAP estimate \citep{kenny2005eigenvoice}. The mean vector (i-vector) is actually a concatenation of the mean vectors of the mixture components, called a supervector \citep{campbell2006svm}. A speaker-specific utterance supervector $\boldsymbol{M}$ is composed of the factors
  \begin{equation}
    \boldsymbol{M} = \boldsymbol{m} + \boldsymbol{T} \boldsymbol{w}
  \end{equation}
  where $\boldsymbol{m}$ is the UBM supervector.  The extractor compresses the high-dimensional statistics from the UBM into the dense i-vector representation for a given utterance. 

  Another approach to modelling speaker characteristics is to train a feedforward deep neural network to project speakers into an embedding space that models the speaker variance. \citet{snyder2017deep} introduced a \emph{speaker embedding} method  where the DNN is trained to classify speakers from variable-length segments. The DNN learns to assign each speaker an embedding space vector which can be then used in the AM training similarly to an i-vector.
  \citet{snyder2018x} describe the use of speaker embeddings, which they call x-vectors, in speaker  recognition. In contrast with i-vectors, x-vectors model only the speaker characteristics since the DNN is trained to identify speakers. 

  Speaker embeddings can be extracted either online or offline. This refers to which frames can be used in the extraction. When streamed audio is transcribed on the fly, the transcribed utterance is partial: only the frames up to time $t$ can be be used in the embedding instead of the complete utterance. Online decoding is simulated in the standard recipes in Kaldi, and the embeddings are extracted using partial utterances every 10 frames or so. In online extraction, the features are carried over from previous utterances of the same speaker.
  Offline extraction can be used when there is no need to stream the features or you do not want to simulate this application). Offline extraction is standard when using the embeddings in speaker recognition instead of speech recognition.


  % In decoding, i-vectors can be extracted, for example, every 10 frames, and an utterance-level i-vector can be obtained by averaging the frame-level i-vectors across the utterance.
  
  

  %%%%  
    % A speaker-independent (SI)!!!!!!!!!!! ASR system can be trained on and transcribe the speech of different speakers, in different recording conditions, 
    % % about different topics,
    % and with other distinct attributes. When these speech audio attributes are known, and in some way consistent, they can be taken into account in the training of and decoding with the system.
    
    % This section discusses how an ASR system training can be improved by  and learning to adapt the general acoustic model to each speaker.
    
    % The attributes that are distinct but consistent for each speaker include obviously the qualities of the speaker's voice, but also the recording conditions since it can be assumed that these stay constant per speaker. Alternatively, adaptation can be performed for each environment \citep{}.
    
    % The idea can also be generalised to any other attributes of the speech which modulates the speech in some consistent manner, for example the gender of the speaker. 

    % For each speaker, each feature vector is transformed by multiplying it by a matrix that encodes information about how this speaker differs from the general speaker.
    
    % The transform can be done when testing the final model (speaker adaptation, discussed in Section \ref{sec:adaptation}) and in the training process. The latter is called adaptive training.
    
    % First, a transform is generated for each speaker. 


\subsubsection{Mapping words to phoneme sequences} \label{sec:lexicon} 

  The hidden state sequence decoded from a HMM corresponds to a phoneme sequence, but the ultimate aim is to generate a word sequence for a given observation sequence. For mapping words to phoneme sequences, the ASR system applies a \emph{lexicon}, also referred to as \emph{pronouncing dictionary}, or just \emph{dictionary}. 

  In Finnish there is generally a one-to-one mapping from letters to phonemes. In the jargon of linguistics, Finnish has a \emph{phonemic orthography}. This makes creating a lexicon very simple, as each letter can be assigned to a phoneme, and the word-to-phonemes mapping follows trivially.

  A lexicon for a language like English has to be constructed largely by hand, as most of the mapping from letters to phonemes does not have a sound
  % \footnote{Ignore the pun, please.}
  logic behind it. This is due to historical change in the pronunciations of words that was not translated into corresponding change in their written forms (e.g., during the Great Vowel Shift) as well as loan words from other languages which have different rules of orthography \citep{english}.
  
  Similar change can be observed in Finnish as words are imported from other languages, mainly from English. Loan words often do not follow the Finnish orthography, which means that the one-to-one mapping from letters to phonemes is no longer accurate for these words. For example, "googlata" is pronounced "G U U G L A T A"\footnote{These are not official phonetic alphabet (they are the phoneme symbols that are used in this thesis) but hopefully the example still makes sense.} instead of following the Finnish orthographic rules to pronounce it "G O O G L A T A", or alternatively spelling it "guuglata". In this study these inaccuracies are ignored, and the Finnish lexicon is generated automatically using the simple letter-to-phoneme correspondence, where a letter correspond to a single phoneme, usually denoted by itself (with a few exceptions: e.g., "c"->"K", "q"->"K V"). Table \ref{tab:lexicon} is an example of a few lexicon entries that follow the word-to-phoneme mapping used in the experiments of this thesis.

  \begin{table}[htb]
    \centering
      \begin{tabular}{|ll|}
      \hline
      {[oov]}   & SPN \\
      !SIL    & SIL \\
      {[laugh]} & SPN \\
      {[reject]} & NSN \\
      +i+     & I \\
      +loma   & L O M A \\
      +ssa    & S S A \\
      avoim+  & A V O I M \\ 
      zoom+    & T S O O M   \\
      äitiys+ & AE I T I Y S \\
      överi   & OE V E R I  \\
      über    & Y Y B E R  \\
      \hline 
    \end{tabular}
    \caption{An example of a lexicon.
    The first entries have special phonemes: spoken noise (SPN), silence (SIL) and non-spoken noise (NSN). "oov" refers to out-of-vocabulary words. 
    "+" is the intra-word boundary marker in subword vocabularies.
    Note also the inaccurate pronunciation of "zoom": "T S U U M" would be more accurate.
    }
    \label{tab:lexicon}
  \end{table}

  For a language that does not have a phonemic orthography, a machine learning approach can also be used to learn a mapping for the dictionary.
   

\subsubsection{Weighted finite-state transducers in Kaldi} \label{sec:wfst} 

  Weighted finite-state transducers are a type of automaton in which a transition has an input label, an output label, and a weight. A special case of FST is a finite-state acceptor (FSA) where the input and output labels of a transition are equal. A FST, having both input and output labels, maps an input sequence to an output sequence when a path is taken through it. Figure \ref{fig:wfst} displays a simple example of an FST.
  \begin{figure}[htb]
    \centering
    \includegraphics[width=9cm]{wfst}
    \caption{A weighted finite-state transducer \citep{openfst}.}
    \label{fig:wfst}
  \end{figure}
  The example transducer has three states, the initial state (0) denoted with a bold circle and final state (2) with a double circle. The arcs between states have input labels \{a, b, c\}, output labels \{x, y, z\}, and real number (floating point) weights associated with them. Also the final state has a weight. This transducer would map the input sequence "ac" to the output sequence "xz" with the weight calculated by summing the individual arc weights, in this case adding up to 6.5.

  FSTs are used in Kaldi to encode many of the components of the ASR system. In training, the lexicon is represented as a FST. In decoding, the complete ASR system is encoded into a FST, called a \emph{decoding graph} before the actual decoding, as the graph creation is typically the most resource-hungry step in the process. In Kaldi chain models, FSTs are used in the LF-MMI training to encode the denominator and numerator of the objective function (see Section \ref{sec:dnn_am}).

  \paragraph*{Kaldi decoding graph creation}
  Kaldi composes the $HCLG$ decoding graph from four component transducers: the HMM $H$, the context $C$, the lexicon $L$, and the LM (or grammar) $G$ \citep{povey2012wfst}. 

  The n-gram language model that is used in decoding
  % (in training, only a lexicon is needed)
  is converted from the ARPA format, described in \ref{tab:arpa}, into the $G$ transducer using the \texttt{arpa2fst} program. The purpose of $G$ is not to transduce a sequence from one domain to another, but to assign weights to the possible word sequences. For this reason, the input and output labels are equal, making it technically a finite-state acceptor. The ARPA model lists the conditional base-10 logarithmic probabilities for each n-gram. These give weights to the arcs of the corresponding paths in the acceptor, the arc weights having an inverse relation to the probabilities (i.e., they are negated) and using natural logarithm instead of 10-base.  Referring to the weight of decoding graph FST as "cost" is more intuitive, so in this text this term is used, too. A complete path through the acceptor corresponds to a word sequence and the cost of the path indicates how \emph{un}likely the sequence is.
  If a probability of the highest-order n-gram has not been explicitly specified  in the n-gram model, a path is taken through the backoff node which corresponds to the lower-order backoff n-grams and the costs are determined by the associated backoff probabilities. Since the model can recursively back off  all the way to unigrams, any word sequence is accepted and given a cost. The backoff arcs do not have a word label, which raises the problem that there are multiple paths for a single input sequence, making processing the graph inefficient\footnote{more disadvantages?}. The backoff arcs are therefore assigned a \emph{disambiguation symbol} "\texttt{\#0}" as the input label. The disambiguation symbol allows for an operation (in practice, an algorithm) called \emph{determinisation}. A deterministic transducer has the property that one input string matches at most one path \citep{mohri2008speech}.
  The acceptor also dismisses sentence start and end tokens of the LM  (\texttt{<s>} and \texttt{</s>} or whatever they are) since these are not wanted in the speech transcripts. The number of word types used in speech recognition can also be limited to make the decoding graph of feasible size, in which case some types are omitted from the LM acceptor.

  The lexicon transducer $L$ maps a phoneme sequence input to an output consisting of one word. If the lexicon has multiple words with the same pronunciation, or when a phoneme sequence is a part of multiple word pronunciations, word disambiguation symbols \{\texttt{\#1}, \texttt{\#2},...\} are needed to ensure each phoneme sequence has only one possible word output. Ambiguity in the transducer means that there could be multiple paths matching one input string.  
  See Section
  \ref{sec:lexicon} for discussion about orthography and Section \ref{sec:sil_prob} for discussion about the case where one word has multiple pronunciations. 

  The grammar acceptor $G$ and lexicon transducer $L$ are combined to compose a new transducer $LG$ that maps a phoneme sequence input to a word sequence output. The composition is done in the program \texttt{fsttablecompose}, and the new transducer is determinised in the program \texttt{fstdeterminizestar}.

  $C$ is the transducer from context-dependent phonemes to context-independent phonemes. By composing $C$ with $LG$, the triphones are mapped to words. $C$ is built dynamically in the process of composing it with the existing $LG$ FST.
  The final FST is the HMM, which maps the state transitions to triphones, or more generally any context-dependent phonemes. After $H$ is composed with the $CLG$, the $HCLG$ is complete and can be used for lattice generation and decoding.


  % , for example, in the lexicon FST (the file L.fst) to transduce a phoneme sequence to a (sub)word token sequence. A grammar or a language model is encoded in a finite-state acceptor (G.fst) since its purpose is to define the possible sequences without transducing sequences from a domain to another. 

  % Figure \ref{fig:fst} is an example of a grammar-lexicon (LG.fst) transducer. 

  % \begin{figure}
  % \caption{} \label{fig:fst}
  % \end{figure}


\subsubsection{Lattices and n-best lists} \label{sec:lattice} 

  The decoding graph built by the Kaldi programs encodes all possible word sequences and the corresponding HMM state sequences. A sequence of observations is decoded with the graph in the manner described by \citet{povey2012wfst}. First, a WFSA $U$ is built from the observations that encodes the acoustic weights for each arc of each state transition that can emit each observation in the sequence. $U$  includes
  % about\footnote{not necessarily exactly due to $\epsilon$ labels.}
  $T+1$ states with arcs between the states that correspond to the HMM states at the time step $t$. The costs of the arcs encode the acoustic log-likelihoods. The utterance-specific \emph{search graph} $S$ that is traversed during decoding is generated by composing $U$ with the decoding graph. The search graph is not searched completely, but a subset of the best paths are selected by \emph{beam pruning}.
  When generating the lattice that encodes the subset of the best paths, some of the desiderata are: that the lattice includes all word sequences within the beam size $\alpha$ that represents a log-likelihood difference to the optimal path; that the scores and alignments in the lattice are accurate; that the lattice does not contain duplicate paths of the same word sequence \citep{povey2012wfst}.

  An alternative to a word lattice is to simply create lists of $n$ best transcriptions of the utterance. Though sometimes simpler to create and use, n-best lists are less efficient than lattices. In n-best lists, there are often transcriptions that differ only by a few words, which is redundant. n-best lists are capable of encoding far fewer transcriptions than lattices in the same amount of memory, and are therefore usually much more restrictive when, for instance, doing rescoring with a language model.


\subsubsection{Silence and pronunciation probability modelling} \label{sec:sil_prob} 

  The Kaldi toolkit allows also for modelling the probability of an optional silence between specific words, and the probability of different pronunciations of a word. These methods have been found to improve WER results by a small but consistent margin \citep{chen2015pronunciation}. 

  Differences of pronunciation are significant in many languages. However, in Finnish, which has a phonemic orthography (see Section \ref{sec:lexicon}), pronunciation probabilities are not applicable. If the mapping from phonemes to graphemes is one-to-one, there are no alternative pronunciations for a word.

  The probability of an optional silence phone between two words is estimated from statistics collected from alignments. 
  When using a subword vocabulary, it is important to indicate which boundaries are actual word boundaries, because the optional silence should not be inserted in the middle of a word. \citet{smit2017improved} described and implemented a method to build the lexicon FST in such a way that restricts the use optional silence to only word boundaries.


\subsubsection{Deep neural networks for acoustic modelling} \label{sec:dnn_am}

  In the previous decade, deep neural networks achieved state-of-the-art results in acoustic modelling, supplanting the Gaussian mixture models as the most accurate method to classify observations into phoneme classes. In the HMM/DNN hybrid approach, DNNs provide \emph{pseudo-likelihoods} of the observations for each HMM state.
  % This is due to DNNs being discriminative instead of generative like GMMs.
  The approach defined in Eq. \ref{eq:asr-bayes-2} works well for traditional ASR systems that use GMMs for acoustic modelling. However, if the generative GMM is replaced with a discriminative neural network model, it does not produce an acoustic likelihood $P(\boldsymbol{o}_t|\boldsymbol{s}(t))$ but a state level posterior probability $P(\boldsymbol{s}(t)|\boldsymbol{o}_t)$, which is a problem because the decoding (Eq. \ref{eq:asr-bayes-2}) relies on the likelihoods. This mismatch can be bypassed by applying the Bayes' rule to produce pseudo-likelihoods:
  \begin{equation}
    P(\boldsymbol{x}_t|\boldsymbol{s}(t)) =
    \frac{P(\boldsymbol{s}(t)|\boldsymbol{x}_t)P(\boldsymbol{x}_t)} {P(\boldsymbol{s}(t))} \propto	 
    \frac{P(\boldsymbol{s}(t)|\boldsymbol{x}_t)} {P(\boldsymbol{s}(t))}
  \end{equation}
  The state priors $P(\boldsymbol{s}(t))$ can be gathered from corpus frequencies \citep{Bourlard1994}. 

  \paragraph*{TDNNs and RNNs} 
    Two common types of DNN used for acoustic modelling are RNNs and time delay neural networks (TDNN). Both RNNs and TDNNs are inherently suited to modelling time-series data, where it is important to capture long-term time dependencies, such as speech. However, the two types of DNN are different in how they achieve this.

    RNNs (see Section \ref{sec:rnn})

    TDNNs 

    \citep{peddinti2015time}

  % (explaining RNNs and TDNNs in previous sections?)

  \paragraph*{Sequence-level lattice-free MMI} 
    Currently, the state-of-the-art implementations of DNNs in Kaldi are trained using lattice-free MMI (LF-MMI) training criterion, as described by \citet{povey2016purely}. These are called "chain" models in Kaldi. LF-MMI is a sequence discriminative criterion, which means that the aim is to maximise the conditional log-likelihood (Eq. \ref{eq:mmi}) of the correct transcript on the sequence level. In the traditional MMI approach, a cross-entropy system is trained to  generate lattices for a weak language model, and the lattices are used to approximate the possible word sequences for the discriminative objective function denominator. In LF-MMI, however, the possible word sequences are not approximated with a lattice, but a phone-level language model is computed so that the sum in the denominator is not approximated, but can be computed exactly. This is possible since a phone-level LM requires significantly less mamory than a normal word-level LM.  The phone LM is represented as an FST, created in a similar manner as the normal decoding FST described in Section \ref{sec:wfst}. In this case there is no lexicon and grammar FSTs but a phone grammar FST, so the graph consists of the component FSTs $H$, $C$, and $P$. The numerator FST uses a lattice to represent the utterance. The numerator FST is composed with the denominator FST so that the phoneme LM of the denominator removes illegal output sequences. The composition also ensures that the objective function value is negative. 
    % https://desh2608.github.io/2019-05-21-chain/

  \paragraph*{Regularisation of DNN AMs} 
    Regularisation is important in DNN training to impede overfitting. Kaldi chain models incorporate a number of regularisation methods in the DNN architectures. One of the methods is called \emph{L2-regularisation}, which applies the euclidean norm to penalise elements in the output vector that tend to blow up. This is achieved by subtracting $\frac{1}{2}c||\boldsymbol{y}||_2^2$ from the objective function of each frame output $\boldsymbol{y}$, where $c$ is a user-set scaling factor, e.g. 0.005 \citep{povey2016purely}. In Kaldi, L2-regularisation is applied on the outputs; note that this method is different from the L2-regularisation applied to the weights, which is another commonly used regularisation method.
    
    Another widely used regularisation method for deep neural nets are \emph{dropout layers} which randomly sample a subset of the neurons and their connections which are omitted, dropped out, during an iteration of the training \citep{srivastava2014dropout}. This prevents the neurons from relying on the other neuron's outputs too much, i.e., prevents too much \emph{co-adaptation} among the parameters. Dropping out random units in training shifts the unit of adaptation down from large groups of neurons to individual neurons, since it is not guaranteed that the other neurons are present. This principle is explained through an analogy to sexual selection, in the above mentioned original paper. Since genes are mixed randomly with another set of genes from a conspecific between every generation, individual genes cannot rely on specific other genes to be present in the future. This inhibits aggregation of large interdependent gene complexes, in which genes function well only with the group of co-adapted genes. These kinds of complexes would become rigid, and fragile to the inevitable mutations that will always occur quite randomly: new genes would be difficult to incorporate in the complex. This kind of rigidity is analogous to overfitting to the training data and failing to find generalisable rules that apply to new data samples from the hidden distribution, which regularisation attempts to lessen.

    % It has been empirically shown that dropout layers 

    A third type of regularisation technique is called \emph{leaky HMM} in Kaldi. Transitioning from any state $a$ to any state $b$ is allowed once per frame with the probability of a small (typically around 0.01) \emph{leaky-hmm-coefficient} times the probability of state $b$. The aim is to effect a gradual forgetting of the context, since transitioning to a random state is equal to stopping and restarting the HMM \citep{povey2016purely}. This reduces the overfitting caused by too much memorising of the training data sequences.

    Another technique to regularise the sequence-level training is to add a separate output layer to the network that learns the cross entropy objective, as well as a separate last hidden layer. This means the network has two output branches with two separate weight matrices in each branch. After the training, the cross entropy branch can be discarded, and the main, sequence output branch is left to be used in decoding.
    This technique is abbreviated as \textt{xent\_regularize} in the Kaldi code, and the associated hyperparameter is a scaling factor for the cross entropy objective, typically 0.1 because its dynamic range is naturally larger than that of the MMI objective function \citep{povey2016purely}.


\subsection{Spoken and written conversations in Finnish} \label{sec:conv}

  Informal Finnish 



% \clearpage
% \section{Methods}

\clearpage
\section{Experiments}

% \subsection{Baseline} 

%   The baseline language models and the ASR system is based on the systems developed by \citet{enarvi2017automatic}.

\subsection{Acoustic modelling experiments}
\subsubsection{Speech corpora} 

  The acoustic model training, development and test sets are the same as in \citep{enarvi2017automatic}. The training speech data are 85 hours of read and spontaneous speech from three different sources. The SPEECON corpus consists of 550 speakers reading 30 sentences and 30 single words as well as speaking 10 spontaneous sentences \citep {iskra2002speecon}. The DSPCON\footnote{\url{http://urn.fi/urn:nbn:fi:lb-201708251}} corpus consists of 5281 spontaneous sentences from 218 different male students and 24 female students, totalling 9.8 hours \citep{enarvi2018modeling}. The third source is the FinDialogue part of the FinINTAS corpus \citep{lennes2009segmental}. The development and test sets contain additionally spontaneous utterances from radio shows, referred to as RadioCon. Table \ref{tab:am_data} lists the acoustic modelling data sets.

  \begin{table}[ht!]
      \begin{tabularx}{\columnwidth}{|X|l|l|l|}
          \hline
          \textbf{AM corpora} & \# of speakers &\# of utterances & \# of hours \\ \hline
          DSPCON                                & 242 &     & 9.8  \\ \hline
          SPEECON                               & 550 &     &     \\ \hline
          FinDialogue                           &     &     &     \\ \hline
          total AM training data                &     &     & 85   \\ \hline
          RadioCon (devel, test sets)           &     &     &     \\ \hline
          VoxCeleb (speaker embeddings)         &     &     &     \\ \hline
      \end{tabularx}
      \caption{}
      \label{tab:am_data}
  \end{table}

  \emph{Speed perturbation} is a data augmentation method in which the speed of the audio is increased or decreased \citep{ko2015audio}. This method is used for the DNN AM training data, augmenting the speech data by changing its speed by a factor of 0.9 and 1.1, increasing the amount of data by a factor of three. As the quality, or domain, of the three different data sets is different, it could be useful to augment only the in-domain data, i.e., DSPCON. A system is trained with the whole dataset augmented, as well as one system augmenting only DSPCON. The difference

  
\subsubsection{HMM/GMM acoustic model architecture and training}

  Different features are used in GMMs and DNNs. The GMMs input 13 MFCCs and their $\Delta$ and $\Delta\Delta$ features, amounting to a 39-dimensional feature vector. Cepstral mean and variance normalisation is applied to the features per speaker.

  
  The GMM phoneme model set was trained for the most part by following the Kaldi recipe for the Wall Street Journal corpus\footnote{\url{https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/run.sh}}. The monophone model set is trained with a subset of 2000 shortest utterances. The features are first aligned equally with the states, after which 40 iterations of Viterbi training are performed to estimate the HMM/GMM model set and generate an improved alignment. The number of Gaussians is increased in between iterations, reaching a total of 1000 Gaussians. 

  The triphone model set is initialised from the monophone models and their alignments, and is trained in a three consecutive steps. Subsets of the data set is selected, including 4000 utterances for the first step, 8000 utterances for the second step, and then using the whole ~15000 utterance training data set for the third triphone training step. The first step (\texttt{train\_deltas.sh}) is similar to monophone training, using the delta and delta-delta features in addition to the MFCCs, and increasing the total number of Gaussians to 10k. The second step (\texttt{train\_lda\_mllt.sh}) splices the MFCC features, reduces the dimensionality back to 40 using LDA, and estimates an MLLT transform. The MLLT transform is applied on the features and new estimates of the models are computed, increasing the total number of Gaussians to 15k. The third and final triphone training step (\texttt{train\_sat.sh}) does speaker adaptive training utilising fMMLR. The total number of Gaussians is increased to 40k.

  After the triphone model set is trained, silence probabilities (see Section \ref{sec:sil_prob}) are estimated from the statistics of the training data alignments. Since the Finnish lexicon is a one-to-one mapping from letters to phonemes, there are no optional pronunciations of words for which to calculate pronunciation probabilities.

  An MMI (see Section \ref{sec:mmi}) system is trained based on the triphone alignments and silence probabilities. The MMI model generates the alignments that are used to train the deep neural network acoustic model.

  The GMM triphone model set was kept fixed throughout the experiments, after first running a couple of experiments to tune the model. These experiments included evaluating the MMI model compared to a fourth triphone training step and experimenting on a couple of different 


\subsubsection{HMM/DNN acoustic model architecture and training}

  The DNN AM inputs higher-resolution features than the GMM AM. 40-dimensional MFCCs are extracted from the data set, which has been augmented using speed perturbation.

  The DNN is a Kaldi chain model, trained on the LF-MMI objective
  
  % The first layer of the DNN

  % Table \ref{tab:am} lists the TDNN AM layers.

  % \begin{table}[ht!]
  %     \begin{tabularx}{\columnwidth}{|X|l|l|}
  %         \hline
  %         \textbf{layer} & number of neurons & activation function \\ \hline
  %          &   & \\ \hline
  %     \end{tabularx}
  %     \caption{}
  %     \label{tab:am}
  % \end{table}
  

\subsubsection{Speaker embedding experiments}

  In the speaker embedding experiments, pretrained extractors are used as well as extractors trained on the AM training data. 

  Both online and offline i-vectors are extracted and compared. 
  
  \paragraph*{Pretrained VoxCeleb i-vector and x-vector extractors} 
    Pretrained i-vector and x-vector extractors trained on the VoxCeleb data \citep{nagrani2017voxceleb, chung2018voxceleb2} are used in the experiments. The VoxCeleb1 data contains about 100k utterances from 1251 celebrities and the VoxCeleb2 data contains about 1M utterances from over 6000 speakers. The code that was used for training the extractors is in the Kaldi repository\footnote{i-vectors: \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/voxceleb/v1}}\footnote{x-vectors: \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/voxceleb/v2}} and the pretrained extractors are downloaded online\footnote{\url{https://kaldi-asr.org/models/m7}}.
    
    The VoxCeleb i-vector extractor was trained on 24 MFCCs and their delta and delta-delta coefficients. An energy-based VAD system is used to select the voiced frames for both i-vector and x-vector systems. The i-vector system UBM is a GMM that has 2048 full-covariance component Gaussians. The i-vectors have 400 dimensions, and are subsequently reduced to 200 dimensions using an LDA model.

    The VoxCeleb x-vector extractor is trained on the VoxCeleb speech data that has been augmented in various ways. The MUSAN corpus \citep{snyder2015musan} of music, noise and speech as well as simulated room impulse response \citep{ko2017study} are used to generate noise for the speech data. The noise is added to the speech data and a subset of the noisy audio files is randomly selected and pooled with the clean audio files. This increases the amount of data roughly twofold. 
    The x-vector extractor was trained on 30 MFCCs with their deltas and delta-deltas. The DNN is a TDNN with ReLU non-linearities where the five first layers use frame-level training with a temporal context of a few adjacent frames. The architecure is described by \citet{snyder2017deep}. After the frame-level layers is a pooling layer that aggregates the frame-level outputs, calculating the mean and standard deviation of the whole segment. The last two layers before softmax operate on the segment level statistics. The x-vector is extracted from the penultimate layer (the 6th layer), with 512 dimensions. An LDA model reduces the x-vector dimensionality to 200.


  \paragraph*{Pretrained i-vector extractor trained on Yle and Parliament data}
    A pretrained i-vector extractor was used as a comparison to the models trained in this work.

  \paragraph*{i-vector extractor trained on the conversational Finnish data}

    Most of Kaldi recipes for training a HMM/DNN model include i-vectors as the speaker-adaptation method. In this thesis, the extractor for the baseline i-vectors is trained similarly to the Switchboard recipe. The extractor is trained on the same data as the acoustic model for extracting online i-vectors. The speed perturbation data augmentation is used also for the extractor training data. 




  \paragraph*{Domain adaptation by finetuning the pretrained extractors}

    Since the VoxCeleb extractor models are trained on English speech, it is possible that the extractors are suboptimal for Finnish. The extractors are trained on task of identifying speakers based on an utterance. There could be differences in the two languages in what kind of acoustic cues are utilised to identify the speaker. As with other modelling tasks, such as acoustic modelling, the extractor model could benefit from finetuning the large pretrained model on a smaller set of in-domain (or closer to the test domain) speech corpus. 

    The Voxceleb x-vector extractor was finetuned with the augmented Finnish speech corpus. The softmax layer was replaced so that the number of target classes (speakers) was compatible with the finetuning data. Apart from the ouptu layer, all the other layers and their parameters were kept from the pretrained model, and the model was trained on the Finnish data for one epoch. This improves the evaluation set WER result a little (Table \ref{tab:spk_emb}).

    Replacing the last hidden layer of the extractor model was experimented on, but this did not improve the WER results. Adding a new layer after the 7th layer was also tried, but here too the results were worse than for the pretrained model.
    
    
  \begin{table}[ht!]
      \begin{tabularx}{\columnwidth}{|X|l|l|}
          \hline
          \textbf{Speaker embedding} & 1st-pass WER & Oracle WER \\ \hline
          None & 27.3 &  \\ \hline
          x-vecs conv speech & 26.3 & \\ \hline
          x-vec VoxCelec pretrained & 25.5 & \\ \hline
          x-vec Voxceleb pretrained and conv speech finetuned & 25.3 &  \\ \hline
      \end{tabularx}
      \caption{}
      \label{tab:spk_emb}
  \end{table}

   


\subsection{Language modelling experiments}

\subsubsection{Text corpora}

  \begin{table}[ht!]
      \begin{tabularx}{\columnwidth}{|X|l|l|}
          \hline
          \textbf{LM training corpora} & \# of word tokens & 4-gram interpolation weight \\ \hline
          DSP conversational speech corpus transcriptions & 61k  & 0.41 \\ \hline
          WEB corpus, conversational written text corpus & 75.9M & 0.59 \\ \hline
      \end{tabularx}
      \caption{}
      \label{}
  \end{table}

\subsubsection{n-gram language model}

\subsubsection{LSTM language model}

\subsubsection{Transformer-XL}

\clearpage
\section{Results}

\clearpage
\section{Conclusion} 


\clearpage
\small
% for counting pages
\thesisbibliography

\bibliographystyle{apalike}
\bibliography{references}


% \begin{thebibliography}{99}

%% Alla pilkun j\"alkeen on pakotettu oikea v\"ali \<v\"alily\"onti>-merkeill\"a.
% \bibitem{Kauranen} Kauranen,\ I., Mustakallio,\ M. ja Palmgren,\ V.
%   \textit{Tutkimusraportin kirjoittamisen opas opinn\"aytety\"on
%     tekij\"oille.}  Espoo, Teknillinen korkeakoulu, 2006.

% \end{thebibliography}

%% Appendices
%% If you don't have appendices, remove \clearpage and \thesisappendix below.
% \clearpage
% \thesisappendix
% \section{}

\end{document}