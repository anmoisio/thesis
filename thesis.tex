%% License 
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %% This is licensed under the terms of the MIT license below.                 %%
  %%                                                                            %%
  %% Written by Luis R.J. Costa.                                                %%
  %% Currently developed at the Learning Services of Aalto University School of %%
  %% Electrical Engineering by Luis R.J. Costa since May 2017.                  %%
  %%                                                                            %%
  %% Copyright 2017-2018, by Luis R.J. Costa, luis.costa@aalto.fi,              %%
  %% Copyright 2017-2018 Swedish translations in aaltothesis.cls by Elisabeth   %%
  %% Nyberg, elisabeth.nyberg@aalto.fi and Henrik Wallén,                       %%
  %% henrik.wallen@aalto.fi.                                                    %%
  %% Copyright 2017-2018 Finnish documentation in the template opinnatepohja.tex%%
  %% by Perttu Puska, perttu.puska@aalto.fi, and Luis R.J. Costa.               %%
  %% Copyright 2018 English template thesistemplate.tex by Luis R.J. Costa.     %%
  %% Copyright 2018 Swedish template kandidatarbetsbotten.tex by Henrik Wallen. %%
  %%                                                                            %%
  %% Permission is hereby granted, free of charge, to any person obtaining a    %%
  %% copy of this software and associated documentation files (the "Software"), %%
  %% to deal in the Software without restriction, including without limitation  %%
  %% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
  %% and/or sell copies of the Software, and to permit persons to whom the      %%
  %% Software is furnished to do so, subject to the following conditions:       %%
  %% The above copyright notice and this permission notice shall be included in %%
  %% all copies or substantial portions of the Software.                        %%
  %% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
  %% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
  %% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
  %% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
  %% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
  %% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
  %% DEALINGS IN THE SOFTWARE.                                                  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%

%% spellcheck-on

%% Configurations, meta information, prefaces etc. 
  \documentclass[english, 12pt, a4paper, elec, utf8, a-1b, hidelinks]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, dvips, online]{aaltothesis}

  %% Use the following options in the \documentclass macro above:
  %% your school: arts, biz, chem, elec, eng, sci
  %% the character encoding scheme used by your editor: utf8, latin1
  %% thesis language: english, finnish, swedish
  %% make an archiveable PDF/A-1b or PDF/A-2b compliant file: a-1b, a-2b
  %%                    (with pdflatex, a normal pdf containing metadata is
  %%                     produced without the a-*b option)
  %% typeset in symmetric layout and blue hypertext for online publication: online
  %%            (no option is the default, resulting in a wide margin on the
  %%             binding side of the page and black hypertext)
  %% two-sided printing: twoside (default is one-sided printing)

  \usepackage{amsfonts, amssymb, amsbsy, amsmath, natbib, graphicx, tabstackengine}

  \DeclareMathOperator*{\argmax}{argmax}

  % \university{Aalto-yliopisto}
  % \school{Sähkötekniikan korkeakoulu}
  \degreeprogram{Computer, Communication and Information Sciences}
  \major{Signal, Speech and Language Processing}
  \code{ELEC3031}
  \univdegree{MSc}
  \thesisauthor{Anssi Moisio}

  %% A possible "and" in the title should not be the last word in the line, it
  %% begins the next line.
  %% Specify the title again without the linebreak characters in the optional
  %% argument in box brackets. This is done because the title is part of the 
  %% metadata in the pdf/a file, and the metadata cannot contain linebreaks.
  \thesistitle{Automatic recognition of conversational speech} % working title
  %\thesistitle[Title of the thesis]{Title of\\ the thesis}
  \place{Espoo}
  \date{}

  %% Thesis supervisor
  %% Note the "\" character in the title after the period and before the space
  %% and the following character string.
  %% This is because the period is not the end of a sentence after which a
  %% slightly longer space follows, but what is desired is a regular interword
  %% space.
  \supervisor{Prof.\ Mikko Kurimo}

  %% Advisor(s)---two at the most---of the thesis.
  \advisor{Dr.}

  %% Aaltologo: syntax:
  %% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
  %% The logo language is set to be the same as the thesis language.
  \uselogo{aaltoRed}{''}

  %% The English abstract:
  %% All the details (name, title, etc.) on the abstract page appear as specified
  %% above.
  %% Thesis keywords:
  %% Note! The keywords are separated using the \spc macro
  \keywords{For keywords choose\spc concepts that are\spc central to your\spc thesis}

  %% The abstract text. This text is included in the metadata of the pdf file as well
  %% as the abstract page.
  \thesisabstract{abstract here
  }

  %% Copyright text. Copyright of a work is with the creator/author of the work
  %% regardless of whether the copyright mark is explicitly in the work or not.
  %% You may, if you wish, publish your work under a Creative Commons license (see
  %% creaticecommons.org), in which case the license text must be visible in the
  %% work. Write here the copyright text you want. It is written into the metadata
  %% of the pdf file as well.
  %% Syntax:
  %% \copyrigthtext{metadata text}{text visible on the page}
  %% 
  %% In the macro below, the text written in the metadata must have a \noexpand
  %% macro before the \copyright special character, and macros (\copyright and
  %% \year here) must be separated by the \ character (space chacter) from the
  %% text that follows. The macros in the argument of the \copyrighttext macro
  %% automatically insert the year and the author's name. (Note! \ThesisAuthor is
  %% an internal macro of the aaltothesis.cls class file).
  %% Of course, the same text could have simply been written as
  %% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
  %% {Copyright \copyright{} 2018 Eddie Engineer}
  \copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
  {Copyright \copyright{} \number\year{} \ThesisAuthor}

  %% You can prevent LaTeX from writing into the xmpdata file (it contains all the 
  %% metadata to be written into the pdf file) by setting the writexmpdata switch
  %% to 'false'. This allows you to write the metadata in the correct format
  %% directly into the file thesistemplate.xmpdata.
  %\setboolean{writexmpdatafile}{false}

  %% All that is printed on paper starts here
  \begin{document}

  \pagenumbering{roman}

  %% Create the coverpage
  % \makecoverpage

  %% Typeset the copyright text.
  %% If you wish, you may leave out the copyright text from the human-readable
  %% page of the pdf file. This may seem like a attractive idea for the printed
  %% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
  %% on the page. However, the recommendation is to print this copyright text.
  % \makecopyrightpage

  %% Note that when writting your thesis in English, place the English abstract
  %% first followed by the possible Finnish or Swedish abstract.

  %% Abstract text
  %% The text in the \thesisabstract macro is stored in the macro \abstractext, so
  %% you can use the text metadata abstract directly as follows:
  % \begin{abstractpage}[english]
  %   \abstracttext{}
  % \end{abstractpage}

  % \newpage
  % %% Abstract in Finnish.
  % \thesistitle{Opinnäyteen otsikko}
  % \supervisor{Prof.}
  % \advisor{TkT}
  % \degreeprogram{Tieto-, tietoliikenne- ja informaatiotekniikka}
  % \major{Signaalin-, puheen- ja kielenkäsittely}
  % %% The keywords need not be separated by \spc now.
  % \keywords{}
  % %% Abstract text
  % \begin{abstractpage}[finnish]
  %   tiivistelmä tähän
  % \end{abstractpage}

  % %% Preface
  % %% This section is optional. Remove it if you do not want a preface.
  % \mysection{Preface}
  % preface here

  % \vspace{5cm}
  % Otaniemi, 31.8.2018

  % % \vspace{5mm}
  % % {\hfill Eddie E.\ A.\ Engineer \hspace{1cm}}

  % \newpage
  \thesistableofcontents
%%

%% Symbols and abbreviations 
  \mysection{Symbols and abbreviations}

  \subsection*{Symbols}

  \begin{tabular}{ll}
  % examples
  % $\mathbf{B}$  & magnetic flux density  \\
  % $c$              & speed of light in vacuum $\approx 3\times10^8$ [m/s]\\
  % $\omega_{\mathrm{D}}$    & Debye frequency \\
  % $\omega_{\mathrm{latt}}$ & average phonon frequency of lattice \\
  % $\uparrow$       & electron spin direction up\\
  % $\downarrow$     & electron spin direction down \\
  % /examples
  $\boldsymbol{\Delta}$                       & delta feature vector \\
  $\boldsymbol{\mu}$                          & mean vector of a GMM \\
  $\boldsymbol{\Sigma}$                       & covariance matrix of a GMM \\


  $\boldsymbol{o}_t$    & observation, i.e., feature vector, at time $t$ \\ 
  $P(.)$              & probability \\
  $p(.|.)$            & likelihood \\
  
  \end{tabular}

  % \subsection*{Operators}

  % \begin{tabular}{ll}
  % examples
  % $\nabla \times \mathbf{A}$                  & curl of vectorin $\mathbf{A}$\\
  % $\displaystyle\frac{\mbox{d}}{\mbox{d} t}$  & derivative with respect to 
  %                                               variable $t$\\[3mm]
  % $\displaystyle\frac{\partial}{\partial t}$  & partial derivative with respect 
  %                                               to variable $t$ \\[3mm]
  % $\sum_i $                                   & sum over index $i$\\
  % $\mathbf{A} \cdot \mathbf{B}$               & dot product of vectors $\mathbf{A}$ and 
  %                                               $\mathbf{B}$ \\
  % /examples

  % \end{tabular}

  \subsection*{Abbreviations}

  \begin{tabular}{ll}
  AM            & acoustic model \\
  ASR           & automatic speech recognition \\
  CER           & character error rate \\
  CMVN          & cepstral mean and variance normalization \\
  DNN           & deep neural network \\
  EM            & expectation maximisation \\
  fMLLR         & feature space maximum likelihood linear regression \\
  FSA           & finite-state acceptor \\
  FST           & finite-state transducer \\
  GMM           & Gaussian mixture model \\
  GRU           & gated recurrent unit \\
  HMM           & hidden Markov model \\
  LDA           & linear discriminant analysis \\
  LM            & language model \\
  LSTM          & long short-term memory \\
  MCE           & minimum classification error \\
  MFCC          & mel-frequency cepstral coefficient \\
  ML            & maximum likelihood \\
  MLE           & maximum likelihood estimate \\
  MLLR          & maximum likelihood linear regression \\
  MLLT          & maximum likelihood linear transformation \\
  MMI           & maximum mutual information \\
  MPE           & minimum phone error \\
  NN            & neural network \\
  NNLM          & neural network language model \\
  PDF           & probability density function \\
  PPL           & perplexity \\
  RNN           & recurrent neural network \\
  SAT           & speaker adaptive training \\
  TDNN          & time delay neural network \\
  WER           & word error rate \\
  WFST          & weighted finite-state transducer \\
  VTLN          & vocal tract length normalization \\
  \end{tabular}
%%

%% \clearpage is similar to \newpage, but it also flushes the floats (figures
%% and tables).
\cleardoublepage
\pagenumbering{arabic}
\setcounter{page}{1}

%% Text body begins.
\section{Introduction}

%% Leave page number of the first page empty
\thispagestyle{empty}
\clearpage

\section{Finnish conversations to text} 

  The task of converting speech into text, or \emph{automatic speech recognition}, can be divided into
  % four
  subtasks: \emph{feature extraction}, \emph{acoustic modelling}, \emph{phoneme-to-grapheme mapping}, and \emph{language modelling}. The audio signal is first divided into $T$ segments, and the segments converted into feature vectors, also called observations, $\boldsymbol{O}=\boldsymbol{o}_1,...,\boldsymbol{o}_T$, which are a compressed representation of the audio signal. 
  The task is then to find $\argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O})$, where $\boldsymbol{w}$ is a word sequence. This probability is not practicable to compute directly, but by Bayes' rule it can be expanded to 
  \begin{equation}\label{eq:asr-bayes}
    \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
    \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
  \end{equation}

  The probability of the observations ${P(\boldsymbol{O})}$ is not relevant in finding the best transcription for the observations, which leaves the product $P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})$ to be estimated. Here, the former factor is the a priori probabilities of word sequences, modelled by language models, discussed in Section \ref{sec:lm}. The acoustic model determines likelihoods of observations given phoneme sequences that are mapped to grapheme sequences by a lexicon (also called a dictionary), yielding $P(\boldsymbol{O}|\boldsymbol{w})$. Acoustic modelling is discussed in Section \ref{sec:am}.
  % The first
  % % four
  % sections of this chapter respectively describe the
  % % four
  % components of the conventional ASR system.
  Section \ref{sec:conv} discusses the specifics of recognising conversational Finnish.

  %  A language model gives an a priori probability for each word sequence: $P(\boldsymbol{w})$. The estimate $\hat{\boldsymbol{w}}$ of the best transcription given the observations is the most likely sequence 

  % \begin{equation}\label{asr-bayes-argmax}
  %   \hat{\boldsymbol{w}} = \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
  %   \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
  % \end{equation} 

\subsection{Statistical language modelling} \label{sec:lm} 

  In statistical, probabilistic language modelling, the probability distribution $P(\boldsymbol{w})$ over word sequences is estimated based on information drawn from a training corpus. The probability of a word sequence is typically the product of the probability of the constituent words in the context
  \begin{equation}\label{eq:lm}
    P(w_t|\textup{context}) = \prod 
  \end{equation}
  What constitutes the context depends on the method.

\subsubsection{n-gram language models}

  % An n-gram language model bases the prediction of the last token in a sequence of $n$ tokens on statistics gathered from a training corpus. 
  The task is to calculate the probability distribution over the vocabulary for the prediction of the word after $n-1$ previous words (i.e., the context).
  A simple method to accomplish is to count the frequency of each word in the context:
  \begin{equation}\label{eq:ngram}
    P(w_i|w_{i-(n-1)},...,w_{i-1}) = \frac{\textup{count}(w_{i-(n-1)},...,w_{i-1}, w_i)}{\textup{count}(w_{i-(n-1)},...,w_{i-1}}
  \end{equation}

  To improve the predictions, \emph{smoothing} can be applied to the simple occurrence counts by redistributing probability mass from the most common n-grams to the less common ones, i.e., \emph{discounting} the most frequent n-grams. Alternatively, or in addition, the information of lower-order n-grams ((n-1)-grams,...,unigrams) can be used if there is an insufficient number examples of the n-grams of the desired order; the model can \emph{backoff} to the lower orders. The lower-order n-gram scores can also be \emph{interpolated} with the scores of the n-grams of the nominal order $n$. 
  
  Kneaser-Ney smoothing is a commonly used method, where 


\subsubsection{Recurrent neural language models}

\subsubsection{Attention-based neural language models}

  % \citet{dai2019transformer}


\subsection{The Kaldi toolkit}

  Kaldi is a toolkit for automatic speech recognition, used for the ASR experiments in this study. The next few sections of this chapter aim to give an overview of the process of building an ASR system using Kaldi. 


\subsection{Feature extraction} 

  A speech audio signal contains a lot of information that is irrelevant for converting the signal to text. The first step of ASR is to find the features of the signal that contain the information about what is being said. An assumption is made that the speech signal does not change meaningfully in a time frame of about 10 milliseconds, so that the signal can be divided into frames with this time resolution. The frames overlap so that each frame is about 20 or 25 milliseconds, and a tapered window function, such as Hamming, is applied to (i.e., multiplied by) each frame. This window function removes the discontinuities that occur on the borders of frames, and the overlapping compensates for the tapering of the window function so that the distorting effect on the signal statistics is minimised \citet{}. % https://wiki.aalto.fi/display/ITSP/Windowing

  The stationary frames' frequency components can be then computed with the Fourier transform. A commonly used method is to extract the MFCCs by applying a logarithmic mel-scale filterbank to the frequency spectrum, and lastly computing the DCT. The log mel-scale emphasises the lower frequencies emulating the way humans perceive sound, i.e., the way the human inner ear recognises lower frequencies with higher frequency resolution. The DCT decorrelates the coefficients so that the use of diagonal covariance matrices is possible in the subsequent stages of the modelling, namely when using GMMs to model the HMM state emissions (Section \ref{phoneme_hmm}). The use of diagonal covariance matrices reduces the number of free parameters.
  
  % ?
  % The MFCC method thus assumes that coefficients adjacent in time are independent of each other, which is a false assumption in the case of speech signals.
  % \?
  An MFCC vector encodes only the stationary frequency features of a frame. However, a speech signal varies in time, and this variation carries meaning about which phones are uttered.
  Thus, it is useful to add information to the feature vectors about how the signal changes in time. 
  Information about temporal change and about change of temporal change is extracted from the MFCCs by calculating the differences and second-order differences of adjacent coefficients. These features are called the delta ($\boldsymbol{\Delta}$) and delta-delta ($\boldsymbol{\Delta\Delta}$), or acceleration, features. 
  % The $\boldsymbol{\Delta}$s and $\boldsymbol{\Delta\Delta}$s are concatenated with the MFCC vectors, increasing the feature vector length threefold.
  The delta feature vector $\boldsymbol{\Delta}_t$ corresponding to the MFCC vector $\boldsymbol{c}_t$ (or the time step if that vector) is calculated by subtracting the weighted previous vector(s) from the weighted subsequent vector(s) and normalising the sum:
  \begin{equation}
  \boldsymbol{\Delta}_t = \frac{\sum_{\theta=1}^{\Theta}
  \theta(\boldsymbol{c}_{t+\theta}-\boldsymbol{c}_{t-\theta})}{2\sum_{\theta=1}^{\Theta}\theta^2}
  \end{equation}
  In Kaldi, the default window length $\boldsymbol{\Theta}$ is 2, so the $\boldsymbol{\Delta}$s are computed by multiplying the MFCCs with a sliding window of values $[-2,-1,0,1,2]$ and then normalising by dividing by $2*(1^2 + 2^2)= 10$ \citep{kaldi}. The $\boldsymbol{\Delta\Delta}$s are computed by applying the same method to the $\boldsymbol{\Delta}$ features. The first and last MFCCs are replicated to fill the window \citep{htkbook}.

  The cepstral mean and variance normalization is a method for making the features more useful in noisy conditions \citep{viikki1998cepstral}. In this technique, the MFCC feature vectors are normalised to have a zero mean and unit variance over a sliding finite segment. After the normalisation, clean and noisy MFCCs are more similar, which mitigates the performance reduction caused by noisy environments.

\subsection{Acoustic modelling} \label{sec:am}

\subsubsection{Modelling phonemes with hidden Markov models} \label{sec:phoneme_hmm}

  Estimating the likelihoods of observations given phonemes is achieved by creating a HMM for each phoneme. 
  The phoneme-specific HMMs
  generate likelihoods of the observed sequences which can be used to map observations to phonemes. This way the task becomes to estimate the parameters of the HMMs so that each of them models the associated phoneme as accurately as possible.
  % In the conventional ASR system, used also in this thesis, phonemes are modelled by HMMs, which are then concatenated to model utterances.

  A hidden Markov model consists of a hidden Markov chain, also called regime, and the observation sequence, i.e., feature vectors. Each observation $\boldsymbol{o}_t$ has a probability $b_i(\boldsymbol{o}_t)$ of being generated when a hidden state $i$ is entered. In other words, the observation is a probabilistic function of the hidden state. 
  % Gaussian mixtures
  A state's emission probabilities are represented by a PDF, typically a mixture of multivariate Gaussian densities
  \begin{equation}\label{}
    b_i(\boldsymbol{o}_t) = \sum^{M_j}_{m=1}c_{jm}
      \mathcal{N}(\boldsymbol{o}_t ; \boldsymbol{\mu}_{jm}, \boldsymbol{\Sigma}_{jm})
  \end{equation}
  where $\boldsymbol{\mu}_{jm}$ is the mean vector, $\boldsymbol{\Sigma}_{jm}$ is the covariance matrix and $c_{jm}$ is mixture weight for mixture component $m$ in state $j$. The Gaussian mixture density is
  \begin{equation}\label{}
    \mathcal{N}(\boldsymbol{o} ; \boldsymbol{\mu}, \boldsymbol{\Sigma})
       = \frac{1}{\sqrt{(2 \pi)^n |\boldsymbol{\Sigma}|}} 
      e^{-\frac{1}{2}(\boldsymbol{o} - \boldsymbol{\mu})^\top 
      \boldsymbol{\Sigma}^{-1} 
      (\boldsymbol{o} - \boldsymbol{\mu})}
  \end{equation}

  Because a mixture of Gaussians can assume arbitrary shapes, they can model non-Gaussian phenomena. 

  HMMs are used to model sequences, but observations are independent of past observations. Instead, the regime has a memory, although the shortest possible: the probability of being a certain state in the next time step depends only on the current state and not the previous states. This independence of the previous transitions is called the Markov assumption. Each hidden state pair (an arc from a state to another) is associated with a transition probability $a_{ij}$ that describes how probable it is to move from state $i$ to state $j$. 

  A HMM is defined by ...

  

  % in Kaldi 
  % http://kaldi-asr.org/doc/hmm.html
  The typical HMM topology for a phoneme is a left-to-right model, also called the Bakis model, with three emitting states that each have a transition to the next state and a self-loop. The model also includes a fourth non-emitting final state that has no outbound transitions. However, in the Kaldi chain models (see Section \ref{sec:dnn_am}), the topology is reduced to have only one emitting state due to a lower time resolution used.
  In Kaldi, the phoneme topology is defined in the lang/topo file.
  
  
  After initialising a HMM for each phoneme, the parameters, i.e. means, covariances and mixture weights need to be estimated in the process referred to as "training" of the model.

\subsubsection{Finding the best hidden state sequence and training HMMs} \label{sec:hmm_est}

  The speech recognition system is trained in a supervised manner, meaning that the training data consists of a parallel corpus of speech audio and corresponding correct transcription. However, the task of the acoustic model is not as straightforward as finding a label for an input vector\footnote{In End-to-end ASR the whole ASR task is simplified to outputting an arbitrary length grapheme sequence given the sequence of observed features}. The reference transcription can be of arbitrary length but the AM is required to map each observation to a HMM state, generating an equal-length \emph{alignment} of observations and states. The states correspond to phonemes, so an alignment can be mapped to a transcription of the audio.

  ML is one criterium of finding the best HMM parameters; others include the MMI (see Section \ref{sec:mmi}) and the MAP criteria.

  % Baum-Welch algorithm
  The Baum-Welch algorithm is an expectation-maximisation algorithm for estimating the HMM parameters. Maximising the expectedness of the data can be turned around and thought of as minimising how surprising the training data are to the model by modifying the model. Expectation maximisation is an iterative method to finding a local maximum for the likelihood of a set of data given a model.  Here, the task is to maximise the likelihood $P(\boldsymbol{O}|M)$ of the observations $\boldsymbol{O}$  given the parameters of
  the HMM $M$. % here all HMMs, viterbi: only the best ?
  If the HMM had only one state $j$, the maximum likelihood estimate $\hat{\boldsymbol{\mu}}_j$ would simply be the average of the observations, and $\hat{\boldsymbol{\Sigma}}_j$, too, could be determined directly, using the covariance definition.
  In practice, there are many states which is why the parameters need to be estimated numerically, iteratively. 
  However, the initial parameter values can be taken from simple statistics of the observations. The observations are divided equally between the states and the means and variances of the states are taken from the average values.

  The maximum likelihood estimates for the parameters are 
  \begin{equation}\label{eq:mu_hat}
    \hat{\boldsymbol{\mu}}_j = \frac{\sum^T_{t=1}L_j(t)\boldsymbol{o}_t}
      {\sum^T_{t=1}L_j(t)}
  \end{equation}
  and
  \begin{equation}\label{eq:sigma_hat}
    \hat{\boldsymbol{\Sigma}}_j = \frac{\sum^T_{t=1}L_j(t)
      (\boldsymbol{o}_t - \boldsymbol{\mu}_j)
      (\boldsymbol{o}_t - \boldsymbol{\mu}_j)^\top }
      {\sum^T_{t=1}L_j(t)}
  \end{equation}
  The numerator and denominator sums are \emph{accumulated} from the observations.

  % kaldi: gmm-init, HTK: Hinit

    % 
  
  % The training starts with some trial values of the parameters, for which

  % , for which the likelihood is calculated, given each training example.
  
  The state occupation probability
  \begin{equation}\label{eq:occ}
    L_j(t) = P(x(t)=j|\boldsymbol{O},M),
  \end{equation}
  i.e., the probability of being in state $j$ at time $t$, is calculated using using the forward-backward algorithm. The forward probability $\alpha_j(t)$ and backward probability $\beta_j(t)$ are defined as 
  \begin{align}
    \alpha_j(t) &= P(\boldsymbol{o}_1,...,\boldsymbol{o}_t,x(t)=j|M)  \label{eq:forward} \\
    \beta_j(t) &= P(\boldsymbol{o}_{t+1},...,\boldsymbol{o}_T|x(t)=j,M) \label{eq:back}
  \end{align}
  Spelled out, $\aplha_j(t)$ is the probability of the partial observation sequence up to time $t$ and that $M$ is in state $j$ at the time step. The backward probability is the probability of the partial observation sequence at the subsequent time steps up to the last vector, given that at the current time the model state is $j$. 
  The forward probability is a joint probability of the observations and the state, whereas the backward probability of the observations is conditional on the state. This allows for the state occupation probability to be determined by the product of the forward and backward probabilities (from Eqs. \ref{eq:occ} \ref{eq:forward} and \ref{eq:back})
  \begin{align}\label{}
    L_j(t) = \frac{ \alpha_j(t) \beta_j(t) }{P(\boldsymbol{O}|M)}
  \end{align}
  
  $\alpha$ and $\beta$ are calculated respectively using the recursions
  \begin{align}
    \alpha_j(t) &= \bigg[ \sum^{N-1}_{i=2}\alpha_i(t-1)a_{ij} \bigg] b_j(\boldsymbol{o}_t)
    \label{eq:alpha_recurs} \\
    \beta_i(t) &= \sum^{N-1}_{j=2} \beta_j(t+1) a_{ij} b_j(\boldsymbol{o}_{t+1}) 
    \label{eq:beta_recurs}
  \end{align}
  and the initial conditions,
  \begin{align}
    \alpha_1(1) &= 1, \; \; \; \; \alpha_j(1) = a_{ij} b_j(\boldsymbol{o}_1) 
    \label{eq:alpha_init} \\
    \beta_i(T) &= a_{iN} \label{eq:beta_init}
  \end{align}
  for $1<j<N$ and the final conditions
  \begin{align}
    \alpha_N(T) &= \sum^{N-1}_{i=2}\alpha_i(T)a_{iN} \label{eq:afinal} \\
    \beta_1(1) &= \sum^{N-1}_{j=2} a_{1j} b_j(\boldsymbol{o}_1) \beta_j(1) \label{eq:bfinal}
  \end{align}
  where the limits of the sums exclude the states $1$ and $N$ because they are non-emitting. The recursion \ref{eq:alpha_recurs} calculates the forward probabilities (of seeing the specified observations and being at the state $j$) by summing all possible forward probabilities for all possible predecessor states $i$ weighted by the transition probability $a_{ij}$.

  From \ref{eq:forward}, \ref{eq:afinal} and \ref{eq:bfinal} it follows that calculating the forward probability also yields the total likelihood $P(\boldsymbol{O}|M)=\alpha_N(T)$.

  % Viterbi training
  An alternative approach to the B-W algorithm is an iterative procedure called Viterbi training, also called Viterbi extraction or Baum-Viterbi algorithm since it involves the Baum re-estimation (Eqs. \label{eq:mu_hat} and \label{eq:sigma_hat}) and the Viterbi algorithm \citep{lember2008adjusted}. Instead of maximising the likelihood of all the data as in Eq. \ref{eq:alpha_recurs}, in VT the probability of only the most likely hidden sequence is maximised
  \begin{equation}
    \phi_N(T) = \max_i \{ \phi_i(T)a_{iN} \}
  \end{equation}
  for $1<i<N$ where
  \begin{equation}
    \phi_j(t) = \max_i \{ \phi_i(t-1)a_{ij} \} b_j(\boldsymbol{o}_t)
  \end{equation}
  and initially
  \begin{align}
    &\phi_1(1) = 1 \\
    &\phi_j(1) = a_{1j}b_j(\boldsymbol{o}_t).
  \end{align}
  for $1<j<N$. This alignment process finds an arc $ij$ for each observation $\boldsymbol{o}_t$ so that the last

  This results in an approximation of the maximum likelihood estimate (which was computed in the B-W algorithm)
  % ?
  .
  The Viterbi training is computationally less expensive than the B-W algorithm.
  In Viterbi training, the HMM parameters and the most probable hidden state sequence, both unknown, are estimated alternately. After updating the HMM parameters, the training data observations are aligned with the states, and within each state, a further alignment is made to align observations with mixture components. 
  % Using the Viterbi algorithm, the states are aligned with the observation sequence by maximising



  % AS stated in the Kaldi documentation, computed alignment is a sequence of transitions that corresponds to an utterance. 


\subsubsection{Phones in context}
  Phones of the same phoneme sound different when flanked by different phonemes. For this reason, contextual information is modelled, too, by assigning each triphone a HMM. 
  The monophone models are first trained
  % (with single-component Gaussians?)
  and the triphone models are initialised with the monophone model set parameters. The number of Gaussians in the triphone models is increased gradually and the parameters are re-estimated.

  % transition modelling
  % The transition probabilities from a state to another are essentially the counts of the transitions seen in the training corpus.



\subsubsection{State tying and phonetic decision trees}
  % tying
  The states of the phoneme HMMs are can be tied together so that the parameters of the output distributions of those states are shared. This makes the estimation of the parameters more robust because there are more training data occurrences, and also makes the total system more compact with fewer parameters \citep{young1992general}. States are clustered based on some metric of similarity.  

  % http://kaldi-asr.org/doc/tree_externals.html
  In tree-based clustering as described by \citet{young1994tree}, the states are divided into branches in a top-down optimisation procedure. Starting from the root node, the question that maximises the likelihood is selected for the node, with the data on each side of the divide being modelled by a single Gaussian.
  In a phonetic decision trees the questions are about the context of the phone, e.g. "Is the phone on the left of the current phone a fricative?". 

  After the procedure, the leaves of the tree are the state clusters in which the states are tied. In the final stage, leaves can be merged if the likelihood does not decrease more than a threshold value.

  After a tree has been constructed for the states of the triphone models, also previously unseen triphones can be synthesised by traversing the tree to the appropriate leaf node, i.e. cluster, by answering the questions about that triphone's context and using the tied states of that cluster.


\subsubsection{Adaptive training} \label{sec:adaptive_training}
  % alignments % lda + mllt
  % https://kaldi-asr.org/doc/transform.html

  % acoustic normalisation = feature space

  An ASR system 

  Each speaker has a distinct voice, which should be taken into account when training and using a general, speaker-independent ASR system. This can be done by representing each speaker's voice as a transform in the feature space. For each speaker, each feature vector is transformed by multiplying it by a matrix that encodes information about how the voice differs from the general speaker. The transform can be done when testing the final model (speaker adaptation, discussed in Section \ref{sec:adaptation}) and in the training process. The latter is called adaptive training.
  
  First, a transform is generated for each speaker. 

  % The Kaldi program 'transform-feats' is used to multiply the feature vectors with transform matrices

  An affine transform applied by appending a 1 to the feature vector, and multiplying it with the linear transform $\boldsymbol{A}$ concatenated with the constant offset (bias) $\boldsymbol{b}$: $\bracketVectorstack{  \boldsymbol{A} ; \boldsymbol{b}} \bracketVectorstack{\boldsymbol{o} \\ 1}$

  Linear transformations can be applied either in model-space or feature-space \citep{gales1998maximum}. MLLT and LDA transforms are speaker-independent

  fMLLR transform is speaker- or utterance-specific.

  % model-space or feature-space 
  % speaker- or utterance-specific

  % Speaker adaptive training \citep{povey2008fast}


\subsubsection{Discriminative training} \label{sec:mmi}

  % The maximum likelihood method aims to  
  % Typically?
  % Discriminative training is based on an objective function which is minimised or maximised using an optimisation algorithm.

  The MLE method described in Section \ref{sec:hmm_est} aims to maximise the likelihood of the observed sequence given the most probable HMM in VT, or all the possible HMMs in B-W. This method results in a model that predicts the training data by determining a probability distribution over the feature space, i.e., over all possible observations. This is called a generative model since the distribution can be sampled for predictions. However, the ability to produce examples of the modelled data is redundant in classification tasks. Furthermore, the MLE method maximises the likelihood of the observations for all of the competing HMMs independently of each other. The underlying task is to find the HMM that most accurately models the observation sequence, so it makes sense to also try to find the meaningful differences between HMMs.
  % The HMMs competing for being the most accurate model of the observations are not compared to each other.
  In discriminative training, instead of maximising the likelihood of the data given a generative model, a model is trained to discriminate between the classes, which in this case are different phoneme sequences. This way more capacity is used to model the boundaries between different HMMs, instead of using it to model the relations between individual HMMs and alignments. 

  The objective function can be simply the difference between the correct classifications (e.g., a phoneme sequence) for a set of examples and the classifications assigned to them by the model. This is called the minimum classification error (MCE) criterion.
  Another type of objective function is the maximum mutual information (MMI) criterion
  \begin{equation} \label{eq:mmi}
    \mathcal{F}_{\textup{MMI}}(M) = \sum_{r=1}^R \log 
      \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})}
        {\sum P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})}
  \end{equation}
  % equivalent to conditional maximum likelihood 
  % extended b-w

\subsubsection{Mapping phonemes to graphemes}

\subsubsection{Weighted finite-state transducers in Kaldi} \label{sec:wfst}

  Weighted finite-state transducers are a type of automaton in which a transition has an input label, an output label, and a weight. A special case of FST is a finite-state acceptor where the input and output labels of a transition are equal. FSTs, having both input and output labels, map an input sequence to an output sequence when a path is taken through it. FSTs are used in Kaldi, for example, in the lexicon FST (the file L.fst) to map a phoneme sequence to a grapheme sequence. FSAs are used in Kaldi to encode a grammar or a language model, in the file G.fst. 

  Kaldi uses the OpenFST \citep{} library for building and handling FSTs. 

  \begin{figure}
  \end{figure}


\subsubsection{Silence and pronunciation probability modelling}

  \citep{chen2015pronunciation} 




\subsubsection{Deep neural networks for acoustic modelling} \label{sec:dnn_am}

\subsubsection{Speaker identification} \label{sec:spk_id}

  Section \ref{sec:adaptive_training} discussed the differences between speakers and how the differences are modelled in adaptive training. 












\subsection{Spoken and written conversations in Finnish} \label{sec:conv}



\clearpage
\section{Methods}




\clearpage
\section{Experiments}

\subsection{Baseline}

The baseline language models and the ASR system is based on the systems developed by \citet{enarvi2017automatic}.





\clearpage
\section{Results}

\clearpage
\section{Conclusion} 


\clearpage
% for counting pages
\thesisbibliography

\bibliographystyle{apalike}
\bibliography{references}


% \begin{thebibliography}{99}

%% Alla pilkun j\"alkeen on pakotettu oikea v\"ali \<v\"alily\"onti>-merkeill\"a.
% \bibitem{Kauranen} Kauranen,\ I., Mustakallio,\ M. ja Palmgren,\ V.
%   \textit{Tutkimusraportin kirjoittamisen opas opinn\"aytety\"on
%     tekij\"oille.}  Espoo, Teknillinen korkeakoulu, 2006.

% \end{thebibliography}

%% Appendices
%% If you don't have appendices, remove \clearpage and \thesisappendix below.
% \clearpage
% \thesisappendix
% \section{}

\end{document}