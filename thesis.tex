%% License 
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %% This is licensed under the terms of the MIT license below.                 %%
  %%                                                                            %%
  %% Written by Luis R.J. Costa.                                                %%
  %% Currently developed at the Learning Services of Aalto University School of %%
  %% Electrical Engineering by Luis R.J. Costa since May 2017.                  %%
  %%                                                                            %%
  %% Copyright 2017-2018, by Luis R.J. Costa, luis.costa@aalto.fi,              %%
  %% Copyright 2017-2018 Swedish translations in aaltothesis.cls by Elisabeth   %%
  %% Nyberg, elisabeth.nyberg@aalto.fi and Henrik Wallén,                       %%
  %% henrik.wallen@aalto.fi.                                                    %%
  %% Copyright 2017-2018 Finnish documentation in the template opinnatepohja.tex%%
  %% by Perttu Puska, perttu.puska@aalto.fi, and Luis R.J. Costa.               %%
  %% Copyright 2018 English template thesistemplate.tex by Luis R.J. Costa.     %%
  %% Copyright 2018 Swedish template kandidatarbetsbotten.tex by Henrik Wallen. %%
  %%                                                                            %%
  %% Permission is hereby granted, free of charge, to any person obtaining a    %%
  %% copy of this software and associated documentation files (the "Software"), %%
  %% to deal in the Software without restriction, including without limitation  %%
  %% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
  %% and/or sell copies of the Software, and to permit persons to whom the      %%
  %% Software is furnished to do so, subject to the following conditions:       %%
  %% The above copyright notice and this permission notice shall be included in %%
  %% all copies or substantial portions of the Software.                        %%
  %% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
  %% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
  %% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
  %% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
  %% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
  %% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
  %% DEALINGS IN THE SOFTWARE.                                                  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%

%% spellcheck-on

%% Configurations, meta information, prefaces etc. 
  \documentclass[english, 12pt, a4paper, elec, utf8, a-1b, hidelinks]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
  %\documentclass[english, 12pt, a4paper, elec, dvips, online]{aaltothesis}

  %% Use the following options in the \documentclass macro above:
  %% your school: arts, biz, chem, elec, eng, sci
  %% the character encoding scheme used by your editor: utf8, latin1
  %% thesis language: english, finnish, swedish
  %% make an archiveable PDF/A-1b or PDF/A-2b compliant file: a-1b, a-2b
  %%                    (with pdflatex, a normal pdf containing metadata is
  %%                     produced without the a-*b option)
  %% typeset in symmetric layout and blue hypertext for online publication: online
  %%            (no option is the default, resulting in a wide margin on the
  %%             binding side of the page and black hypertext)
  %% two-sided printing: twoside (default is one-sided printing)

  \usepackage{amsfonts, amssymb, amsbsy, amsmath, natbib, graphicx, 
              tabstackengine, tabularx, longtable, multirow, booktabs}
  \numberwithin{equation}{section} % amsmath equation numbering 1.1, 1.2 etc.
  \usepackage{titlesec}
  \setcounter{secnumdepth}{4}
  \titleformat{\paragraph}
  {\normalfont\small\bfseries}{\theparagraph}{1em}{}
  \titlespacing*{\paragraph}
  {0pt}{3.25ex plus 1ex minus .2ex}{0ex plus .2ex}

  \graphicspath{ {./images/} }
  
  \renewcommand{\baselinestretch}{1.1}

  \DeclareMathOperator*{\argmax}{argmax}
  \DeclareMathOperator*{\softmax}{softmax}

  % \university{Aalto-yliopisto}
  % \school{Sähkötekniikan korkeakoulu}
  \degreeprogram{Computer, Communication and Information Sciences}
  \major{Signal, Speech and Language Processing}
  \code{ELEC3031}
  \univdegree{MSc}
  \thesisauthor{Anssi Moisio}

  %% A possible "and" in the title should not be the last word in the line, it
  %% begins the next line.
  %% Specify the title again without the linebreak characters in the optional
  %% argument in box brackets. This is done because the title is part of the 
  %% metadata in the pdf/a file, and the metadata cannot contain linebreaks.
  \thesistitle{Speech Recognition for Conversational Finnish} % working title
  %\thesistitle[Title of the thesis]{Title of\\ the thesis}
  \place{Helsinki}
  \date{\today}

  %% Thesis supervisor
  %% Note the "\" character in the title after the period and before the space
  %% and the following character string.
  %% This is because the period is not the end of a sentence after which a
  %% slightly longer space follows, but what is desired is a regular interword
  %% space.
  \supervisor{Prof.\ Mikko Kurimo}

  %% Advisor(s)---two at the most---of the thesis.
  \advisor{Dr.\ Tamás Grósz}
  \advisor{Dr.\ Mittul Singh}

  %% Aaltologo: syntax:
  %% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
  %% The logo language is set to be the same as the thesis language.
  \uselogo{aaltoRed}{''}

  %% The English abstract:
  %% All the details (name, title, etc.) on the abstract page appear as specified
  %% above.
  %% Thesis keywords:
  %% Note! The keywords are separated using the \spc macro
  \keywords{ASR\spc speaker embedding\spc LSTM\spc Transformer-XL}

  %% The abstract text. This text is included in the metadata of the pdf file as well
  %% as the abstract page.
  \thesisabstract{
    Spontaneous conversational Finnish is a challenging type of speech to recognise due to frequent dysfluencies in sentence structure and the use of various informal word forms. This thesis work was an effort to improve the speech recognition accuracy for conversational Finnish.
    The purpose was to evaluate recent acoustic and language modelling methods on conversational Finnish.
    The main experiments included evaluating the effect of different speaker embedding approaches and comparing Transformer-XL and recurrent neural language models, using word and subword vocabularies.
    Combining the best acoustic and language models built during this thesis work improved the word error rate by 4.9 absolute percentages compared to the previous best result.
  }

  %% Copyright text. Copyright of a work is with the creator/author of the work
  %% regardless of whether the copyright mark is explicitly in the work or not.
  %% You may, if you wish, publish your work under a Creative Commons license (see
  %% creaticecommons.org), in which case the license text must be visible in the
  %% work. Write here the copyright text you want. It is written into the metadata
  %% of the pdf file as well.
  %% Syntax:
  %% \copyrigthtext{metadata text}{text visible on the page}
  %% 
  %% In the macro below, the text written in the metadata must have a \noexpand
  %% macro before the \copyright special character, and macros (\copyright and
  %% \year here) must be separated by the \ character (space chacter) from the
  %% text that follows. The macros in the argument of the \copyrighttext macro
  %% automatically insert the year and the author's name. (Note! \ThesisAuthor is
  %% an internal macro of the aaltothesis.cls class file).
  %% Of course, the same text could have simply been written as
  %% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
  %% {Copyright \copyright{} 2018 Eddie Engineer}
  \copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
  {Copyright \copyright{} \number\year{} \ThesisAuthor}

  %% You can prevent LaTeX from writing into the xmpdata file (it contains all the 
  %% metadata to be written into the pdf file) by setting the writexmpdata switch
  %% to 'false'. This allows you to write the metadata in the correct format
  %% directly into the file thesistemplate.xmpdata.
  %\setboolean{writexmpdatafile}{false}

  %% All that is printed on paper starts here
  \begin{document}

  \pagenumbering{roman}

  %% Create the coverpage
  \makecoverpage

  %% Typeset the copyright text.
  %% If you wish, you may leave out the copyright text from the human-readable
  %% page of the pdf file. This may seem like a attractive idea for the printed
  %% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
  %% on the page. However, the recommendation is to print this copyright text.
  \makecopyrightpage

  %% Note that when writing your thesis in English, place the English abstract
  %% first followed by the possible Finnish or Swedish abstract.

  %% Abstract text
  %% The text in the \thesisabstract macro is stored in the macro \abstractext, so
  %% you can use the text metadata abstract directly as follows:
  \begin{abstractpage}[english]
    \abstracttext{}
  \end{abstractpage}

  \newpage
  %% Abstract in Finnish.
  \thesistitle{Puhekielisen suomen automaattinen tunnistus}
  \supervisor{Prof. Mikko Kurimo}
  \advisor{TkT Tamás Grósz}
  \advisor{TkT Mittul Singh}
  \degreeprogram{Tieto-, tietoliikenne- ja informaatiotekniikka}
  \major{Signaalin-, puheen- ja kielenkäsittely}
  %% The keywords need not be separated by \spc now.
  \keywords{puheentunnistus, puhekielinen suomi}
  %% Abstract text
  \begin{abstractpage}[finnish]
    % Spontaneous conversational Finnish is a challenging type of speech to recognise due to frequent dysfluencies in sentence structure and the use of various informal word forms.
    Puhekielistä suomea on vaikea mallintaa arkikielisten ilmaisujen sekä katkonaisten lauseiden takia. 
    % This thesis work was an effort to improve the speech recognition accuracy for conversational Finnish.
    Tämän diplomityön tarkoitus oli parantaa puheentunnistusjärjestelmien tarkkuutta puhekieliselle suomelle. 
    % The purpose was to evaluate recent acoustic and language modelling methods on conversational Finnish.
    Tavoitteena oli arvioida viimeaikaisten kielen- ja akustiikanmallintamismenetelmien hyödyllisyyttä puhekielisen suomen tunnistamisessa.
    % The main experiments included evaluating the effect of different speaker embedding approaches and comparing Transformer-XL and recurrent neural language models, using word and subword vocabularies.
    Työn tärkeimpiä kokeita ovat puhujan suhteen mukautettujen akustiikkapiirteiden vertailu (i-vektorit ja x-vektorit) sekä Transformer-kielimallien ja takaisinkytkettyjen hermoverkkokielimallien vertailu.
    % Combining the best acoustic and language models built during this thesis work improved the word error rate by 4.9 absolute percentages compared to the previous best result.
    Yhteensä kieli- ja akustiikkamallikokeet pienensivät puheentunnistusjärjestelmän sanavirheastetta 4.9 prosenttiyksikköä verrattuna aiempiin tuloksiin.
  \end{abstractpage}

  %% Preface
  %% This section is optional. Remove it if you do not want a preface.
  % \mysection{Preface}
  % preface here

  % \vspace{5cm}
  % Helsinki, 30.12.2020

  % \vspace{5mm}
  % {\hfill Eddie E.\ A.\ Engineer \hspace{1cm}}

  \newpage
  \thesistableofcontents
%%

%% Symbols and abbreviations 
  \mysection{Symbols and abbreviations}

  \subsection*{Symbols}
  \begin{tabular}{ll}
  % examples
  % $\mathbf{B}$  & magnetic flux density  \\
  % $c$              & speed of light in vacuum $\approx 3\times10^8$ [m/s]\\
  % $\omega_{\mathrm{D}}$    & Debye frequency \\
  % $\omega_{\mathrm{latt}}$ & average phonon frequency of lattice \\
  % $\uparrow$       & electron spin direction up\\
  % $\downarrow$     & electron spin direction down \\
  % /examples
  $\boldsymbol{\Delta}$                       & delta feature vector \\
  $\boldsymbol{\Delta\Delta}$                & delta-delta feature vector \\
  $\theta$                          & the learnable parameters of a model \\
  $\lambda$                                   & language model weight \\
  $\boldsymbol{\mu}$                          & mean vector of a GMM \\
  $\Sigma$                       & covariance matrix of a GMM \\
  
  $\boldsymbol{b}$ & a bias vector in a neural net layer or affine transform \\
  $\boldsymbol{h}$      & high-level representation vector in an encoder-decoder model \\
  $\boldsymbol{o}_t$    & observation, i.e., feature vector, at time $t$ \\ 
  $P()$                 & probability \\
  $r$                   & speaker \\
  $\boldsymbol{w} = w_1,...,w_N$ & a sequence of words or other units of text \\
  $W$                     & a weight matrix in a neural net layer \\
  $x$                   & input of a neural network \\
  $y$                   & output of a neural network \\

  
  
  \end{tabular}

  % \clearpage
  \subsection*{Abbreviations}
  \begin{longtable}{ll}
  AM            & acoustic model \\
  ANN           & artificial neural network \\
  ASR           & automatic speech recognition \\
  BERT          & bidirectional encoder representations from transformers \\
  BPE           & byte pair encoding \\
  CER           & character error rate \\
  CMN           & cepstral mean normalisation \\
  CMVN          & cepstral mean and variance normalisation \\
  DNN           & deep neural network \\
  EM            & expectation maximisation \\
  fMLLR         & feature space maximum likelihood linear regression \\
  FSA           & finite-state acceptor \\
  FST           & finite-state transducer \\
  GLUE          & the general language understanding evaluation \\
  GMM           & Gaussian mixture model \\
  GRU           & gated recurrent unit \\
  HMM           & hidden Markov model \\
  LDA           & latent Dirichlet allocation \\
  LDA           & linear discriminant analysis \\
  LF-MMI        & lattice-free MMI \\
  LM            & language model \\
  LSTM          & long short-term memory \\
  LVCSR         & large vocabulary continuous speech recognition \\
  MAP           & maximum a posteriori \\
  MCE           & minimum classification error \\
  MFCC          & mel-frequency cepstral coefficient \\
  ML            & maximum likelihood \\
  MLE           & maximum likelihood estimation \\
  MLLR          & maximum likelihood linear regression \\
  MLLT          & maximum likelihood linear transformation \\
  MLP           & multilayer perceptron \\
  MMI           & maximum mutual information \\
  MPE           & minimum phone error \\
  NCE           & noise contrastive estimation \\
  NLP           & natural language processing \\
  NLU           & natural language understanding \\
  NN            & neural network \\
  NNLM          & neural network language model \\
  OOV           & out of vocabulary \\
  PDF           & probability density function \\
  PPL           & perplexity \\
  ReLU          & rectified linear unit \\
  RNN           & recurrent neural network \\
  SAT           & speaker-adaptive training \\
  SD            & speaker dependent \\
  SI            & speaker independent \\
  TDNN          & time delay neural network \\
  VAD           & voice activity detection, here synonymous with speech activity detection \\
  VT            & Viterbi training \\
  VTLN          & vocal tract length normalisation \\
  WER           & word error rate \\
  WFST          & weighted finite-state transducer \\
  \end{longtable}

  %% \clearpage is similar to \newpage, but it also flushes the floats (figures
  %% and tables).
  \cleardoublepage
  \pagenumbering{arabic}
  \setcounter{page}{1}
%%

\section{Introduction} \label{sec:introduction}
  \thispagestyle{empty} 
  % tables about colloquial finnish and research qs

  %  maybe subsections: intro, conv finnish, research questions, other contributions (improvement to previous results, implementation)

  %   MOVE 1 CURRENT SITUATION (compulsory)
    % (What is the wider context of the topic studied?)
    % Step 1: Making a centrality claim
    % (Why is this topic area important or relevant?)
    % AND
    % Step 2: Making topic generalizations
    % (What features are important to know about this topic?)
    % AND/OR
    % Step 3: Reviewing previous research and solutions
    % (What earlier solutions or related work have been done in this area?)

    % MOVE 2 PROBLEMS (compulsory)
    % (What is wrong or missing from current solutions?)
    % Step 1: Indicating weaknesses
    % (What are the limitations or inadequacies of current solutions?)
    % AND/OR
    % Step 2: Identifying a gap
    % (What area remains unstudied or overlooked in current solutions?)
    
    % MOVE 3 PROPOSING A NEW APPROACH (optional)
    % (What new strategy could solve this problem?)
    % Step 1: Introducing a potential solution
    % (What new idea could work?)
    % AND
    % Step 2: Justifying the approach
    % (What are the advantages or benefits of this new approach?)

    % MOVE 4 YOUR SOLUTION (compulsory)
    % (How will your thesis overcome this problem or fill this gap?)
    % Step 1: Research Aims (required)
    % (What will the thesis achieve?)
    % Step 2: Methods (required)
    % (How will you use to develop and verify/validate your solution?)
    % Step 3: Thesis Scope (optional)
    % (How much of the topic / problem will you address in the thesis?)
    % Step 4: Describing the main outcome (optional)
    % (What are the features of your solution?)
    % Step 5: Overview of thesis structure (required)
    % (What are the aims and contents of the following chapters?)

  \subsection{Automatic speech recognition} 

    \emph{Automatic speech recognition} (ASR) is the task of converting speech into text. The direct application of ASR is useful in many situations, for example to transcribe patient notes dictated by medical doctors. Speech recognition has also an increasing number of use cases in systems with longer pipelines that achieve some speech-initiated task, such as \emph{voice user interfaces} or \emph{speech-to-speech translation} systems. The ASR piece can often be a bottleneck in the pipeline,  determining to a large degree the accuracy of the whole system.

    % maybe lose the eqs
    In the conventional ASR system, the task is divided into subtasks.
    The first subtask, called \emph{feature extraction}, is to divide the audio signal into  segments, and convert the segments into feature vectors, also called observations. The observations are a compressed representation of the audio signal.
    % % : \emph{feature extraction},  \emph{phoneme-to-grapheme mapping}, and creating
    % The task is then to find $\argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O})$, where $\boldsymbol{w}=w_1,...,w_N$ is a word sequence. This probability is not practicable to compute directly, but by Bayes' rule it can be expanded to 
    % \begin{equation}\label{eq:asr-bayes}
    %   \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
    %   \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
    %   = \argmax_{\boldsymbol{w}} \{ P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w}) \}
    % \end{equation}
    % The probability of the observations ${P(\boldsymbol{O})}$ in the denominator is not relevant in finding the best transcription ($\argmax_{\boldsymbol{w}}$) for the observations, which leaves the product in the numerator to be estimated. This product includes t
    The two most significant subtasks are \emph{acoustic modelling} and a \emph{language modelling}. A language model (LM) generates an \emph{a priori} probability distribution  over possible word sequences. For example, the transcription "the god of thunder was Zeus" should probably be assigned a larger probability than "the god of thunder was juice" even before any speech audio is processed.
    An acoustic model (AM) outputs likelihoods of observations conditional on phoneme sequences. The phoneme sequences are mapped to words by a \emph{lexicon}, after which these a posteriori probabilities can be combined with the a priori probabilities of the language model, yielding an estimation of the optimal transcription for a given speech audio.
    %  (also called a \emph{pronunciation dictionary}).
    % , yielding $P(\boldsymbol{O}|\boldsymbol{w})$.
    % The probabilities of the LM and AM are combined to estimate the most likely transcription of the speech audio.
    %  \emph{a posteriori} probabilities $P(\boldsymbol{O}|\boldsymbol{w})$ of phoneme sequences conditional on the speech audio. A dictionary is used to map the phoneme sequences to words, or more generally, the same set of tokens that the LM uses, which can also be subword units or characters, for instance. 
    % The former factor $P(\boldsymbol{w})$ is the a priori probabilities of word sequences, modelled by language models. 
    % To avoid numerical underflow, the probabilities are converted to the logarithmic domain. A scalar weight $\lambda$ is added to determine how significant the LM probabilities are compared to the AM probabilities.
    % \begin{equation}\label{eq:asr-bayes-2}
    %   \argmax_{\boldsymbol{w}} \{ 
    %   P(\boldsymbol{w})^{\lambda}P(\boldsymbol{O}|\boldsymbol{w}) \} 
    %   = \argmax_{\boldsymbol{w}}
    %     \{ \lambda  \log \{ P(\boldsymbol{w}) \} +
    %      \log \{ P(\boldsymbol{O}|\boldsymbol{w}) \} \}
    % \end{equation}

    Acoustic and language modelling are achieved using machine learning models, primarily deep neural networks (DNNs), which are estimated based on training data. Modelling acoustics requires a parallel corpus of speech and correct transcriptions, whereas modelling language only requires text. 


  \subsection{Spontaneous, conversational Finnish}
    % move these up
    The difficulty of ASR depends on how varied and noisy the speech audio signals are.
    The confined problem of recognising a few different words pronounced clearly by one speaker
    % recorded in noise-free conditions
    was essentially solved
    % some
    years ago. However, speech recognition becomes more difficult when the speech is continuous and recorded in differing noise conditions from many speakers. Current state-of-the-art ASR systems are nearing human-level recognition accuracy also in the \emph{large vocabulary continuous speech recognition} (LVCSR) task if the speech is planned and pronounced clearly, as it is, for example, in broadcast news or parliament sessions. Yet, spontaneous, informal conversations remain a challenging type of speech to recognise, and the gap between human- and machine-level accuracy is still large for this type of speech.

    The difficulty depends also on the language. The most obvious factor is the availability of training data. The state-of-the-art ASR systems are based on DNNs that require large training data sets. For languages such as Finnish, the resources are more limited than for the most widely spoken languages in the world, which makes the ASR task harder. Thus, achieving accurate ASR for small languages will require more work.

    % The quality and quantity of the training data pose additional challenges for ASR, as 
    
    % the  state-of-the-art ASR systems are based on deep neural networks (DNN) that require large training data sets. 
    
    % For some of the most prominent languages, such as English or French, the size of the speech data sets enables training 

    % \subsection{Spoken and written conversations in Finnish} \label{sec:conv}

    Some inherent idiosyncratic properties of Finnish 
    % make it easier a language for ASR, while other properties make it more difficult a task, compared to ASR for other languages.
    should also be taken into account when developing a Finnish ASR system.
    Finnish is a morphologically rich language. Suffixes and other conjugations perform grammatical functions, such as cases, which in
    % many Indo-European
    other 
    languages such as English would be denoted by separate words. This makes the number of word types in the vocabulary large, requiring not only a considerable amount of computational resources but also much text for training the LM. This problem can be avoided by segmenting words into smaller units, referred to as \emph{subwords} \citep{hirsimaki2006unlimited}.
    
    However, the morphology of colloquial spoken Finnish often differs from that of the formal language, with some of the suffixes and other inflections often being omitted or changed to an incorrect one.
    % For example,it is common to not use the first person plural inflections for verbs, and instead use the passive voice verb inflection ("me ollaan" instead of the correct form "me olemme") or the incorrect singular form when the third person plural form should be used ("ne on" instead of the correct "ne ovat").
    For example, it is common to replace  first person plural inflections  with the passive voice verb inflection ("me ollaan" instead of the correct form "me olemme") or to use the incorrect singular form when the third person plural form should be used ("ne on" instead of the correct "ne ovat").
    Other common characteristics of informal Finnish include  shortening words (e.g., from "minä" to "mä") and/or combining words (e.g., "oliko se" to "olikse"). The differences between formal and informal Finnish pose difficulties when modelling informal language: text from formal sources such as books or newspapers does not resemble colloquial Finnish very closely.
    However, another relevant feature of the Finnish language is its phonemic orthography, i.e., the fact that one letter generally corresponds to one phoneme, and vice versa. As a result, it is possible
    % , and to a varying degree common,
    to also \emph{write} colloquial Finnish as it is spoken. 
    It is therefore possible to find written text that resembles the spoken language, and  use this to model colloquial Finnish. The above mentioned incorrect inflections are examples of informal Finnish that is also written in the incorrect way, as it is pronounced.
    Colloquial Finnish is written, for example, on online forums, especially in direct-message conversations. 
    \citet{enarvi2013studies} collected a conversational Finnish text corpus from Internet forums by searching for key phrases that indicate informal conversations. This text corpus is similar in style to informal spoken Finnish, and it can be used to model conversational Finnish.
    % This text corpus is used also in this thesis to model conversational Finnish.
    % \citet{enarvi2017automatic} developed and evaluated different ASR systems for the conversational data.
    % In their work, the acoustic models are trained on 85 hours of speech.
    % In their work, a first pass of large-vocabulary decoding and word lattice generation is done using an n-gram language model trained on the text corpus. A second pass of rescoring the lattices and generating transcripts is done using a recurrent neural network language model trained on the same text corpus.
    % Subword-vocabulary language models based on statistical segmentation of words \citep{creutz2002unsupervised, creutz2007unsupervised} were found to perform better than a word vocabulary.
    % In the years after these publications, many new acoustic and language modelling methods have been developed. 
    % Both the speech data set used in this thesis and the previous best results are described by \citet{enarvi2017automatic}.
  % 3. What will be your methodology?
  % \subsection{Related work and methodology}
    % This thesis aims to experiment with some of the latest  methods to improve upon the previous best results obtained for this informal, spontaneous Finnish conversation speech data set. 
  % 
  \subsection{Research questions and contributions}
    %  maybe move to background -- expand on research context -> speaker adaptation, better LMs
    The purpose of this thesis is to improve the ASR accuracy for the spontaneous, conversational Finnish corpus.
    The aim is to evaluate recent acoustic and language modelling methods on the data and assess whether they can improve the recognition results.
    % Furthermore, the ASR pipeline, which includes the acoustic models and language models built with different toolkits, is rebuilt using the newest versions of the toolkits.
    % The experiments showed that utilising the latest Kaldi recipes for acoustic modelling improves the recognition accuracy compared to older recipes.
    These recent methods include speaker-adaptive features to improve the AM
    % . The language modelling experiments included comparing recurrent neural LMs with 
    and transformer LMs to improve the LM scores. The research questions of this thesis are:
    \begin{enumerate}
      % for Finnish conversational data
      % leave out scores
      % 
      \item Can the recognition accuracy be improved using new Kaldi recipes? 
      \item Speaker-adaptive features: i-vectors vs x-vectors
        \begin{itemize}
          \item Can x-vectors achieve better results than the previously used i-vectors?
          \item Do extractors pre-trained on a different language (English) with more data improve the ASR accuracy?
          \item Is it beneficial to concatenate i- and x-vectors?
        \end{itemize}
      \item RNN vs Transformer language models:
      \begin{itemize}
        \item Can transformer models achieve better results than the RNN models when rescoring the n-best lists generated with n-gram LM scoring?
        \item Can transformer models improve the recognition results when rescoring the n-best lists generated with RNN LM scoring?
      \end{itemize}
    \end{enumerate}
    Other contributions of this thesis work include tuning the hyperparameters of LMs and rebuilding the ASR system, which is based on the models developed by \citet{enarvi2017automatic}. The programming code is available on
    Github\footnote{AM experiments: \url{https://github.com/anmoisio/keskustelu2020},\newline
    LSTM experiments: \url{https://github.com/anmoisio/conv_lm},\newline 
    Transformer-XL experiments: \url{https://github.com/anmoisio/transformer-xl}}.

    % In the previous decade, i-vectors \citep{ivector} have been used to model speaker and channel variability. I-vectors can be added to the features, providing information that enables the ASR system to adapt to differing channels and speakers  during both training and testing. \citet{snyder2018x} described how also DNNs can  be utilised to create speaker embeddings, called x-vectors, which model differences between speakers. In this work, the effects of i-vectors and x-vectors on the ASR accuracy are evaluated and compared on the conversational Finnish data.
    % The experiments aim to determine whether one of the methods is more suitable for this type of data.

    % \citet{vaswani2017attention} introduced a neural network architecture for language modelling, called the Transformer, which is based solely on (self-)attention mechanisms \citep{bahdanau2014neural}. Since then, the state-of-the-art language models have been of the transformer model type for many language processing tasks.
    % One of the advantages of attention mechanisms is that they are able to exploit parallel computing, unlike the previously common recurrent neural networks (RNN), since their hidden states do not depend on the hidden states of previous time steps.
    % In this thesis, Transformer-XL \citep{dai2019transformer} language models are trained and evaluated on the conversational Finnish data. The goal is to determine whether the transformer models achieve better results than the RNN models on the conversational Finnish data.
    % Word and subword vocabularies are compared for n-gram LMs, RNN LMs and transformer-XLs.
    % Other language modelling experiments in this work include tuning the hyperparameters and comparing word and subword vocabularies for n-gram LMs, RNN LMs and transformer-XL LMs. 
    % , tuning the hyperparameters of the language models (including constant- and variable-order \citep{siivola2007morfessor} n-gram models, RNN LMs as well as Transformer-XLs)

    % in the ASR task, in a similar manner as described by \citet{jain2020finnish}.
    % The baseline system is the same in this thesis, and the work begun by replicating the previous results. The Kaldi toolkit includes acoustic model training pipelines, called "recipes", that are tuned to achieve optimal results for a particular speech data set. In the past three years after the above mentioned previous best results were achieved, the Kaldi recipes have been developed further, and the latest machine learning algorithms have been implemented in the toolkit. By applying the latest Kaldi recipes for the Finnish speech data used in this thesis, the previous best results can be improved. Other acoustic modelling experiments of this work include modelling the speaker and channel variability using i-vectors \citep{ivector} and x-vectors \citep{snyder2018x}.

    % , and experimenting with topic modelling (see e.g., \citet{xiong2018session, xing2016topic}).

    % This thesis explores methods for improving ASR for conversational Finnish.

    % \subsection{The basic structure of an ASR system} 
    % \subsection{Finnish conversations to text} 

    % possibly a bulletin list
    %  more interesting info here
    % The thesis is divided into seven chapters. 
    The background theory of the utilised acoustic and language modelling methods is described in three separate chapters. Chapter \ref{sec:ml} defines some of the main concepts of machine learning as well as the basic structure of an ASR system. Chapter \ref{sec:lm} describes the relevant language modelling techniques: the n-gram, RNN, and transformer language models. Acoustic modelling and the Kaldi toolkit are described in Chapter \ref{sec:am}.
    % The first
    % % four
    % sections of this chapter respectively describe the
    % % four
    % components of the conventional ASR system.
    % The Section \ref{sec:conv} discusses the specifics of recognising conversational Finnish.
    %  A language model gives an a priori probability for each word sequence: $P(\boldsymbol{w})$. The estimate $\hat{\boldsymbol{w}}$ of the best transcription given the observations is the most likely sequence 
    % \begin{equation}\label{asr-bayes-argmax}
    %   \hat{\boldsymbol{w}} = \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
    %   \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
    % \end{equation} 
    Chapter \ref{sec:experiments} presents the experiments and results. 
    Chapter \ref{sec:discussion} discusses the results and describes possible future work on the subject.
    Chapter \ref{sec:conclusion} concludes the thesis by answering the research questions.



    % history of ASR
    % typical ASR system


\clearpage
\section{Machine learning} \label{sec:ml} 

  % \subsubsection{Models} 

    A \emph{model} is a simplified or abstract representation of a phenomenon, an artefact that aims to hold relevant and distilled information about its real-world counterpart.
    % The term is very general: there are physical, mathematical, and conceptual models, just to name a few different types.
    Models can be either physical or conceptual, and the latter group can be subdivided into many different kinds of conceptual models, such as logical, scientific, or mathematical models.
    One category of mathematical models is \emph{statistical models}.
    According to \citet{davison2003statistical}, a statistical model is "a probability distribution constructed to enable inferences to be drawn or decisions made from data". A basic example of a probability distribution is a normal, or \emph{Gaussian}, distribution, whose shape is defined by its \emph{parameters} $\theta$, the mean and variance. Given a set of data points, an optimal Gaussian distribution can be superimposed on the data by simply calculating the parameter values from their
    formulae\footnote{The mean is $\mu = \frac{1}{N} \sum_i x_i$ and the variance is $ \sigma^2 = \frac{1}{N-1} \sum_i (x_i - \mu)^2$ where $x_i$ is a data point and $i \in \{ 1,...,N \}$.}.
    However, if the data points are not assumed to be from a simple univariate Gaussian and more degrees of freedom are introduced to the model, there are no simple formulae from which to determine the parameter values. Instead, a numerical solution need to be sought, which is the task of mathematical optimisation.
    % aasinsilta

    \emph{Machine learning} is often formulated as optimisation of an objective function, with the added goal of being able to generalise to unseen samples of data.  Most relevant to this thesis, statistical models of language and speech acoustics are created using machine learning algorithms.
    This chapter briefly introduces machine learning concepts that are central to the language and acoustic modelling methods described respectively in the next two chapters.
    
    % 
      % Brains have evolved to build both unconscious and  consciously accessible models of consequential things, such as animals, weather, or phonemes that compose a language.
      %     % of plants, of the body that contains the brain, of weather, water etc.
      %     % representations of time, space, temperature etc. 
      %     Model of any of these kinds, however, is ...

      %     The abstraction and simplification can happen by grouping similar things into one. For example, a mental model of a tree integrates many individual leaves into one foliage, and
      %     % , more fundamentally, 
      %     a visual model of a leaf integrates many chloroplasts and other small things into one green surface that is perceived when looking at a leaf.

      % \subsection{Fitness criterion}

      % A mathematical model is often formulated as a function that generates an output given an input, emulating how the real-world phenomenon behaves.
      % % Inputting a knife into a coconut outputs coconut milk, and an accurate mental model of a coconut should be able to predict that a coconut will function in this manner.
      % More generally, a model can be any kind of mapping from a domain to another. The acoustic and language models used in ASR are formulated as probabilistic models that output probabilities for a given input: how probable is a sequence of words, or how likely is a sequence of audio observations conditional on a particular phoneme sequence.
          
      % %     % ; via lowering the resolution: a mental model of a 

      % % \subsubsection{Estimating models statistically}

      % There are two directions, ways to create a model of a phenomenon: top-down or bottom-up\footnote{In mathematics, solving a function in a top-down fashion is called an analytical or a symbolic solution, and bottom up a numerical solution.}.
      % % 
      % Designing from top down means roughly to analyse the task on a high-level and deducing from that how the model should function in the lower levels.
      % For example, a chess program can be programmed top-down using if-else clauses, e.g. "if pawn is in D7 and D8 is empty, move pawn to D8". To build a competent chess program this way requires a good understanding of what moves are beneficial. Top-down design of models is an exclusively human undertaking.
      % % , and a rare way to build a model.
      % % a recipe for a dish, which is then used to cook up the real thing. This way the simplified model is created without the real phenomenon existing yet, or ever.
      % % However, top-down design depends on previous models that have ultimately been estimated from bottom up. 
      % A bottom-up generation of a model is the more usual way models come to being. In this approach, the abstract model is estimated from data gathered from the manifest world. In the chess example, the designing work can be delegated to an algorithm that performs bottom-up estimation of the desired behaviour using machine learning. In practice, the program learns by trial and error, preferring (reinforcing) those moves that have preceded winning. When the actual decisions of moves are left for the algorithm, the programmer herself need not understand the game. By merely defining the learning algorithm and leaving the model to teach itself, the model can eventually surpass the human-level competence, as has happened with most board games, among other tasks.
      % Some tasks are inherently so complex that no one has seriously tried to build rule-based models of them. As the behaviour to be modelled becomes too complex and non-linear for a human to design it, the only way is to teach it to the machine by brute-force trial and error. 
      
      % a recipe for the dish can be developed by experimenting with random recipes in some finite search space of possible recipes, tasting the finished product, and iteratively narrowing down the search space, homing in on an optimal experience. Once a cook has done enough bottom-up design, top-down design becomes possible by deducing from the mental model what a never-before-experienced combination of ingredients will taste like.
      
      

      %   In computer science, top-down design has been the usual way to develop systems.
      %   % If the function to be estimated is simple enough
      %   For example, the best computer chess programs were based on rules until the first decade of this century. In recent years, however, the designing work has been more and more often delegated to an algorithm that performs bottom-up estimation of the desired behaviour. This is referred to as machine learning\footnote{More accurately, machine learning is not the only bottom-up estimation approach, an example of another one being optimisation, which is similar to machine learning but does not aim to generalise beyond the data set used. Machine learning involves optimisation algorithms but what separates it from mere optimisation is the desire to build a model that can generalise to data that was not used for building the model.}. The best chess programs no longer do decisions by following human-written if-then clauses; instead they have estimated the mapping that inputs chess board configurations and outputs chess moves by playing a large number of games against themselves and taken notes about which moves lead to winning. As the behaviour to be modelled becomes too complex and non-linear for a human to design it, the only way is to teach it to the machine by brute-force trial and error. 

  \subsection{Artificial neural networks} \label{sec:dnn_intro}

    Artificial neural networks (ANN) are a family of model types, historically based on the concept of a perceptron \citep{rosenblatt1957perceptron}, which functions similarly to a biological neuron and is therefore called an artificial neuron, or just neuron.
    The core functioning, common to the perceptron and the biological neuron, is that an input either activates or does not activate the output, based on an activation function. In the case of a perceptron, the activation function is a unit step function: if the input is above a scalar threshold the output is activated. Perceptron learns a binary classifier by tuning this threshold based on information from training examples. The idea of a perceptron has since been generalised and extended into the wide range of different kinds of artificial neural networks, but the core idea of a neuron remains.
    
    A multilayer perceptron (MLP) \citep{rosenblatt1962principles} stacks neurons in multiple layers that are connected in the forward direction from the input to the output layer, therefore called also feedforward neural networks. An MLP includes an input layer of neurons with linear or non-linear activation functions, one or more hidden layers, and an output layer. The layered network learns the weights of the connections through backpropagation and other algorithms described in the next two subsectons.
    
    An important class of ANNs are deep neural networks (DNNs), called "deep" because they include many hidden layers. MLPs are a subclass of DNNs; other subclasses include, for instance, recurrent neural networks (RNNs), which are particularly useful in speech recognition since they are capable of modelling long-term time dependencies in data by incorporating a cycle between the network connections. Section \ref{sec:rnn} discusses RNNs for language modelling.

  \subsection{Objective function} 

    % The mapping from input to output is determined by the parameters $\theta$ that are learned. 
    There are different approaches to defining how good a model is, given a training dataset. Using \emph{maximum likelihood estimation} (MLE), parameters of a model are adjusted to maximise how probable the model judges the data set. The maximum likelihood estimate $\theta_{\textup{ML}}$ of the model parameters is the point in the parameter space that maximises the likelihood of the observed data. Incorporating some Bayesian reasoning into the approach, when a prior distribution is considered in addition to the current data MLE becomes maximum a posteriori (MAP) estimation.  The confidence of how good the MLE/MAP point estimate is is related to its variance. If alternative samplings of the observations are likely to change the estimate, the variance is larger.  
    
    In contrast to the MLE/MAP approach,
    % and frequentist statistics in general,
    the complete Bayesian approach is to generate a full probability distribution function for $\theta$. When the model is used to estimate the probability of an observation, the distribution is integrated over the parameter space, and the probabilities of the observations given by each parameter point estimate is weighted by how probable the parameter point is. This way the confidence on the parameter estimate is incorporated in the data likelihood. However, using a full probability distribution instead of a point estimate is computationally expensive, and therefore the MLE/MAP method is the more commonly used approach in machine learning. 

    % To tune the model for a given data set
    A machine learning process can be framed in different ways, and many related and synonymous terms are used in the literature.
    In practice, a model is trained by optimising an \emph{objective function} that encodes the goal of the model. This function is also called the \emph{cost}, \emph{error} or \emph{loss function} when the output of the function is minimised, \emph{utility function} when it is maximised, or generally the \emph {criterion}. Maximising the likelihood of the parameters is often phrased as minimising a cost function. More specifically, the aim is to minimise the \emph{cross entropy} between the observed distribution and the model distribution, or equivalently, to minimise the negative log-likelihood of the model, given the data \citep{Goodfellow-et-al-2016}. 

    
    % incorporate the degree of certainty to the model itself

  \subsection{Optimisation} 

    Depending on the criterion for an optimal model, a optimisation method is chosen and applied to tune the learnable parameters $\theta$ of a model.  The choice of optimisation algorithm depends also on other factors, one of which is whether the model includes latent variables.
    A latent (i.e., hidden) variable is a variable that is not directly observed, but can affect the observed variables. In general, a latent variable is some phenomenon in the real world that is not directly observed, but whose properties are inferred from measurements that are observable.
    Two types of models that include latent variables are GMMs and HMMs (see Section \ref{sec:phoneme_hmm}), both of which are central to the conventional methods of automatic speech recognition. Commonly in GMMs, one latent variable is the identity of the mixture component $c$ of a data point $o$, and the probability distribution $P(o)$ is determined by the joint distribution $P(o,c)=P(o|c)P(c)$.
    Hidden Markov models are even named after the latent, hidden sequence that generates an observed sequence\footnote{They are also named after the Russian mathematician Andrey Markov.}; in speech recognition the latent sequence might be a sequence of phones that generate the sequence of processed audio signal chunks, referred to as features (see Section \ref{sec:feats}).
    % 'Maximum likelihood modeling with Gaussian distributions for classification'
    % gupta2011theory

    When the model to be estimated includes latent variables, the maximum likelihood estimate can be computed using the \emph{expectation maximisation} (EM) algorithm.
    % Maximising the expectedness of the data can be turned around and seen as minimising how surprising the training data are to the model by modifying the model.
    Expectation maximisation is an iterative method to finding a local maximum for the likelihood of a set of data given a model.
    The EM algorithm consists of iterating the expectation step (E-step) and the maximisation step (M-step). Roughly, the E-step estimates the latent variables by which a function can be created for calculating the likelihood of the parameters. The M-step then finds the parameter values that maximise the likelihood. Section \ref{sec:hmm_est} describes an EM algorithm for estimating the HMMs for modelling phonemes, called the Baum-Welch algorithm.
    
    An optimisation algorithm used in many machine learning approaches, and especially in training DNNs, is called \emph{gradient descend}.
    This algorithm minimises the cost function by taking steps in the opposite direction of the gradient of the cost function for a given batch of training examples, since this is the direction of steepest decline. An important hyperparameter\footnote{Those parameters that are not learned but are set before the learning are called \emph{hyperparameters} of the model.} is the \emph{learning rate}, which determines the size of the step. 
    An important part of gradient descend is the backpropagation algorithm, described by \citet{rumelhart1986learning}, which propagates the cost of the output backward in the DNN to calculate the gradient. 

    % tdnn
    % Residual connections \citep{he2016deep}
    % highway networks

  \subsection{Machine learning for speech recognition}

    % Brains, too, build 
    % % mental
    % models, conscious and subconscious, of phenomena that they encounter in their environment, which is the basis of how animals are able to perceive
    % % , 
    % % understand, predict, 
    % and act in an environment.
    % Language is a prominent phenomenon in the environment that humans live in, and the human brain naturally develops models to process and generate language. Humans learn a language gradually through exposure to it. Similarly, to build a statistical model of a language or its pronunciation, machine learning algorithms are used.

    % Machine learning has been an integral part of ASR 

    Nowadays, machine learning is an integral method of speech recognition, but the first ASR systems in the 1960s did not utilise learning algorithms. These machines could recognise a few words, for instance the digits, by matching templates of the words with the data. In the 1970s and 80s, HMMs were demonstrated useful in ASR after which HHM-based systems, and learning algorithms in general, have remained as the basis of ASR. The breakthrough of neural models during the previous decade provided much improvement in the recognition accuracy. Incorporating a DNN into the acoustic model of a conventional HMM-based system is called the hybrid DNN/HMM approach.
    % The latest paradigm of ASR systems, called end-to-end systems, dispenses also with HMMs, relying completely on DNNs. 

    % aasinsilta

    In the conventional and hybrid ASR systems, the task is divided into subtasks. The first subtask, called feature extraction, is to divide the audio signal into $T$ segments, and convert the segments into feature vectors, also called observations, $\boldsymbol{O}=\boldsymbol{o}_1,...,\boldsymbol{o}_T$. The observations are a compressed representation of the audio signal.
    % : \emph{feature extraction},  \emph{phoneme-to-grapheme mapping}, and creating
    The task is then to find $\argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O})$, where $\boldsymbol{w}=w_1,...,w_N$ is a word sequence. This probability is not practicable to compute directly, but by Bayes' rule it can be expanded to 
    \begin{equation}\label{eq:asr-bayes}
      \argmax_{\boldsymbol{w}} P(\boldsymbol{w}|\boldsymbol{O}) = 
      \argmax_{\boldsymbol{w}} \frac{P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w})} {P(\boldsymbol{O})}
      = \argmax_{\boldsymbol{w}} \{ P(\boldsymbol{w})P(\boldsymbol{O}|\boldsymbol{w}) \}
    \end{equation}
    The probability of the observations ${P(\boldsymbol{O})}$ in the denominator is not relevant in finding the best transcription ($\argmax_{\boldsymbol{w}}$) for the observations, which leaves the product in the numerator to be estimated. This product includes the two most significant subtasks: \emph{acoustic modelling} and a \emph{language modelling}. A language model (LM) generates an \emph{a priori} probability distribution $P(\boldsymbol{w})$ over possible word sequences. For example, the transcription "the god of thunder was Zeus" should probably be assigned a larger probability than "the god of thunder was juicy" even before any speech audio is processed.
    An acoustic model (AM) outputs likelihoods of observations conditional on phoneme sequences. The phoneme sequences are mapped to words by a \emph{lexicon} (also called a \emph{pronunciation dictionary}), yielding $P(\boldsymbol{O}|\boldsymbol{w})$.
    % The probabilities of the LM and AM are combined to estimate the most likely transcription of the speech audio.
    %  \emph{a posteriori} probabilities $P(\boldsymbol{O}|\boldsymbol{w})$ of phoneme sequences conditional on the speech audio. A dictionary is used to map the phoneme sequences to words, or more generally, the same set of tokens that the LM uses, which can also be subword units or characters, for instance. 
    % The former factor $P(\boldsymbol{w})$ is the a priori probabilities of word sequences, modelled by language models. 
    To avoid numerical underflow, the probabilities are converted to the logarithmic domain. A scalar weight $\lambda$ is added to determine how significant the LM probabilities are compared to the AM probabilities.
    \begin{equation}\label{eq:asr-bayes-2}
      \argmax_{\boldsymbol{w}} \{ 
      P(\boldsymbol{w})^{\lambda}P(\boldsymbol{O}|\boldsymbol{w}) \} 
      = \argmax_{\boldsymbol{w}}
        \{ \lambda  \log \{ P(\boldsymbol{w}) \} +
         \log \{ P(\boldsymbol{O}|\boldsymbol{w}) \} \}
    \end{equation}
    Acoustic and language modelling are achieved using machine learning models, primarily deep neural networks (DNNs), which are estimated based on training data. To model acoustics a parallel corpus of speech and text is needed, whereas to model language only text is needed.

    In the past few years, end-to-end (E2E) speech recognition systems have achieved promising results (e.g., \citet{hannun2014deep, chan2016listen}). An E2E system dispenses with the division to an LM and an AM, and instead learns a mapping  from (preprocessed) audio straight to the transcription. This makes the training procedure simpler since only one model is trained instead of multiple. However, it has been shown that E2E models can still benefit from, for example, incorporating an external language model \citep{toshniwal2018comparison} or speaker embeddings \citep{rouhe2020speaker}, into an E2E system, making it arguably no longer a pure E2E model, depending on how "E2E" is defined. Results such as these indicate that pure E2E systems will not completely supplant conventional ASR systems, or systems that include multiple separately trained models, any time soon, although E2E systems benefit from the simplified training procedure. The state-of-the-art results are still obtained with the HMM/DNN systems in many ASR tasks, and for this reason, this thesis explores methods in this conventional paradigm.

    
\clearpage
\section{Statistical language modelling} \label{sec:lm} 

  %
    A language model defines the probability distribution
    % \footnote{This would be a probability mass function since word sequences are discrete but, since there is no limit to how long the sequences can be, there is an infinite set of possible word sequences.}
    $P(\boldsymbol{w})$ from Eq. \ref{eq:asr-bayes} over word sequences $\boldsymbol{w} = w_1,...,w_N$. In statistical language modelling, the distribution  is estimated based on statistics drawn from a training corpus. The statistical approach is in contrast with linguistically motivated models of language that take into account, for example, the grammaticality of a string when determining its probability. Currently, state-of-the-art language modelling is based solely on statistics, disregarding many such linguistic considerations.
    
    Typically, $P(\boldsymbol{w})$ is calculated as the product of the constituent word probabilities, which are conditional on the context of each word:
    \begin{equation} \label{eq:lm}
      P(w_1,...,w_N) = \prod_{i=1}^N P(w_i|context)
    \end{equation}
    What is included in the context of a word depends on the method. A constant-order n-gram model uses a simple context of $n-1$ previous tokens. Variable order n-grams use a context of a variable length, which depends on factors such as the frequency of the word sequence. In neural language models, the context can include also subsequent tokens, which creates a bi-directional context.
    % In neural language models, the context can encompass the all the previous words $w_1,...,w_{i-1}$ in the sequence. max sequence


  \subsection{Choice of vocabulary} \label{sec:vocab} 

    A language model defines a set of units that the output sequence can include, called the vocabulary. One natural choice of vocabulary is to include in it the word types that appear in the training corpus. However, using smaller units has some benefits over a word vocabulary. If the words are segmented into smaller pieces, or all the way to individual characters, the vocabulary is smaller. This decreases computation and memory requirements for neural language models (see Section \ref{sec:nnlm}),
    % as well as the language model FSA (Section \ref{sec:wfst}),
    for instance. Another benefit of using a \emph{subword} vocabulary is that words absent in the training corpus can possibly be composed of the subword units, which means that the out-of-vocabulary (OOV) rate is smaller. The downside of subword vocabularies is that each sentence includes a larger number of units to be processed. The number of units in the context should therefore be larger if the units are shorter. This creates difficulties especially with n-gram language models (Section \ref{sec:ngram}), because the number of possible n-grams increases exponentially w.r.t. $n$. NNLMs are better able to model long contexts, and the benefits of using shorter units typically outweigh the costs, which is why modern SOTA NNLMs, such as BERT (see Section \ref{sec:attention}), often use subword units.
    % , which increases the computational costs for some systems, for example when 

    The segmentation is an minimisation problem with a trade-off between two desiderata: a small vocabulary size and a small number of units in the corpus. As one of these decreases, the other increases (as a general rule), so an optimal compromise ought to be sought. When segmenting words into subword units it is therefore important to take into account the frequency of a character string or word. In general, frequent character strings should be added to the vocabulary so that the number of units in the corpus decreases.

    Byte pair encoding (BPE) \citep{gage1994new} is a compression method that can be, and is commonly, applied to segmenting words into smaller pieces \citep{sennrich2015neural}. The algorithm first divides the training corpus into characters. It then combines the two characters or subwords that are most frequently adjacent and combines these into a new subword. The algorithm iterates this for a number of times to generate the vocabulary that minimises the size of the training corpus given the size of the vocabulary.  

    The Morfessor family of algorithms \citep{creutz2002unsupervised, creutz2007unsupervised} segments words in a more linguistically-motivated way. The segmenting model learns an optimal way to segment words into subwords based on a criterion, for example the maximum likelihood of the model  given the training data. 


  \subsection{n-gram language models} \label{sec:ngram} 

    An n-gram language model bases the prediction of the last token in a sequence of $n$ tokens on statistics gathered from a training corpus. 
    The probability of a word sequence is the product of the probabilities of the constituent words, and the probability of each word is conditional on $n-1$ previous words (i.e., the context). The Eq. \ref{eq:lm} becomes
    \begin{equation}\label{eq:ngram}
      P(w_1,...,w_N) = \prod_{i=1}^N P(w_i|w_{i-(n-1)},...,w_{i-1})
    \end{equation}
    
    A simple 
    % (the simplest of the ones that make any sense)
    method to define the probability of an n-gram is to let the probability of each word be its normalised frequency in the context:
    \begin{equation}\label{eq:ngram-ml}
      P(w_i|w_{i-(n-1)},...,w_{i-1}) = \frac{\textup{count}(w_{i-(n-1)},...,w_{i-1}, w_i)}{\textup{count}(w_{i-(n-1)},...,w_{i-1})}
    \end{equation}
    This is the maximum likelihood estimate of $P(w_i|w_{i-(n-1)},...,w_{i-1})$ since it maximises the likelihood of the language model conditional on
    % the language model for 
    this specific training data set \citep{chen1998empirical}. As in machine learning in general, however, the aim is to use the training data to distil from it a generalising model which predicts patterns also in previously unseen data instead of building a model that maximally accounts for the training data set. 
    
    % problems aimed to be fixed by smoothing

    To improve the predictions for unseen data, \emph{smoothing} can be applied to the simple occurrence counts by redistributing probability mass from the most common n-grams to the less common ones, i.e., \emph{discounting} the most frequent n-grams. Low occurrence counts become a problem especially when $n$ is high, since the number of possible n-grams increases exponentially as $n$ increases. If there is an insufficient number of examples of the n-grams of the desired order, the information of lower-order n-grams (unigrams,...,(n-1)-grams) can be used; the model can \emph{backoff} to the lower orders to make the probability distribution smoother. The lower-order n-gram scores can also be \emph{interpolated} with the scores of the n-grams of the nominal order $n$. 
    
    Backing off to lower orders revives % better  word ?
    problems that motivated using higher-order n-grams in the first place. One of them is that of two words that are equally frequent, and thus have the same unigram probability, one may have a very specific kind of context in which it almost always appears whereas the other appears in various contexts. Given a novel context, which is why backing off to unigrams is necessary, the former is statistically less probable to appear in it than the latter, but this is not captured by unigram statistics.
    \citet{kneser1995improved} introduced a smoothing method, which has become commonly used, where the number of different bigram contexts of a word correlate with the unigram backoff probability. The number of seen bigram types where the word $w$ is the latter word, i.e., the number of types of the previous token $w'$ that at least once precede $w$ in the corpus, can be expressed as $| \{ {w' : \textup{count}(w',w)>0} \} |$. This count is normalised by the number of all word types that are seen as the first word of a bigram to get the KN unigram probability:
    \begin{equation}\label{eq:continuation}
      P_{\textup{KN}}(w) = \frac{| \{ {w' : \textup{count}(w',w)>0} \} |}
                                { \sum_{w''} |\{ w' : \count(w'w'')>0\}|}
    \end{equation}
    For a Kneser-Ney smoothed bigram model, the unigram and bigram statistics are then interpolated:
    \begin{equation}\label{eq:kn_bigram}
      P_{\textup{KN}}(w_i|w_{i-1}) = \frac{\max(\textup{count}(w_{i-1}w_i)-d, 0)}
                                          {\textup{count}(w_{i-1})} 
                                    \lambda(w_{i-1})
                                    P_{\textup{KN}}(w_i)
    \end{equation}
    where $d$ is a discount constant, usually $0<d<1$, and $\lambda$ is a normalising factor that defines how the discounted probability mass is redistributed:
    \begin{equation}\label{eq:kn_lambda}
        \lambda(w_{i-1}) = \frac{d}
                            {\sum_{w'} \textup{count}(w_{i-1},w')  }
                            | \{ {w' : \textup{count}(w_{i-1},w')>0} \} |
    \end{equation}
    The discount is normalised by the sum of the counts of all bigrams where the $w_{i-1}$ is the first word, and the normalised discount is multiplied by the number of word types that have followed $w_{i-1}$, i.e., the number of word types that have been discounted, so that $P_{\textup{KN}}(w_i|w_{i-1})$ over all $w_i$ equals to one.

    The bigram formulation can be generalised to higher-order n-grams:
    \begin{align}\label{eq:kn}
      P_{\textup{KN}}(w_i|w_{i-n+1}^{i-1}) = 
          &\frac{\max(\textup{count}_{\textup{KN}}(w_{i-n+1}^{i-1},w_{i})
              - d, 0)}{\sum_{w'} \textup{count}_{\textup{KN}}(w_{i-n+1}^{i-1},w')}
              \nonumber \\[6pt]
          &+ \ \lambda(w_{i-n+1}^{i-1})
          P_{\textup{KN}}(w_i|w_{i-n+2}^{i-1})
    \end{align}
    where $w_{i-n+1}^{i-1}$ is the $n-1$ words before $w_i$ and $\textup{count}_{\textup{KN}}$ is the count for the highest order and the number of different words that precede the n-gram for the lower orders \citep{Jurafsky2019}.

    In the modified Kneser-Ney smoothing, introduced by \citet{chen1998empirical}, the discount constants are different for n-grams that have one, two, or more than two occurrences, changing also $\lambda$ for the distribution to still sum to one. This is motivated by empirical results suggesting that the optimal $d$ depends on the frequency.

    A common approach to making an n-gram model more efficient is to \emph{prune} n-grams that are least relevant. The simplest way to prune n-grams from the model is to use a frequency cut-off below which the n-grams are omitted from the model. Other methods include comparing the log-probability of the model and the model where an n-gram has been removed, and removing the n-gram if the difference is small \citep{siivola2007growing}. 

    % The ARPA (named after DARPA which was previously named ARPA) backoff n-gram format is used ... \footnote{\url{http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html}}


  \subsection{Neural language models}  \label{sec:nnlm}

    % 
      n-gram language models treat each word without relations to other words in the vocabulary. The words have no properties other than n-grams they belong to, and the probabilities of those n-grams.
      % in a set with no other attributes besides an ID. 
      \citet{bengio2003neural} proposed a method able to take into account the similarity of words, and able to generalise exploiting the similarities. Words can be similar to each other in different ways: "cat" is similar to "dog", and "cat" is similar to "whiskers", but in a different way. Different kinds of similarities can be represented as dimensions in a vector that encodes a word. By this method, words are embedded into a vector space that encodes relations between words. Similarity  of words corresponds to proximity in the embedding space, and can be measured by the cosine similarity of two vectors $\boldsymbol{a}$ and $\boldsymbol{b}$
      \begin{equation}
        \cos(\phi) = {\boldsymbol{a} \cdot \boldsymbol{b} \over \|\boldsymbol{a}\| \|\boldsymbol{b}\|} 
      \end{equation}
      or some other similarity metric.
      Being able to measure the similarity of two words allows for generalising from word sequences in the language modelling training text to sequences that have not been encountered. If an embedding space holds the information that "cat" is similar to "dog" in the particular way that "cat" is, the language model can generalise from a seen sentence "the cat is running" to a new sentence "the dog is running" by assuming that "dog" works in this sequence similarly to "cat".

      One of the seminal word embedding models was published by \citet{mikolov2013efficient}, called Word2Vec. The goal of this model is to be able to train word embeddings using large data sets of billions of words and to create embedding spaces that encode multiple degrees of similarity between words. Substructures of the embedding space can be probed by the word analogy task, for example asking which word is closest in the vector space when the word "man" is subtracted from "king", and the word "woman" is added. The intuitive answer "queen" is given by Word2Vec embedding space
      % whereas LSA embedding space is not able to preserve this kind of information
      \citep{mikolov2013efficient}. The paper introduced two methods for computing the Word2Vec embeddings. In both methods the idea is to train a shallow feedforward neural network with one hidden layer for a classification task and then use the learned hidden layer weights as the vector representation of the word. The Continuous Bag-of-Words Model (CBOW) learns the embeddings by predicting the current word given a few (e.g. 4) previous words and a few next words. The method uses a bag-of-words, i.e. the order of the words is not taken into account. The continuous Skip-gram model is similar to the CBOW, but instead of predicting the current word given a context, the training task is to predict the context of a given word. Using the negative sampling method \citep{mikolov2013distributed}, a small subset of the words outside the context are sampled. This way the task does not require to classify all the other outside-of-the-context words as "not in the context" but only some sample in the order of 10.

      Learning to embed words by an affine transform is usually incorporated into modern NNLMs as the first layer or first few layers of the network. The embedding is learned in the same training process as the language modelling task. Initialising the embedding of an NNLM from a pre-trained word embedding model is also possible. 
      
  \subsubsection{Recurrence} \label{sec:rnn} 

    Feedforward networks feed the output of a layer to the next layer until the output layer. The network output at time $t$ is therefore dependent only on the input at $t$. In natural languages, each unit of speech or text is strongly connected to the previous units. Both language models and acoustic models ought to capture dependencies across many time steps, and RNNs have been successfully applied to these tasks. In a simple RNN, the output of a layer is fed back to the same layer in the next time step, creating a type of memory of previous states of the network. Other recurrence methods are also possible, but all RNNs are based on some type of cycle between the network connections, creating a dependence of the current output on the previous outputs. This enables modelling dependencies between time steps.

    \paragraph*{Long short-term memory}
    A widely used type of recurrent NN is called a long short-term memory (LSTM) network \citep{hochreiter1997long}. The term "short-term" alludes to the activations of recurrent connections as type of memory, in contrast with "long-term" memory in the form of the weights of the connections that change by learning. The short-term memory is made longer and more expressive than in the usual RNN memory by utilising special gate layers that comprise a memory cell unit. The LSTM cell contains in total four neural network layers that perform different gating functions and interact to jointly learn which information should be memorised, i.e., which of the processed previous inputs contain the relevant \emph{context} that the next output should depend on. The gate layers can be called the forget gate, the add gate, the input gate, and the output gate.   Figure \ref{fig:lstm} illustrates the LSTM cell with its four gate layers.
    \begin{figure}[htb]
      \centering
      \includegraphics[width=14cm]{LSTM-cell}
      \caption{An LSTM cell. At time step $t$ the cell inputs the input vector $\boldsymbol{x}_t$, the previous output (the high-level representation) $\boldsymbol{h}_{t-1}$, and the previous  context vector $\boldsymbol{c}_{t-1}$.}
      \label{fig:lstm}
    \end{figure}

    The forget gate inputs the previous output $h_{t-1}$ and the current input $x_t$ and outputs a vector that decides which elements in the previous context vector (aka cell state) $c_{t-1}$ should be kept in the memory and how completely, on a scale from 0 to 1.
    \begin{align}
      f_t &= \sigma(U_f h_{t-1} + W_f x_t + b_f) \\
      k_t &=  f_t \circ c_{t-1}
    \end{align}
    where $\circ$ represents the element-wise (also known as Hadamard) product of the vectors. In a first version of the LSTM published by \citet{hochreiter1997long}, there is no forget gate but a self-recurrent connection, which is equivalent to $f$ being always unity. This is called the \emph{constant error carousel} because it retains information from the previous cell states and alleviates the problem of vanishing or exploding gradients. \citet{gers1999learning} introduced the forget gate to avoid problems caused by continual input streams that are not segmented by ends at which the LSTM state could reset. The forget gate incorporates to the LSTM cell learnable parameters whose purpose is to discard information when it becomes useless.

    The add gate extracts from  $h_{t-1}$ and $x_t$ a new candidate context vector $\hat{c}_t$ that could be added to the context in lieu of the forgot information. The hyperbolic tangent squishes the values between -1 and 1. The input gate $i_t$ selects from $\hat{c}_t$ the new context that is added to the context.
    \begin{align}
      \hat{c}_t &= \tanh(U_c h_{t-1} + W_c x_t + b_c) \\
      i_t &= \sigma(U_i h_{t-1} + W_i x_t + b_i) \\
      j_t &= \hat{c}_t \circ i_t \\
      c_t &= k_t + j_t
    \end{align}

    The output gate $o_t$ generates the current hidden state $h_t$ from the newly computed current context $c_t$:
    \begin{align}
      o_t &= \sigma(U_o h_{t-1} + W_o x_t + b_o) \\
      h_t &= o_t \circ \tanh(c_t)
    \end{align}

    LSTMs are inherently uni-directional, as the input sequence units are processed one-by-one. If an application benefits from bi-directional context, two separate cells can be used for the two directions. Furthermore, LSTM cells can be stacked on top of each other to create a deep LSTM network. Each layer feeds the output to the input of the next layer, and the recurrent connection feeds the context vector and the output vector also to the same cell in the next time step.   
    Figure \ref{fig:blstm} illustrates a 2-layer bidirectional LSTM network.
    \begin{figure}[htb]
      \centering
      \includegraphics[width=14cm]{blstm}
      \caption{A bidirectional LSTM network.}
      \label{fig:blstm}
    \end{figure}
    

  \subsubsection{Attention} \label{sec:attention}
    \citet{Bahdanau2015NeuralMT} developed a mechanism that has been widely adopted in speech and language processing systems, called \emph{attention}. The term refers to a search technique in which a model learns to determine which parts of the input are most relevant for an output at a particular time step: which parts of the input should be attended to when generating an output.
    
    \paragraph*{Background: seq2seq encoder-decoder models} 
      Attention was introduced in the context of machine translation with sequence-to-sequence \citep{sutskever2014sequence} encoder-decoder systems. "Sequence-to-sequence" means that the system maps an input sequence to an output sequence; in machine translation these are a sentence in two different languages. "Encoder-decoder" is a neural network architecture type that first encodes the input sequence as a high-level representation and then a decodes the output using this representation vector. The encoder typically uses a feedforward neural network to project the tokens in the sequence into an embedding space, and an RNN, e.g., an LSTM network, to turn the word embedding sequence (multiple vectors) into the single context\footnote{The term "context" is used in the context LSTMs and attention; it denotes different kinds context vectors in LSTMs and in the attention mechanism.} vector $\boldsymbol{c}$. 
      % , called the context.

      Also the decoder is usually  an LSTM network, followed by a softmax layer to generate the output word sequence $\boldsymbol{w}=w_1,...,w_{N}$ by determining a probability distribution $p(w_i|{w_1,...,w_{i-1}},\boldsymbol{c})$ over the words given the context vector and previous outputs.
      \begin{equation}
        p(\boldsymbol{w}) = \prod_{i=1}^N p(w_i|{w_1,...,w_{i-1}},\boldsymbol{c})
      \end{equation}

      In the conventional encoder-decoder model, the context vector is the output of the encoder LSTM (or GRU) cell, as described in Section \ref{sec:rnn}, after processing the last input token, and the context vector initialises the first decoder state: 
      \begin{align}
        \boldsymbol{c} &= \boldsymbol{h}^e_U \\
        \boldsymbol{h}^d_0 &= \boldsymbol{c}
      \end{align}
      where the superscripts $e$ and $d$ distinguish the encoder and decoder state vectors.
      % , but $\boldsymbol{h}$ can be generated differently

      If the decoder is an LSTM network, the probability distribution depends on the previous output $w_{i-1}$, the encoder output $\boldsymbol{c}$, and the decoder LSTM state $\boldsymbol{h}^d_i$. Call this function $g$:
      \begin{equation} \label{eq:decoder_prob}
        p(w_i|{w_1,...,w_{i-1}},\boldsymbol{c}) = g(w_{i-1}, \boldsymbol{h}^d_i, \boldsymbol{c}) 
      \end{equation}
      The decoder is therefore an \emph{autoregressive} generator of the output sequence, generating one unit in the sequence at a time based on the previously generated units \citep{graves2013generating}.
    
    \paragraph*{An attentive decoder} 
      Attention can be used in the decoder to help find the relevant information from the encoder output. Whereas the traditional decoder bases the output word probabilities on the single context vector $\boldsymbol{c}$ common to one sequence, attention computes a separate $\boldsymbol{c}_i$ vector for each time step $i$, and uses all of the encoder  states $H^e=\boldsymbol{h}^e_1,...,\boldsymbol{h}^e_U$ to generate the context vectors $\boldsymbol{c}_1,...,\boldsymbol{c}_N$. The output probability distribution from Eq. \ref{eq:decoder_prob} becomes
      \begin{equation}
        p(w_i|{w_1,...,w_{i-1}},\boldsymbol{c}_i) = g(w_{i-1}, \boldsymbol{c}_i, H^e)
      \end{equation}
      The decoder LSTM network state depends on the current context vector, along with the previous decoder state and the previous output:
      \begin{equation}
        \boldsymbol{h}^d_i = f(w_{i-1}, \boldsymbol{h}^d_{i-1}, \boldsymbol{c}_i)
      \end{equation}
      The context vector $\boldsymbol{c}_j$ is a weighted sum of annotations $\boldsymbol{h}^{e}_i$, which are typically the concatenated  outputs of the forward and backward layers in a bi-directional LSTM network.
      \begin{equation}
      \boldsymbol{h}^{e}_j = [ \overrightarrow{\boldsymbol{h}}^{e}_j ;
                                \overleftarrow{\boldsymbol{h}}^{e}_j ]
      \end{equation}

      
      The weights $\alpha_{ij}$ aim to capture how relevant each encoder state $\boldsymbol{h}^{e}_i$ is to the decoder output at the current time step. First, a scoring function gives the pair of encoder and decoder state vectors a value $e_{ij}$ that indicates how relevant the encoder state at index $j$ is to the decoder state at index $i$. This could be achieved by a simple dot product of the vectors, but a more adaptive way to assess the similarity is to learn the similarity function. The scoring function is parametrised with a weight matrix $W_s$:
      \begin{align}
      score(\boldsymbol{h}^{d}_{i-1},\boldsymbol{h}^{e}_j) &= 
            \boldsymbol{h}^{d}_{i-1} W_s \boldsymbol{h}^{e}_j
            \label{eq:att_score}
            \\
      e_{ij} &= score(\boldsymbol{h}^d_{i-1},
              \boldsymbol{h}^e_{j}) 
      \end{align}
      The scoring function is called the \emph{alignment model} by \citet{Bahdanau2015NeuralMT} as it aligns the encoder states with the decoder outputs. As the encoder states are dependent on the inputs at the same time step, the alignment is between the input sequence and the output sequence. In the application of machine translation, this means an alignment of the words in the source and target languages. If the application is end-to-end ASR, the alignment would be between observations and output tokens (e.g., characters, as in the influential paper by \citet{chan2016listen}). The alignment is soft, i.e., each input affects many outputs with variable weights.
      The scores are normalised with the usual softmax function to ensure that they are positive values which add up to one:
      \begin{equation} \label{eq:att_softmax}
        \alpha_{ij} = \softmax(e_{ij}) = \frac{\exp(e_{ij})}
                {\sum_k \exp(e_{ik})}
      \end{equation}
      The softmax also increases the relative differences between the values, drowning the irrelevant units and emphasising the relevant ones.
      Finally, the context vector is calculated by the weighted sum of the encoder states: 
      \begin{align}
        \boldsymbol{c}_i &= \sum_j \alpha_{ij}
            \boldsymbol{h}^e_j
      \end{align}
      Figure \ref{fig:attention} illustrates the attention mechanism used in an encoder-decoder model.
      \begin{figure}[htb]
        \centering
        \includegraphics[width=12cm]{attention}
        \caption{Flow chart of the attention calculations.}
        \label{fig:attention}
      \end{figure}

    \paragraph*{Self-attention and transformers} 

       
      The encoder-decoder described in the previous subsection applies the attention mechanism in between two recurrent neural networks. As a deviation from this prevailing method, \citet{vaswani2017attention} asserted that "Attention Is All You Need", in the title of their paper describing the transformer network. The network architecture proposed in their paper evades the use of recurrence altogether, applying instead feedforward networks and \emph{multi-head self-attention} blocks, which are briefly described in this subsection. One good reason to avoid recurrence is its inherently sequential nature: the state of an LSTM cell depends on the previous state, which means they need to be computed in succession. Attention mechanism has no such restriction, which enables parallelising the computation, decreasing significantly the required training time.

      The basic attention described in the previous section aligns the input sequence with the output sequence. Self-attention, also called intra-attention, aligns the input sequence with itself, as well as with the output sequence. By relating each unit in the input sequence with the other units, the self-attention mechanism aims to compute a more informative encoding of the units. This idea is based on the fair assumption that there are dependencies across the sequence, not necessarily related to how many indices apart the dependent units are. For example, if the input sequence is "Iris was the goddess of the rainbow and the messenger of the Olympian gods while Arke, her twin sister, became the messenger of the Titans." it is probably useful to attend to "Iris" when encoding "her", to "Arke" when encoding "messenger", and to both "Iris" and "Arke" when encoding "sister". At least, this is roughly how a human reader would perceive stronger relations between some words than others. \citet{vaswani2017attention} noted that the attention mechanism connects words within a sequence in a way that is usually similar to how a human reader does.

      The self-attention mechanism is an interplay between three input vectors called \emph{key}, \emph{value} and \emph{query}, which map to an output vector. These vectors are derived from the input token embeddings
      % (word embedding in the first layer)
      by multiplying each with a single matrix of learned parameters, i.e., performing a linear projection on the input. This typically reduces the dimensionality of the input vector to the key, value and query vectors. For each position in the input sequence, the query vector of the input at that position is paired with the keys of every other positions. For each pair, a score is calculated. The idea here is similar to the basic attention score in the previous subsection (Eq. \ref{eq:att_score}): to determine how relevant the other input units in the sequence are when encoding this input unit.
      The particular implementation of self-attention described by \citet{vaswani2017attention} is called scaled dot product attention, in which the score is calculated by a dot product of the key and query, scaled by the square root of the dimension $d_k$ of the key vector.
      In practice, the vectors are stacked together to compose matrices, simplifying the calculations:
      \begin{align}
        Attention(Q,K,V) = \softmax \left( \frac{QK^T}{\sqrt{d_k}} \right) V 
      \end{align}
      The score is fed through a softmax function, as in Eq. \ref{eq:att_softmax}. The softmax output, which can be thought of as probabilities of how much each position should be attended to, are multiplied by the value vectors of the particular positions, i.e., the value vectors are weighted by the softmaxed scores. The attention function outputs are summed together and normalised to create the encoding for the particular input unit.

      The attention mechanism works nicely as is, but \citet{vaswani2017attention} noted that it is beneficial to compute multiple different linear projections for the keys, values and queries, and perform the attention function separately on these different branches, or \emph{heads}. The outputs of the heads are concatenated and a final linear projection generates the output of the multi-head attention.

      Since the attention mechanism treats each position equally, an explicit representation of each position is required to model the information expressed by the order of the units in a sequence. A \emph{positional encoding} is calculated for each unit and summed with the input embedding. Either learned or fixed positional encodings are possible, and they can be based on the absolute position in the sequence, or relative to the length of the sequence. \citet{vaswani2017attention} evaluated a few positional encoding functions and ended up using a fixed function that uses the fixed positions $pos$:
      \begin{equation} \label{eq:pos_enc}
        PositinalEncoding(pos,2i) = \sin \left(
          \frac{pos}{10000^{2i/d}} \right)
          % \frac{pos}{10000^{\frac{2i}{d}}} \right) 
      \end{equation}
      where $i$ is the dimension, and $d$ is the number of dimensions.

      The transformer network follows the encoder-decoder structure. The encoder includes six stacked 2-part layers that each include the multi-head self-attention mechanism followed by a fully-connected feedforward network. Residual connections \citep{he2016deep} are added around the attention and around the feedforward layers, and the layer outputs are normalised. The residual connections require that the outputs and inputs are of the same dimensionality. 

      The decoder is similar to the encoder, but an additional layer is integrated to it to perform multi-head attention to the encoder output. The transformer output is also fed to a multi-head attention. This output is masked so that the subsequent positions cannot be attended to, but the units depend only on the known previous positions.
      % \paragraph*{Transformer language models}
      \newline
      \newline
      After the publication of the transformer network, its variants have been applied to many natural language processing and understanding tasks with success. Currently, the common benchmark for these kinds of general language models is the General Language Understanding Evaluation (GLUE), which includes tasks such as classifying sentence sentiment or entailment \citep{wang2018glue}. GLUE aims to assess whether a system is able to extract a variety of information from text, similarly to how humans understand text by extracting meaning from it.
      
      \citet{radford2018improving} pre-trained a large transformer \emph{generatively} on a large corpus of diverse text and applied \emph{discriminative} fine-tuning to modify the model for a specific task with in-domain text. They call the model GPT, or generative pre-trained transformer. GPT was able to achieve SOTA results on many tasks similar to the GLUE score (which was released after GPT), demonstrating the efficacy of general generative pre-training followed by task-specific fine-tuning, and further fortifying the position of transformers as a replacement of RNNs in language modelling.

      \citet{devlin2018bert} developed another prominent transformer system called BERT or Bidirectional Encoder Representations from Transformers.
      % BERT was designed to be applicable to many different NLP and NLU tasks, and it was
      Two of the main design choices that made BERT stand out and improve upon the results of the GPT and other models were bi-directionality and multi-task learning. GPT was trained to predict subsequent words given a previous context, but BERT training utilises masking to predict words given both previous and subsequent words in the context. Masking is a deeper way to use the bi-directional context than concatenating two encodings derived from left and right context separately, as described in Section \ref{sec:rnn} with bLSTMs. Because the same encoding model can see tokens on either side of the masked token, dependencies across left and right contexts are possible to model, too.
      BERT was pre-trained on a second task besides the masked language modelling task. The second task was to predict whether a given sentence follows the current sentence. This task aims to train a more high-level understanding of the sentences, needed in many down-stream tasks such as question answering. This also makes the pre-training resemble the fine-tuning more, as fine-tuning could be done, for example, to train the model to answer questions. The next sentence prediction is implemented by adding a special binary classification token as the first token in the  output  sequence of embeddings. In other tasks, the classification embedding can be used differently. The input consists of three embedding types that are summed together: token embeddings, segment embeddings and the positional encoding. The segment embedding encodes the information which sentence of the two an input token belongs to. The sentence pair is also separated by an additional separating token.  
      The 2-task pre-training was deemed important in the ablation experiments of \citet{devlin2018bert}. 

      While the transformer has proven more effective than RNNs for NLU tasks and supplanted RNNs in many applications of language modelling, recurrence itself has made a return of sorts. \citet{dai2019transformer} addressed some problems caused by dispensing with recurrence entirely and utilising attention only, pertaining to what they call \emph{context fragmentation}. As a corpus of training text is fed to a model such as BERT, it needs to be segmented into smaller input snippets due to memory restrictions of the computation. In text, there  are usually natural linguistic segments such as sentences, paragraphs and documents, but segmenting the text into these would effect segments of very uneven lengths. In practice, a corpus is usually segmented into fixed-length segments of a few hundred characters. These segments cannot conform to the natural linguistic boundaries, so the used context may lack some relevant context as, e.g., sentences are sometimes cut in half. \citet{dai2019transformer} proposed a remedy for the context fragmentation, incorporated in their system called Transformer-XL (extra long). Their solution is based on two modifications of the transformer: reusing the hidden states of previous segments in a recurrent manner and using relative positional encodings instead of absolute ones. The second modification is necessary to avoid temporal confusion when reusing hidden states. 

      Reusing the previous hidden states is implemented by simply concatenating the hidden states of two consecutive segments. This creates a recurrence in the network whereby the utilised context extends beyond two segments, since all of the previous segments affect the current segment hidden state to some degree. One detail that makes this recurrence different from the recurrence commonly implemented in RNNs is that the reused hidden states are taken from one layer below instead of the same layer. Consequently, the longest possible dependency length increases linearly as more layers added to the network. 

      When the previous state is reused, the positional encoding is also carried over from the previous state. This obviously creates confusion, and the  whole purpose of encoding positional information is undermined, if the usual positional encoding function (Eq. \ref{eq:pos_enc}) is used. Therefore, \citet{dai2019transformer} use a different positional encoding scheme which avoids this problem. As noted before, in the traditional transformer, the positional encoding $\boldsymbol{u}_i$ is summed with the token embedding $\boldsymbol{x}_i$. This sum is linearly projected using a learned weight matrices $W_k$ and $W_q$ to respectively generate the key vector $\boldsymbol{k}$ and query vector $\boldsymbol{q}$. 
      \begin{align}
        \boldsymbol{u}_i^{\textup{abs}} = PositinalEncoding&(pos,2i) = \sin \left(
          \frac{pos}{10000^{2i/d}} \right) \\[4pt]
          % \frac{pos}{10000^{\frac{2i}{d}}} \right) 
          \boldsymbol{q}_i &= W_q(\boldsymbol{x}_i +
          \boldsymbol{u}_i^{\textup{abs}}) \\
          \boldsymbol{k}_j &= W_k(\boldsymbol{x}_j +
          \boldsymbol{u}_j^{\textup{abs}}) 
      \end{align}
      And the score of a key-query pair is simply their product (omitting the scaling factor for simplicity). This product can be expanded:
      \begin{align}
          score_\textup{abs}(\boldsymbol{q}_i, \boldsymbol{k}_j)
           = W_q(\boldsymbol{x}_i + \boldsymbol{u}_i^{\textup{abs}})
           &W_k(\boldsymbol{x}_j + \boldsymbol{u}_j^{\textup{abs}}) \\[4pt]
           = \boldsymbol{x}_i W_q W_k \boldsymbol{x}_j &+
           \boldsymbol{x}_i W_q W_k \boldsymbol{u}_j^{\textup{abs}} \nonumber \\[4pt]
           + \ \boldsymbol{u}_i^{\textup{abs}} W_q W_k \boldsymbol{x}_j &+ 
           \boldsymbol{u}_i^{\textup{abs}} W_q W_k \boldsymbol{u}_j^{\textup{abs}} 
           \nonumber
      \end{align}
      To convert the absolute positional encodings to relative, \citet{dai2019transformer} change the scoring function to
      \begin{align}
          score_\textup{rel}(\boldsymbol{q}_i, \boldsymbol{k}_j)
           = \boldsymbol{x}_i W_q W_{k,x} \boldsymbol{x}_j &+
           \boldsymbol{x}_i W_q W_{k,u}
           \textcolor{blue}{\boldsymbol{u}_{i-j}^{\textup{rel}}} \\[4pt]
           + \ \textcolor{red}{\boldsymbol{v}^{x}} W_{k,x} \boldsymbol{x}_j &+ 
           \textcolor{red}{\boldsymbol{v}^{u}} W_{k,u}
           \textcolor{blue}{\boldsymbol{u}_{i-j}^{\textup{rel}}} \nonumber 
      \end{align}
      where $\textcolor{blue}{\boldsymbol{u}_{i-j}^{\textup{rel}}}$ is a relative counterpart to $\boldsymbol{u}_i^{\textup{abs}}$. When the positional encoding is relative, only the distance between $i$ and $j$ is taken into account. Furthermore, $\textcolor{red}{\boldsymbol{v}^{x}}$ and $\textcolor{red}{\boldsymbol{v}^{u}}$ are learnable vectors of parameters which replace $\boldsymbol{u}_i^{\textup{abs}} W_q$. This is motivated by the fact that the query vector is equal
      (not multiplied by $\boldsymbol{x}_i$ in these terms)
      for all query positions, which suggests that the attentive bias towards different positions should also be equal for each query position. The weights for content $W_{k,x}$ and position $W_{k,u}$ are also distinguished from each other. 
      This formulation allows for an intuitive interpretation: the first term encodes content-based attention, the second term encodes content-dependent positional information, the third term encodes global content bias, and the last term encodes global positional bias \citep{dai2019transformer}.

      
      % The main differences are that the positional encoding of the key vector is relative, and the linear transformation of the query vector at 


\clearpage
\section{Acoustic modelling and the Kaldi toolkit} \label{sec:am}
  
  %
    Kaldi\footnote{"Kaldi" is one in the series of coffee-related names for computer science projects. The Kaldi documentation explains: "According to legend, Kaldi was the Ethiopian goatherder who discovered the coffee plant."} is a toolkit for automatic speech recognition \citep{Povey_ASRU2011}, used in the ASR experiments in this study. This chapter aims to give an overview of the most relevant theoretical underpinnings of Kaldi ASR systems, as well as some of the practical details of using Kaldi.

    The acoustic models built in this thesis are hybrid HMM/DNN models, in which a deep neural network is used to generate the probability distributions over phonemes, given observations. However, training DNNs requires alignments of the observations and phonemes. These can be generated from a HMM/GMM acoustic model. The first step of acoustic modelling is feature extraction, described in Section \ref{sec:feats}. Building HMM/GMM acoustic models is discussed in Sections \ref{sec:phoneme_hmm} and \ref{sec:hmm_est}. DNN AMs are described in Section \ref{sec:dnn_am}. The last sections in this chapter explain in more detail some further methods for modelling the acoustics.
    
    % Section \ref{sec:adaptive_training} explains some of the speaker adaptation methods for both GMM and DNN AMs.

  \subsection{Feature extraction and feature-space transforms} \label{sec:feats} 

    A speech audio signal contains a lot of information that is irrelevant for converting the signal to text. The first step of ASR is to find the features of the signal that contain the information about what is being said. An assumption is made that the speech signal does not change meaningfully in a time frame of about 10 milliseconds so that the signal can be divided into frames with this time resolution. The frames overlap so that each frame is about 20 or 25 milliseconds, and a tapered window function, such as Hamming, is applied to (i.e., multiplied by) each frame. This window function removes the discontinuities that occur on the borders of frames, and the overlapping compensates for the tapering of the window function so that the distorting effect on the signal statistics is minimised.
    %  \citet{}. % https://wiki.aalto.fi/display/ITSP/Windowing

    \paragraph*{Mel-frequency cepstral coefficients}
      The stationary frames' frequency components can be then computed with the Fourier transform. A commonly used method is to extract the MFCCs by applying a logarithmic mel-scale filterbank to the frequency spectrum, and lastly computing the DCT. The log mel-scale emphasises the lower frequencies emulating the way humans perceive sound, i.e., the way the human inner ear recognises lower frequencies with higher frequency resolution. The DCT decorrelates the coefficients so that the use of diagonal covariance matrices is possible in the subsequent stages of the modelling, namely when using GMMs to model the HMM state emissions (Section \ref{sec:phoneme_hmm}). The use of diagonal covariance matrices greatly reduces the number of free parameters, but the trade-off is that correlation between feature vector elements is not modelled.
    

    \paragraph*{Delta and delta-delta features} 
      % ?
      % The MFCC method thus assumes that coefficients adjacent in time are independent of each other, which is a false assumption in the case of speech signals.
      % \?
      An MFCC vector encodes only the stationary frequency features of a frame. However, a speech signal varies in time, and this variation carries meaning about which phones are uttered.
      It is therefore useful to add information to the feature vectors about how the signal changes in time. 
      Information about temporal change and about change of temporal change is extracted from the MFCCs by calculating the differences and second-order differences of adjacent coefficients. These features are called the delta ($\boldsymbol{\Delta}$) and delta-delta ($\boldsymbol{\Delta\Delta}$), or acceleration, features. 
      % The $\boldsymbol{\Delta}$s and $\boldsymbol{\Delta\Delta}$s are concatenated with the MFCC vectors, increasing the feature vector length threefold.
      The delta feature vector $\boldsymbol{\Delta}_t$ corresponding to the MFCC vector $\boldsymbol{c}_t$ (or the time step of that vector) is calculated by subtracting the weighted previous vector(s) from the weighted subsequent vector(s) and normalising the sum:
      \begin{equation}
      \boldsymbol{\Delta}_t = \frac{\sum_{\theta=1}^{\Theta}
      \theta(\boldsymbol{c}_{t+\theta}-\boldsymbol{c}_{t-\theta})}{2\sum_{\theta=1}^{\Theta}\theta^2}
      \end{equation}
      In Kaldi, the default window length $\boldsymbol{\Theta}$ is 2, so the $\boldsymbol{\Delta}$s are computed by multiplying the MFCCs with a sliding window of values $[-2,-1,0,1,2]$ and then normalising by dividing by $2(1^2 + 2^2)= 10$. The $\boldsymbol{\Delta\Delta}$s are computed by applying the same method to the $\boldsymbol{\Delta}$ features. The first and last MFCCs are replicated to fill the window \citep{htkbook}.

      Another commonly used method to add temporal context to the feature vectors is to \emph{splice} the MFCC features. This simply means concatenating a few (e.g. 3 or 4) previous and a few subsequent feature vectors. For example if the MFCCs are 13-dimensional, splicing with a context of $\pm$3 frames would increase the dimensionality to 91. After splicing, it is common to reduce the dimensionality back to about 40 using LDA (see below).


    \paragraph*{Cepstral mean and variance normalisation}

      Cepstral mean normalisation (CMN) \citep{rosenberg1994cepstral} and cepstral mean and variance normalisation (CMVN) \citep{viikki1998cepstral} are methods to make the features more useful in noisy conditions. In these techniques, MFCC feature vectors are normalised to have a zero mean and, in CMVN, a unit variance, over a sliding finite segment. After the normalisation, clean and noisy MFCCs are more similar, which mitigates the performance reduction caused by noisy environments.
      
      Variance of MFCCs from a noisy speech signal is generally lower than variance of those from a clean signal. By requiring the variance be constantly unity, noisy and clean speech MFCCs resemble each other more closely. Similarly, when noise is added to a signal the mean changes, and by requiring the mean to be zero the characteristics of clean and noisy signals become more alike. Normalising variability between the speech signals is in general important in training an ASR system that ought to recognise different types of speech by different speakers in different recording conditions.
      
      The topic of handling meaningless variability, i.e., variability that does not contribute to the phoneme content of the utterance, between the utterances is revisited in Section \ref{sec:adaptive_training}, which addresses speaker-adaptive training.


    \paragraph*{Dimensionality reduction and feature-space transforms}

      Features can be compressed by a dimensionality reduction method. Two common methods for this are the principal component analysis (PCA) \citep{pearson1901liii} and linear discriminant analysis (LDA), also called Fisher discriminant analysis \citep{martinez2001pca}.
      % The general idea behind PCA is that a feature vector contains interdependencies between its elements, which means there is redundant information that can be lessened by making the elements (i.e., dimensions) more independent. 
      The general idea of these methods is to find a linear combination of variables that best explain the observations.
      PCA achieves this by performing an orthogonal transformation that transforms the data matrix into a space where the dimensions, called the principal components, are ordered by their variances in decreasing order. After this, dimensionality reduction is achieved by pruning the dimensions with the lowest variance since they contribute the least to the information content of the features. LDA, on the other hand, searches for those dimensions that best discriminate between classes. For LDA, labelled training data is needed. 

      % MLLT and LDA \citep{somervuo2003feature, pylkkonen2006lda} transforms 
      
      Maximum likelihood linear transform (MLLT), also called \emph{semi-tied co-variance matrix}, is a feature orthogonalizing linear transform. MLLT transforms the data into a space where the data is more closely Gaussian distributed, thus enabling the GMMs, as well as DNNs, to model the data more accurately \citep{gales1999semi,gopinath1998maximum}.
      Dimensionality reduction methods such as LDA and dimension decorrelation methods such as MLLT can be, and often are, applied to the same features in succession.
      % that makes the features more accurately modeled by diagonal-covariance Gaussians
      % ->Improved feature processing for Deep Neural Networks

      % The intuition behind LDA is that defining new axes that maximise variance between speakers makes discriminating easier.


    \paragraph*{Differences between GMM and DNN input}

      GMMs are sensitive to the number of dimensions of the feature vectors: even a small increase in the vector length will increase the number of GMM parameters substantially. The usual number of dimensions used with GMMs is about 40. With DNNs, however, the input vector determines only the width of the input layer--widths of the other layers are not constrained by the dimensionality of the input features. Therefore, DNNs can learn to use longer feature vectors, and often the used number of dimensions is a few hundred \citep{rath2013improved}. 

      

      % frame splicing


  \subsection{Modelling phonemes with hidden Markov models} \label{sec:phoneme_hmm} 

    Estimating the likelihoods of observations given phonemes is achieved by creating a HMM for each phoneme. 
    The phoneme-specific HMMs
    generate likelihoods given observed sequences which can be used to map observations to phonemes. This way the task becomes to estimate the parameters of the HMMs so that each of them models the associated phoneme as accurately as possible.
    % In the conventional ASR system, used also in this thesis, phonemes are modelled by HMMs, which are then concatenated to model utterances.

    A hidden Markov model consists of a hidden Markov chain, also called regime, and the observation sequence, i.e., feature vectors. Each observation $\boldsymbol{o}_t$ has a probability $b_i(\boldsymbol{o}_t)$ of being generated when a hidden state $i$ is entered. In other words, the observation is a probabilistic function of the hidden state. 
    % Gaussian mixtures
    A state's emission probabilities are represented by a probability density function (PDF), typically a mixture of multivariate Gaussian densities
    \begin{equation}\label{}
      b_i(\boldsymbol{o}_t) = \sum^{M_j}_{m=1}c_{jm}
        \mathcal{N}(\boldsymbol{o}_t ; \boldsymbol{\mu}_{jm}, \boldsymbol{\Sigma}_{jm})
    \end{equation}
    where $\boldsymbol{\mu}_{jm}$ is the mean vector, $\boldsymbol{\Sigma}_{jm}$ is the covariance matrix and $c_{jm}$ is mixture weight for mixture component $m$ in state $j$. The Gaussian mixture density is
    \begin{equation}\label{}
      \mathcal{N}(\boldsymbol{o} ; \boldsymbol{\mu}, \boldsymbol{\Sigma})
        = \frac{1}{\sqrt{(2 \pi)^n |\boldsymbol{\Sigma}|}} 
        e^{-\frac{1}{2}(\boldsymbol{o} - \boldsymbol{\mu})^\top 
        \boldsymbol{\Sigma}^{-1} 
        (\boldsymbol{o} - \boldsymbol{\mu})}
    \end{equation}
    Because a mixture of Gaussians can assume arbitrary shapes, they can model non-Gaussian phenomena; no restricting assumption is made about the shape of the PDF when a GMM is used.

    HMMs are used to model sequences, but an observation is independent of past observations. Instead, the regime has a memory, although the shortest possible: the probability of being a certain state in the next time step depends only on the current state and not the previous states. This independence of the previous transitions is called the Markov assumption. Each hidden state pair, represented in Kaldi by an arc from a state to another, is associated with a transition probability $a_{ij}$ that describes how probable it is to move from state $i$ to state $j$. 

    All in all, a HMM is defined by the set of states $S=s_1,s_2,...s_N$, the transition probability matrix $A=a_{11},...,a_{ij},...,a_{NN}$, the emission probabilities $B=b_i(\boldsymbol{o}_t)$, and the initial probability distribution $\pi=\pi_1,\pi_2,...\pi_N$ that models the probability of a state being the first state in the hidden sequence.
    
    % The observations $\boldsymbol{O}=\boldsymbol{o}_1,\boldsymbol{o}_2,...,\boldsymbol{o}_T$ 

    % in Kaldi 
    % http://kaldi-asr.org/doc/hmm.html
    The typical HMM topology for a phoneme is a left-to-right model, also called the Bakis model, with three emitting states that each have a transition to the next state and a self-loop. The model also includes a fourth non-emitting final state that has no outbound transitions. However, in the Kaldi chain models (see Section \ref{sec:dnn_am}), the topology is reduced to have only one emitting state due to a lower time resolution used.
    In Kaldi, the phoneme topology is defined in the lang/topo file.
    
    
    After initialising a HMM for each phoneme, the parameters, i.e. means, covariances and mixture weights need to be estimated in the process referred to as "training" of the model.


  \subsection{Training HMM/GMM AMs and finding the hidden state sequence} \label{sec:hmm_est} 

    The speech recognition system is trained in a supervised manner, meaning that the training data consists of a parallel corpus of speech audio and corresponding correct transcription.
    % However, the task of the acoustic model is not as straightforward as finding a label for an input vector\footnote{In End-to-end ASR the whole ASR task is simplified to outputting an arbitrary length grapheme sequence given the sequence of observed features}.
    The reference transcription can be of arbitrary length but the AM is required to map each observation to a HMM state, generating an equal-length \emph{alignment} of observations and states. The states correspond to phonemes, so an alignment can be mapped to a transcription of the audio, given a lexicon that maps the phoneme sequences to word sequences.

    ML is one criterion of determining the best HMM parameters; others include the MMI criterion which is discussed in Section \ref{sec:mmi}. Section \ref{sec:ml} was a brief discussion about different fitness criteria and optimisation algorithms. It was noted there that for finding the maximum likelihood estimate of a model with latent variables, expectation maximisation is a commonly used optimisation algorithm.

    \subsubsection{The Baum-Welch algorithm} 
      The Baum-Welch algorithm is an expectation maximisation algorithm for estimating the HMM parameters.  Here, the task is to maximise the likelihood $P(\boldsymbol{O}|M)$ of the observations $\boldsymbol{O}$  given the parameters of
      the HMM $M$. % here all HMMs, viterbi: only the best ?
      If the HMM had only one state $j$, the maximum likelihood estimate $\hat{\boldsymbol{\mu}}_j$ would simply be the average of the observations, and $\hat{\boldsymbol{\Sigma}}_j$, too, could be determined directly, using the covariance definition.
      In practice, there are many states which is why the parameters need to be estimated numerically, iteratively. 
      However, the initial parameter values can be taken from simple statistics of the observations. Initially, the observations are divided equally between the states and the means and variances of the states are taken from the average values.

      The maximum likelihood estimates for the mean and covariance are 
      \begin{equation}\label{eq:mu_hat}
        \hat{\boldsymbol{\mu}}_j = \frac{\sum^T_{t=1}L_j(t)\boldsymbol{o}_t}
          {\sum^T_{t=1}L_j(t)}
      \end{equation}
      and
      \begin{equation}\label{eq:sigma_hat}
        \hat{\boldsymbol{\Sigma}}_j = \frac{\sum^T_{t=1}L_j(t)
          (\boldsymbol{o}_t - \boldsymbol{\mu}_j)
          (\boldsymbol{o}_t - \boldsymbol{\mu}_j)^\top }
          {\sum^T_{t=1}L_j(t)}
      \end{equation}
      The numerator and denominator sums for both parameter groups are \emph{accumulated} from the observations. This is the M-step in this expectation maximisation algorithm. The E-step includes finding the optimal alignment given the current HMM parameters, which is achieved using the forward-backward algorithm.

      % kaldi: gmm-init, HTK: Hinit

        % 
      
      % The training starts with some trial values of the parameters, for which

      % , for which the likelihood is calculated, given each training example.
      
      The state occupation probability
      \begin{equation}\label{eq:occ}
        L_j(t) = P(x(t)=j|\boldsymbol{O},M),
      \end{equation}
      i.e., the probability of being in state $j$ at time $t$, is calculated using using the forward-backward algorithm. The forward probability $\alpha_j(t)$ and backward probability $\beta_j(t)$ are defined as 
      \begin{align}
        \alpha_j(t) &= P(\boldsymbol{o}_1,...,\boldsymbol{o}_t,x(t)=j|M)  \label{eq:forward} \\
        \beta_j(t) &= P(\boldsymbol{o}_{t+1},...,\boldsymbol{o}_T|x(t)=j,M) \label{eq:back}
      \end{align}
      Spelled out, $\alpha_j(t)$ is the probability of the partial observation sequence up to time $t$ and that $M$ is in state $j$ at the time step. The backward probability is the probability of the partial observation sequence at the subsequent time steps up to the last vector, given that at the current time the model state is $j$. 
      The forward probability is a joint probability of the observations and the state, whereas the backward probability of the observations is conditional on the state. This allows for the state occupation probability to be determined by the product of the forward and backward probabilities (from Eqs. \ref{eq:occ}, \ref{eq:forward}, and \ref{eq:back}):
      \begin{align}\label{}
        L_j(t) = \frac{ \alpha_j(t) \beta_j(t) }{P(\boldsymbol{O}|M)}
      \end{align}

      The forward and backward probabilities, $\alpha$ and $\beta$, are calculated respectively using the recursions
      \begin{align}
        \alpha_j(t) &= \bigg[ \sum^{N-1}_{i=2}\alpha_i(t-1)a_{ij} \bigg] b_j(\boldsymbol{o}_t)
        \label{eq:alpha_recurs} \\
        \beta_i(t) &= \sum^{N-1}_{j=2} \beta_j(t+1) a_{ij} b_j(\boldsymbol{o}_{t+1}) 
        \label{eq:beta_recurs}
      \end{align}
      and the initial conditions
      \begin{align}
        \alpha_1(1) &= 1, \; \; \; \; \alpha_j(1) = a_{1j} b_j(\boldsymbol{o}_1) 
        \label{eq:alpha_init} \\
        \beta_i(T) &= a_{iN} \label{eq:beta_init}
      \end{align}
      for $1<j<N$. The final conditions are
      \begin{align}
        \alpha_N(T) &= \sum^{N-1}_{i=2}\alpha_i(T)a_{iN} \label{eq:afinal} \\
        \beta_1(1) &= \sum^{N-1}_{j=2} a_{1j} b_j(\boldsymbol{o}_1) \beta_j(1) \label{eq:bfinal}
      \end{align}
      where the limits of the sums exclude the states $1$ and $N$ because they are non-emitting. The recursion of Eq. \ref{eq:alpha_recurs} calculates the forward probabilities (of seeing the specified observations and being at the state $j$) by summing all possible forward probabilities for all possible predecessor states $i$ weighted by the transition probability $a_{ij}$.
      From Eqs. \ref{eq:forward}, \ref{eq:afinal}, and \ref{eq:bfinal} it follows that calculating the forward probability also yields the total likelihood $P(\boldsymbol{O}|M)=\alpha_N(T)$. This allows for selecting the hidden path with the maximum likelihood, which was the aim of the E-step.
      % By calculating the forward probability the best hidden path can be determined. The forward-backward algorithm is therefore the E-step that finds the optimal alignment given the HMM parameters.
      % The M-step consists of accumulating the averages from training examples

    \subsubsection{Viterbi training} 
      An alternative approach to the Baum-Welch algorithm is an iterative procedure called Viterbi training (VT), also called Viterbi extraction or Baum-Viterbi algorithm since it involves the Baum re-estimation (Eqs. \ref{eq:mu_hat} and \ref{eq:sigma_hat}) and the Viterbi algorithm \citep{lember2008adjusted}. Instead of maximising the likelihood of all the data as in Eq. \ref{eq:alpha_recurs}, in VT the probability of only the most likely hidden sequence is maximised
      \begin{equation}
        \phi_N(T) = \max_i \{ \phi_i(T)a_{iN} \}
      \end{equation}
      for $1<i<N$ where
      \begin{equation}
        \phi_j(t) = \max_i \{ \phi_i(t-1)a_{ij} \} b_j(\boldsymbol{o}_t)
      \end{equation}
      and initially
      \begin{align}
        &\phi_1(1) = 1 \\
        &\phi_j(1) = a_{1j}b_j(\boldsymbol{o}_1).
      \end{align}
      for $1<j<N$. This alignment process finds an arc $ij$ for each observation $\boldsymbol{o}_t$.
      % so that the last 

      

      % In Viterbi training, the HMM parameters and the most probable hidden state sequence, both unknown, are estimated alternately. After updating the HMM parameters, the training data observations are aligned with the states. The new alignment is then used for estimating the HMM parameters again, and so on. 
      % Within each state, a further alignment is made to align observations with mixture components. 
      % Using the Viterbi algorithm, the states are aligned with the observation sequence by maximising

      In order to find the most likely hidden sequence the Viterbi algorithm is applied.  The Viterbi algorithm creates paths through the sequence by selecting for each time step the maximum of the previous state likelihoods multiplied by the transition probability to the state. After finding the maximum of the preceding state probabilities for each time step, the sequence is traced back starting from the last unit and selecting the best preceding tags for each tag. This trace-back determines the most likely sequence. In practice, the algorithm becomes computationally too expensive to calculate for long sequences with a large vocabulary, so to approximate the most likely sequence beam search is used, which keeps only $k$ best hypotheses of each hidden state.

      Viterbi training results in an approximation of the maximum likelihood estimate, which was computed in the Baum-Welch algorithm (in theory; beam search makes also the Baum-Welch an approximation). The approximation is convenient, since using only the best hidden sequence for updating the HMM parameters makes the Viterbi training is computationally less expensive than the Baum-Welch algorithm. Viterbi training is used in the Kaldi toolkit for estimating the HMM/GMM acoustic models in the standard recipes.


      % AS stated in the Kaldi documentation, computed alignment is a sequence of transitions that corresponds to an utterance. 

    \subsubsection{A discriminative training criterion: MMI} \label{sec:mmi} 

      % The maximum likelihood method aims to  
      % Typically?
      % Discriminative training is based on an objective function which is minimised or maximised using an optimisation algorithm.

      The MLE method described in Section \ref{sec:hmm_est} aims to maximise the likelihood of the observed sequence given the most probable HMM in Viterbi training, or all the possible HMMs in Baum-Welch.
      % This method results in a model that predicts the training data by determining a probability distribution over the feature space, i.e., over all possible observations. This is called a generative model since the distribution can be sampled for predictions. However, the ability to produce examples of the modelled data is redundant in classification tasks.
      % Furthermore, t
      The MLE method maximises the likelihood of the observations for all of the competing HMMs independently of each other. However, the ultimate aim is to find the HMM that most accurately models the observation sequence, so it would make sense to also try to find meaningful differences between HMMs.
      % The HMMs competing for being the most accurate model of the observations are not compared to each other.
      In discriminative training, instead of maximising the likelihood of the data given the
      % generative
      model, a model is trained to discriminate between the classes, which in this case are different phoneme sequences, corresponding to HMMs. This way, in principle, more of the model capacity is utilised to model the boundaries between different HMMs, instead of using it to model just the relations between individual HMMs and alignments. 

      The discriminative objective function can be simply the difference between the correct classifications (e.g., a phoneme sequence) for a set of examples and the classifications assigned to them by the model. This is called the minimum classification error (MCE) criterion \citep{juang1997minimum}.
      Another type of objective function is the maximum mutual information (MMI) \citep{bahl1986maximum} criterion
      \begin{equation} \label{eq:mmi}
        \mathcal{F}_{\textup{MMI}}(M) = \sum_{r=1}^R \log 
          \frac{P(\boldsymbol{w}_r)P(\boldsymbol{O}_r|\boldsymbol{w}_r)}
            {\sum_{\boldsymbol{w}} P(\boldsymbol{w})P(\boldsymbol{O}_r|\boldsymbol{w})}
      \end{equation}
      where $\boldsymbol{w}_r$ is the correct transcription for the $r$'th speech file \citep{povey2005discriminative}. The numerator is the log-probability of the output sequence, and the denominator is the log-probability of all possible output sequences. This way the probability of a particular sequence is normalised by the probability of all sequences. In other words, the probability of all possible sequences is \emph{minimised}, while maximising the probability of the correct output sequence. Since the correct sequence is included also in the denominator, the maximum value of the objective function is zero.
      % equivalent to conditional maximum likelihood 
      % extended b-w

      Optimisation w.r.t the MMI criterion is achieved by the extended Baum-Welch (EBW) algorithm. The Gaussian parameter updating formulas are reminiscent of the Baum-Welch updating formulas (Eqs. \ref{eq:mu_hat} and \ref{eq:sigma_hat}), whence the name. The EBW is described for example by \citet{jiang2010discriminative}.
      % woodland2002large

      % pylkkonen2012analysis
      
      % vesely2013sequence

      % Traditionally this has been done by training a cross-entropy system, generating word lattices with a weak language model, and using these lattices as an approx- imation for all possible word sequences in the discriminative objective function– as was done when Gaussian Mixture Mod- els were the state of the art povey2016purely

    \subsubsection{Phone context, state tying, and phonetic decision trees} \label{sec:triphone} 
      Phones of the same phoneme sound different when flanked by different phonemes. For this reason, contextual information is modelled, too, by taking into account the preceding and subsequent phonemes of the modelled phoneme. These phonemes with left and right context of one are called triphones, and each can be assigned a HMM \citep{schwartz1985context}.

      % The monophone models are first trained
      % (with single-component Gaussians?)
      % and
      % The triphone models are initialised with the monophone model set parameters. The number of Gaussians in the triphone models is increased gradually and the parameters are re-estimated.

      % transition modelling
      % The transition probabilities from a state to another are essentially the counts of the transitions seen in the training corpus.

      % tying
      When considering triphones instead of monophones, the number of possible phonemes increases significantly. This means each class of triphone will include fewer instances in the training data, which brings difficulties in estimating the state output PDFs. To alleviate the data sparseness, the states of the phoneme HMMs are can be tied together so that the parameters of the output distributions of those states are shared. This makes the estimation of the parameters more robust because there are more training data occurrences, and also makes the total system more compact with fewer parameters \citep{young1992general}.
      
      States are clustered based on a chosen metric of similarity.
      % http://kaldi-asr.org/doc/tree_externals.html
      In tree-based clustering as described by \citet{young1994tree}, the states are divided into branches in a top-down optimisation procedure. Starting from the root node, the question that maximises the likelihood is selected for the node, with the data on each side of the divide being modelled by a single Gaussian.
      In phonetic decision trees the questions are about the context of the phone, e.g. "Is the phone on the left of the current phone a fricative?". 
      After the procedure, the leaves of the tree are the state clusters in which the states are tied. In the final stage, leaves can be merged if the likelihood does not decrease more than a threshold value.

      After a tree has been constructed for the states of the triphone models, also previously unseen triphones can be synthesised by traversing the tree to the appropriate leaf node, i.e., cluster, by answering the questions about that triphone's context and using the tied states of that cluster.


  \subsection{Deep neural networks for acoustic modelling} \label{sec:dnn_am} 

    In the previous decade, deep neural networks achieved state-of-the-art results in acoustic modelling, supplanting the Gaussian mixture models as the most accurate method to classify observations into phoneme classes. In the HMM/DNN hybrid approach, DNNs provide \emph{pseudo-likelihoods} of the observations for each HMM state.
    % This is due to DNNs being discriminative instead of generative like GMMs.
    
    The approach defined in Eq. \ref{eq:asr-bayes-2} works well for traditional ASR systems that use GMMs for acoustic modelling. However, if the generative GMM is replaced with a discriminative neural network model, it does not produce an acoustic likelihood $P(\boldsymbol{o}_t|\boldsymbol{s}(t))$ but a state level posterior probability $P(\boldsymbol{s}(t)|\boldsymbol{o}_t)$, which is a problem because the decoding (Eq. \ref{eq:asr-bayes-2}) relies on the likelihoods. This mismatch can be bypassed by applying the Bayes' rule to produce pseudo-likelihoods:
    \begin{equation}
      P(\boldsymbol{o}_t|\boldsymbol{s}(t)) =
      \frac{P(\boldsymbol{s}(t)|\boldsymbol{o}_t)P(\boldsymbol{o}_t)} {P(\boldsymbol{s}(t))} \propto	 
      \frac{P(\boldsymbol{s}(t)|\boldsymbol{o}_t)} {P(\boldsymbol{s}(t))}
    \end{equation}
    The state priors $P(\boldsymbol{s}(t))$ can be gathered from corpus frequencies \citep{Bourlard1994}. 

    \subsubsection{TDNNs and RNNs} 
      Two common types of DNN used for acoustic modelling are RNNs and time delay neural networks (TDNN) \citep{waibel1989phoneme}. Both RNNs and TDNNs are inherently suited to modelling time-series data, where it is important to capture dependencies across time steps, such as speech. However, the two types of DNN are different in how they achieve this.

      As described in Section \ref{sec:rnn}, RNNs include a cycle between network connections, which creates a dependence of the current output on the previous outputs. LSTMs, also described in Section \ref{sec:rnn}, are a commonly used type of RNN used for acoustic modelling.
      
      TDNNs, also called 1-dimensional convolutional neural networks, are feedforward networks whose layers have a hierarchical structure. Each unit processes inputs from the previous layer across some predefined context, for instance units from time steps $t-4$ to $t+3$. As the next layer has also a context of a few temporal steps, the units in the last layer have a widest temporal dependence. In addition, subsampling the processed inputs can be used to decrease the computational costs \citep{peddinti2015time}. This is similar to the stride of convolutional neural networks, and the subsampling factor is also called time-stride in Kaldi scripts. Figure \ref{fig:tdnn} illustrates the TDNN structure with and without subsampling. 

      \begin{figure}[htb]
        \centering
        \includegraphics[width=12cm]{tdnn}
        \caption{TDNN structure with (red) and without (red and blue) subsampling \citep{peddinti2015time}.}
        \label{fig:tdnn}
      \end{figure}

      In Kaldi, TDNN layer weights are often decomposed into two smaller factors to reduce the number of parameters of an already-trained neural network. The smaller number of dimensions of the two factor matrices is called the bottleneck dimension \citep{povey2018semi}.

    % (explaining RNNs and TDNNs in previous sections?)

    \subsubsection{Sequence-level lattice-free MMI} 
      Currently, the state-of-the-art implementations of DNNs in Kaldi are trained using lattice-free MMI (LF-MMI) training criterion, as described by \citet{povey2016purely}. These are called "chain" models in Kaldi. LF-MMI is a sequence discriminative criterion, which means that the aim is to maximise the conditional log-likelihood (Eq. \ref{eq:mmi}) of the correct transcript on the sequence level. In the traditional MMI approach, a cross-entropy system is trained to  generate lattices for a weak language model, and the lattices are used to approximate the possible word sequences for the discriminative objective function denominator. In LF-MMI, however, the possible word sequences are not approximated with a lattice, but a phone-level language model is computed so that the sum in the denominator is not approximated, but can be computed exactly. This is possible since a phone-level LM requires significantly less memory than a normal word-level LM.  The phone LM is represented as an FST, created in a similar manner as the normal decoding FST described in Section \ref{sec:wfst}. In this case there is no lexicon and grammar FSTs but a phone grammar FST, so the graph consists of the component FSTs $H$, $C$, and $P$. The numerator FST uses a lattice to represent the utterance. The numerator FST is composed with the denominator FST so that the phoneme LM of the denominator removes illegal output sequences. The composition also ensures that the objective function value is negative. 
      % https://desh2608.github.io/2019-05-21-chain/

    \subsubsection{Regularisation of DNN AMs} 
      Regularisation is important in DNN training to impede overfitting. Kaldi chain models incorporate a number of regularisation methods in the DNN architectures. One of the methods is called \emph{L2-regularisation}, which applies the euclidean norm to penalise elements in the output vector that tend to blow up. This is achieved by subtracting $\frac{1}{2}c||\boldsymbol{y}||_2^2$ from the objective function of each frame output $\boldsymbol{y}$, where $c$ is a user-set scaling factor, e.g. 0.005 \citep{povey2016purely}. In Kaldi, L2-regularisation is applied on the outputs; note that this method is different from the L2-regularisation applied to the weights, which is another commonly used regularisation method.
      
      Another widely used regularisation method for deep neural nets are \emph{dropout layers} which randomly sample a subset of the neurons and their connections which are omitted, dropped out, during an iteration of the training \citep{srivastava2014dropout}. This prevents the neurons from relying on the other neuron's outputs too much, i.e., prevents too much \emph{co-adaptation} among the parameters. Dropping out random units in training shifts the unit of adaptation down from large groups of neurons to individual neurons, since it is not guaranteed that the other neurons are present. This principle is explained through an analogy to sexual selection in the above mentioned original paper. Since genes are mixed randomly with another set of genes from a conspecific between every generation, individual genes cannot rely on some particular other genes to be present in the future. This inhibits aggregation of large interdependent gene complexes, in which genes function well only with the group of co-adapted genes. These kinds of complexes would become rigid, and fragile to the inevitable mutations that will always occur quite randomly: new genes would be difficult to incorporate in the complex. This kind of rigidity is analogous to overfitting to the training data and failing to find generalisable rules that apply to new data samples from the hidden distribution, which regularisation attempts to lessen. Kaldi uses dropout schedules that vary the dropout rate as a function of the number of epochs trained \citep{cheng2017exploration}. For example, if the dropout rate is 0 at the start of training ($f(0)=0$), 0.2 midway of the training ($f(0.5)=0.2$), and 0 at the end of the training ($f(1)=0$), the Kaldi notation for the dropout schedule is \texttt{0@0,0.2@0.5,0@1}, which is written in shorthand just \texttt{0,0.2@0.5,1}. The dropout rate is linearly interpolated between these points.

      % It has been empirically shown that dropout layers 

      A third type of regularisation technique is called \emph{leaky HMM} in Kaldi. Transitioning from any state $a$ to any state $b$ is allowed once per frame with the probability of a small (typically around 0.01) \emph{leaky-hmm-coefficient} times the probability of state $b$. The aim is to effect a gradual forgetting of the context, since transitioning to a random state is equal to stopping and restarting the HMM \citep{povey2016purely}. This reduces the overfitting caused by too much memorising of the training data sequences.

      Another technique to regularise the sequence-level training is to add a separate output layer to the network that learns the cross entropy objective, as well as a separate last hidden layer. This means the network has two output branches with two separate weight matrices in each branch. After the training, the cross entropy branch can be discarded, and the main, sequence output branch is left to be used in decoding.
      This technique is abbreviated as \texttt{xent\_regularize} in the Kaldi code, and the associated hyperparameter is a scaling factor for the cross entropy objective, typically 0.1 because its dynamic range is naturally larger than that of the MMI objective function \citep{povey2016purely}.


  \subsection{Speaker-adaptive training} \label{sec:adaptive_training} 
    % alignments % lda + mllt
    % https://kaldi-asr.org/doc/transform.html

    % acoustic normalisation = feature space

    Variability between speakers poses a challenge to an ASR system. Each speaker may have an idiosyncratic voice, distinct style of pronunciation as well as distinct recording conditions, which can degrade the ASR performance as the training speech set and test speech set differ from each other. This section describes some of the methods to account for inter-speaker variability by adapting either the model or the features to a particular speaker. An exhaustive overview of all the used methods is beyond the scope of this thesis.

    The adaptation can be done either in testing or training, usually respectively referred to as \emph{speaker adaptation} and \emph{speaker-adaptive training} (SAT). Speaker adaptation can be done by modifying a speaker-independent (SI) model to create personal, speaker-dependent (SD) models for each speaker.  This can be done by taking a small number of speech data from the speaker and using this adapt a SI model to the specific speaker \citep{shinoda2011speaker}. In SAT, speaker-specific information is incorporated in the training of the model. When a model is trained using SAT, it naturally benefits to use an adaptation scheme also in testing. Both GMM- and DNN-based AMs have been shown to benefit from speaker adaptation as well as SAT. A commonly used SAT method for GMMs is the feature-space MLLR. For DNNs, speaker embeddings can be appended to the speech feature vectors. These methods are described in this section.

    % Cepstral mean and variance normalisation, mentioned in Section \ref{sec:feats}, mitigates the problem of different noise conditions of speech signals. 
    
    % One solution to the problem is \emph{speaker adaptation}, which can be done by modifying a speaker-independent (SI) model to create personal, speaker-dependent (SD) models for each speaker.  This can be done by taking a small number of speech data from the speaker and using this adapt a SI model to the specific speaker. However, this requires data from the speaker and re-estimating the parameters of the model to conform to the speaker.
    % Instead  of adapting the model parameters, speaker adaptation can be done also transforming the features of the test speaker \citep{}.
    
    % Often the the aim is to build a  system that recognises speech also from completely new speakers.
    % Speaker-dependent systems can be adapted from speaker-independent systems by a transformation  
    % The solution to speaker variability discussed in this section is to learn an explicit representation for each speaker's idiosyncratic qualities which is then used in addition to the speaker-independent (SI) model in the training. Because the speaker variabilities are learned by separate models, the SI model encodes, in theory, only the information needed for discriminating between phonemes, and is therefore better able to generalise to new speakers.
    % This is called speaker-adaptive training, or SAT for short.  
    SAT requires identifying each speaker from the metadata that indicates the speaker ID. Adaptation can be useful even if the speaker-identifying metadata is absent, in which case a separate adaptation is learned for each utterance, as if each utterance were spoken by a different person. In this case, the adaptation is for inter-utterance variability in general, so speaker-adaptive training becomes a misnomer. Furthermore, the adaptation can, of course, be done w.r.t any attribute of the data that has been labelled in the metadata, and thus these methods could be termed more generally \emph{attribute-aware training} \citep{rownicka2019embeddings}.

    \paragraph*{Speaker-adaptive training for GMM AMs}
    With a HMM/GMM acoustic model, speaker-adaptive training can be done by representing each speaker's distinct qualities as a transform in the feature space or in the model space. A feature space transform is applied on the observation vectors, and model space transform on the mean and variance of the GMM. Furthermore, a model space transform can be unconstrained or constrained, the former being separate transforms for the means and variances and the latter using the same transform for both \citep{gales1998maximum}.

    Using the model space transform, the aim is to learn the optimal model $\boldsymbol{\theta}$ and adaptation $\boldsymbol{G}^{(r)}$ for speaker $r$
    \begin{equation}
      (\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{G}}) =
          \argmax_{(\boldsymbol{\theta},\boldsymbol{G})} \prod^R_{r=1}
          P(\boldsymbol{O}^{(r)}; \boldsymbol{G}^{(r)}(\boldsymbol{\theta}))
    \end{equation}
    where $\hat{\boldsymbol{\theta}}$ are the model parameters \citep{anastasakos1996compact}.
    The feature space transform is analogous, only transforming the observations instead of the model.

    A common SAT transform is the maximum likelihood linear regression where $\boldsymbol{G}(\boldsymbol{\theta})$ is an affine\footnote{\emph{Linear} regression is thus a slight misnomer.} transformation defined by the matrix $\boldsymbol{A}$ and bias $\boldsymbol{b}$ 
    \begin{equation}
      \boldsymbol{\mu}^{(r)} = \boldsymbol{A}^{(r)}\boldsymbol{\mu}+\boldsymbol{b}^{(r)}
    \end{equation}
    Whether an adaptation modifies the model or the features is in some cases only a question of interpretation when describing the method, and a question of choosing among two equivalent implementations. The constrained MLLR (CMLLR) can be represented as a feature space transform \citep{gales1998maximum}, and is therefore also called feature-space MLLR or fMLLR. The transformation projects the features from the speaker-specific space to the speaker-normalised space.

    In practice, in the Kaldi implementation, affine transform is applied by appending a 1 to the feature vector, and multiplying it with the linear transform $\boldsymbol{A}$ concatenated with the constant offset (bias) $\boldsymbol{b}$: $\bracketVectorstack{  \boldsymbol{A} ; \boldsymbol{b}} \bracketVectorstack{\boldsymbol{o} \\ 1}$. The Kaldi program \texttt{transform-feats} is used to multiply the feature vectors with transform matrices.
    % \citep{povey2008fast}

    \paragraph*{Speaker-adaptive training for DNN AMs}
    As well as GMM parameters, also DNN acoustic models (see Section \ref{sec:dnn_am}) can be estimated speaker-adaptively. Adapting can be performed in the feature-space by appending or transforming the observations or in the model-space by modifying the DNN AM parameters.
    
    A common method is to extract i-vectors from speakers \citep{ivector}, optionally perform a transformation of the i-vectors using a control network, and append them to the features that are fed to the DNN \citep{miao2015speaker}. i-vectors have usually a hundred or a few hundred dimensions that encode properties of a speaker as well as the environment, enabling the AM to generalise more robustly to speech in different conditions. The original paper by \citet{ivector} referred to the i-vector as the \emph{total factors} $\boldsymbol{w}$ in the \emph{total variability space} $\boldsymbol{T}$ because it models both speaker and channel variability in contrast with joint factor analysis \citep{kenny2005joint} which makes a distinction between the two sources of variability. 
    
    The total variability space $\boldsymbol{T}$ models the variability between utterances.
    % , and it is also referred to as the i-vector extractor \citep{alam2014use}.
    % The i-vector extractor is characterised by $\boldsymbol{m}$ $\boldsymbol{T}$ and $\Sigma$
    i-vector extractor training starts by estimating a speaker-independent GMM called an \emph{universal background model}, or UBM. The purpose of the UBM is to represent the general, or universal, characteristics of speech, i.e., it is the speaker-independent model. Baum-Welch statistics are obtained from the UBM at the frame level. The i-vector is then defined as the mean vector of the posterior Gaussian distribution conditioned on the Baum-Welch statistics for a given utterance \citep{ivector}. In practice it is a MAP estimate \citep{kenny2005eigenvoice}. The mean vector (i-vector) is actually a concatenation of the mean vectors of the mixture components, called a supervector \citep{campbell2006svm}. A speaker-specific utterance supervector $\boldsymbol{M}$ is composed of the factors
    \begin{equation}
      \boldsymbol{M} = \boldsymbol{m} + \boldsymbol{T} \boldsymbol{w}
    \end{equation}
    where $\boldsymbol{m}$ is the UBM supervector.  The extractor compresses the high-dimensional statistics from the UBM into the dense i-vector representation for a given utterance. 

    Another approach to modelling speaker characteristics is to train a feedforward deep neural network to project speakers into an embedding space that models the speaker variance. \citet{snyder2017deep} introduced a \emph{speaker embedding} method  where the DNN is trained to classify speakers from variable-length segments. The DNN learns to assign each speaker an embedding space vector which can be then used in the AM training similarly to an i-vector.
    \citet{snyder2018x} describe the use of DNN-generated speaker embeddings, which they call x-vectors, in speaker  recognition.
    In contrast with i-vectors, x-vectors model only the speaker characteristics since the DNN is trained to identify speakers. 

    \begin{figure}[htb]
      \centering
      \includegraphics[width=10cm]{xvec}
      \caption{The DNN used for x-vector extraction \citep{snyder2017deep}.}
      \label{fig:tdnn}
    \end{figure}

    Speaker embeddings can be extracted either online or offline. This refers to which frames can be used in the extraction. When streamed audio is transcribed on the fly, the transcribed utterance is partial: only the frames up to time $t$ can be be used in the embedding instead of the complete utterance. Online decoding is simulated in the standard recipes in Kaldi, and the embeddings are extracted using partial utterances every 10 frames or so. In online extraction, the features are carried over from previous utterances of the same speaker.
    Offline extraction can be used when there is no need to stream the features or you do not want to simulate this application). Offline extraction is standard when using the embeddings in speaker recognition instead of speech recognition.


    % In decoding, i-vectors can be extracted, for example, every 10 frames, and an utterance-level i-vector can be obtained by averaging the frame-level i-vectors across the utterance.
    
    

    %%%%  
      % A speaker-independent (SI)!!!!!!!!!!! ASR system can be trained on and transcribe the speech of different speakers, in different recording conditions, 
      % % about different topics,
      % and with other distinct attributes. When these speech audio attributes are known, and in some way consistent, they can be taken into account in the training of and decoding with the system.
      
      % This section discusses how an ASR system training can be improved by  and learning to adapt the general acoustic model to each speaker.
      
      % The attributes that are distinct but consistent for each speaker include obviously the qualities of the speaker's voice, but also the recording conditions since it can be assumed that these stay constant per speaker. Alternatively, adaptation can be performed for each environment \citep{}.
      
      % The idea can also be generalised to any other attributes of the speech which modulates the speech in some consistent manner, for example the gender of the speaker. 

      % For each speaker, each feature vector is transformed by multiplying it by a matrix that encodes information about how this speaker differs from the general speaker.
      
      % The transform can be done when testing the final model (speaker adaptation, discussed in Section \ref{sec:adaptation}) and in the training process. The latter is called adaptive training.
      
      % First, a transform is generated for each speaker. 


  \subsection{Silence and pronunciation probability modelling} \label{sec:sil_prob} 

    The Kaldi toolkit allows also for modelling the probability of an optional silence between specific words, and the probability of different pronunciations of a word. These methods have been found to improve WER results by a small but consistent margin \citep{chen2015pronunciation}. 

    Differences of pronunciation are significant in many languages. However, in Finnish, which has a phonemic orthography (see Section \ref{sec:lexicon}), pronunciation probabilities are not applicable. If the mapping from phonemes to graphemes is one-to-one, there are no alternative pronunciations for a word.

    The probability of an optional silence phone between two words is estimated from statistics collected from alignments. 
    When using a subword vocabulary, it is important to indicate which boundaries are actual word boundaries, because the optional silence should not be inserted in the middle of a word. \citet{smit2017improved} described and implemented a method to build the lexicon FST in such a way that restricts the use optional silence to only word boundaries.


  \subsection{Mapping words to phoneme sequences} \label{sec:lexicon} 

    The hidden state sequence decoded from a HMM corresponds to a phoneme sequence, but the ultimate aim is to generate a word sequence for a given observation sequence. For mapping words to phoneme sequences, the ASR system applies a \emph{lexicon}, also referred to as \emph{pronouncing dictionary}, or just \emph{dictionary}. 

    In Finnish there is generally a one-to-one mapping from letters to phonemes. In the jargon of linguistics, Finnish has a \emph{phonemic orthography}. This makes creating a lexicon very simple, as each letter can be assigned to a phoneme, and the word-to-phonemes mapping follows trivially.

    \begin{table}[htb]
      \centering
        \begin{tabular}{l|l}
        \toprule
        {[oov]}   & SPN \\
        !SIL    & SIL \\
        {[laugh]} & SPN \\
        {[reject]} & NSN \\
        +i+     & I \\
        +loma   & L O M A \\
        +ssa    & S S A \\
        avoim+  & A V O I M \\ 
        zoom+    & T S O O M   \\
        äitiys+ & AE I T I Y S \\
        överi   & OE V E R I  \\
        über    & Y Y B E R  \\
        \bottomrule 
      \end{tabular}
      \caption{An example of a lexicon.
      The first entries have special phonemes: spoken noise (SPN), silence (SIL) and non-spoken noise (NSN). "oov" refers to out-of-vocabulary words. 
      "+" is the intra-word boundary marker in subword vocabularies.
      Note also the inaccurate pronunciation of "zoom": "T S U U M" would be more accurate.
      }
      \label{tab:lexicon}
    \end{table}

    A lexicon for a language like English has to be constructed largely by hand, as most of the mapping from letters to phonemes does not have a
    sound\footnote{Ignore the pun.}
    logic behind it. This is due to historical change in the pronunciations of words that was not translated into corresponding change in their written forms (e.g., during the Great Vowel Shift) as well as loan words from other languages which have different rules of orthography \citep{english}. For a language that does not have a phonemic orthography but some constant patterns of pronunciation, a machine learning approach can also be used to learn a mapping for the dictionary.
    
    Similar change can be observed in Finnish as words are imported from other languages, mainly from English. Loan words often do not follow the Finnish orthography, which means that the one-to-one mapping from letters to phonemes is no longer accurate for these words. For example, "googlata" is pronounced "G U U G L A T A"\footnote{These are not official phonetic alphabet (they are the phoneme symbols that are used in this thesis) but hopefully the example still makes sense.} instead of following the Finnish orthographic rules to pronounce it "G O O G L A T A", or alternatively spelling it "guuglata". In this study these inaccuracies are ignored, and the Finnish lexicon is generated automatically using the simple letter-to-phoneme correspondence, where a letter corresponds to a single phoneme, usually denoted by itself (with a few exceptions, e.g., "c"->"K", "q"->"K V"). Table \ref{tab:lexicon} is an example of a few lexicon entries that follow the word-to-phoneme mapping used in the experiments of this thesis.

    
  \subsection{Weighted finite-state transducers in Kaldi} \label{sec:wfst} 

    Weighted finite-state transducers are a type of automaton in which a transition has an input label, an output label, and a weight. A special case of FST is a finite-state acceptor (FSA) where the input and output labels of a transition are equal. A FST, having both input and output labels, maps an input sequence to an output sequence when a path is taken through it. Figure \ref{fig:wfst} displays a simple example of an FST.
    \begin{figure}[htb]
      \centering
      \includegraphics[width=9cm]{wfst}
      \caption{A weighted finite-state transducer \citep{openfst}.}
      \label{fig:wfst}
    \end{figure}
    The example transducer has three states, the initial state (0) denoted with a bold circle and final state (2) with a double circle. The arcs between states have input labels \{a, b, c\}, output labels \{x, y, z\}, and real number (floating point) weights associated with them. Also the final state has a weight. This transducer would map the input sequence "ac" to the output sequence "xz" with the weight calculated by summing the individual arc weights, in this case adding up to 6.5.

    FSTs are used in Kaldi to encode many of the components of the ASR system. In training, the lexicon is represented as a FST. In decoding, the complete ASR system is encoded into a FST, called a \emph{decoding graph} before the actual decoding, as the graph creation is typically the most resource-hungry step in the process. In Kaldi chain models, FSTs are used in the LF-MMI training to encode the denominator and numerator of the objective function (see Section \ref{sec:dnn_am}).

    \paragraph*{Kaldi decoding graph creation}
    Kaldi composes the $HCLG$ decoding graph from four component transducers: the HMM $H$, the context $C$, the lexicon $L$, and the LM (or grammar) $G$ \citep{povey2012wfst}. 

    The n-gram language model that is used in decoding
    % (in training, only a lexicon is needed)
    is converted from the ARPA format into the $G$ transducer using the \texttt{arpa2fst} program. The purpose of $G$ is not to transduce a sequence from one domain to another, but to assign weights to the possible word sequences. For this reason, the input and output labels are equal, making it technically a finite-state acceptor. The ARPA model lists the conditional base-10 logarithmic probabilities for each n-gram. These give weights to the arcs of the corresponding paths in the acceptor, the arc weights having an inverse relation to the probabilities (i.e., they are negated) and using natural logarithm instead of 10-base.  Referring to the weight of decoding graph FST as "cost" is more intuitive, so in this text this term is used, too. A complete path through the acceptor corresponds to a word sequence and the cost of the path indicates how \emph{un}likely the sequence is.
    If a probability of the highest-order n-gram has not been explicitly specified  in the n-gram model, a path is taken through the backoff node which corresponds to the lower-order backoff n-grams and the costs are determined by the associated backoff probabilities. Since the model can recursively back off  all the way to unigrams, any word sequence is accepted and given a cost. The backoff arcs do not have a word label, which raises the problem that there are multiple paths for a single input sequence, making processing the graph inefficient.
    % \footnote{more disadvantages?}
    The backoff arcs are therefore assigned a \emph{disambiguation symbol} "\texttt{\#0}" as the input label. The disambiguation symbol allows for an operation (in practice, an algorithm) called \emph{determinisation}. A deterministic transducer has the property that one input string matches at most one path \citep{mohri2008speech}.
    The acceptor also dismisses sentence start and end tokens of the LM  (\texttt{<s>} and \texttt{</s>} or whatever they are) since these are not wanted in the speech transcripts. The number of word types used in speech recognition can also be limited to make the decoding graph of feasible size, in which case some types are omitted from the LM acceptor.

    The lexicon transducer $L$ maps a phoneme sequence input to an output consisting of one word. If the lexicon has multiple words with the same pronunciation, or when a phoneme sequence is a part of multiple word pronunciations, word disambiguation symbols \{\texttt{\#1}, \texttt{\#2},...\} are needed to ensure each phoneme sequence has only one possible word output. Ambiguity in the transducer means that there could be multiple paths matching one input string.  
    See Section
    \ref{sec:lexicon} for discussion about orthography and Section \ref{sec:sil_prob} for discussion about the case where one word has multiple pronunciations. 

    The grammar acceptor $G$ and lexicon transducer $L$ are combined to compose a new transducer $LG$ that maps a phoneme sequence input to a word sequence output. The composition is done in the program \texttt{fsttablecompose}, and the new transducer is determinised in the program \texttt{fstdeterminizestar}.

    $C$ is the transducer from context-dependent phonemes to context-independent phonemes. By composing $C$ with $LG$, the triphones are mapped to words. $C$ is built dynamically in the process of composing it with the existing $LG$ FST.
    The final FST is the HMM, which maps the state transitions to triphones, or more generally any context-dependent phonemes. After $H$ is composed with the $CLG$, the $HCLG$ is complete and can be used for lattice generation and decoding.


    % , for example, in the lexicon FST (the file L.fst) to transduce a phoneme sequence to a (sub)word token sequence. A grammar or a language model is encoded in a finite-state acceptor (G.fst) since its purpose is to define the possible sequences without transducing sequences from a domain to another. 

    % Figure \ref{fig:fst} is an example of a grammar-lexicon (LG.fst) transducer. 

    % \begin{figure}
    % \caption{} \label{fig:fst}
    % \end{figure}


  \subsection{Lattices and n-best lists} \label{sec:lattice} 

    The decoding graph built by the Kaldi programs encodes all possible word sequences and the corresponding HMM state sequences. A sequence of observations is decoded with the graph in the manner described by \citet{povey2012wfst}. First, a WFSA $U$ is built from the observations that encodes the acoustic weights for each arc of each state transition that can emit each observation in the sequence. $U$  includes
    % about\footnote{not necessarily exactly due to $\epsilon$ labels.}
    $T+1$ states with arcs between the states that correspond to the HMM states at the time step $t$. The costs of the arcs encode the acoustic log-likelihoods. The utterance-specific \emph{search graph} $S$ that is traversed during decoding is generated by composing $U$ with the decoding graph. The search graph is not searched completely, but a subset of the best paths are selected by \emph{beam pruning}.
    When generating the lattice that encodes the subset of the best paths, some of the desiderata are: that the lattice includes all word sequences within the beam size $\alpha$ that represents a log-likelihood difference to the optimal path; that the scores and alignments in the lattice are accurate; that the lattice does not contain duplicate paths of the same word sequence \citep{povey2012wfst}. Figure \ref{fig:lattice} depicts a word lattice.
    \begin{figure}[htb]
      \centering
      \includegraphics[width=16cm]{lat}
      \caption{A word lattice FST generated by one of the ASR systems built during the thesis. The correct transcription is "nii voi joo siis ei se varmaa siis emmä usko et sitä ongelmaa niinku se mä vaan mietin niinku nyt tota et et et mitä se tuomas sano sanoks se niinku et ne avaimet on niinku menny sen roskiksen mukana niinku mä ehdotin vai sanoks se". Each edge of the lattice is assigned a word as well as acoustic and language model costs (not printed in the figure). The numbers in the figure are identifiers of nodes and words. As a path is taken through the lattice, a transcription is generated.
      % dspf004_dsp2013_01
      }
      \label{fig:lattice}
    \end{figure}

    An alternative to a word lattice is to simply create lists of $n$ best transcriptions of the utterance. Though sometimes simpler to create and use, n-best lists are less efficient than lattices. In n-best lists, there are often transcriptions that differ only by a few words, which is redundant. n-best lists are capable of encoding far fewer transcriptions than lattices in the same amount of memory, and are therefore usually much more restrictive when, for instance, doing rescoring with a language model.

\clearpage
\section{Experiments} \label{sec:experiments} 

  % 

    %   The baseline language models and the ASR system is based on the systems developed by \citet{enarvi2017automatic}.

    This chapter describes the experiments and presents the results. The used evaluation metric is the word error rate (WER), defined as the combined number of errors (substitutions, deletions, and insertions) in the generated transcript divided by the number of words in the correct transcript.
    Before calculating the WER, words are normalised so that similar words are counted as correct words. For example, the words "nii" and "niin" are both correct if the latter is in the reference transcript. This normalisation method is adopted from \citep{enarvi2017automatic}. Because the words are normalised in this way, character error rate is not used.

    Perplexity is an often used metric for evaluating language models. Perplexity $PP$ measures how improbable the test set $W$ is given the language model \citep{Jurafsky2019}:
    \begin{equation} \label{eq:ppl}
        PP(W) = P(w_1,w_2,...,w_N)^{-\frac{1}{N}} =
        \sqrt[N]{\frac{1}{P(w_1,w_2,...,w_N)}} 
    \end{equation}
    and using the chain rule
    \begin{equation}
        PP(W) = \sqrt[N]{\prod^N_{i=1}\frac{1}{P(w_i|w_1,w_2,...,w_{i-1})}}
    \end{equation}
    Perplexity was used for evaluating some of the language models in the hyperparameter tuning phase. However, perplexity does not necessarily correlate with the speech recognition accuracy which is the ultimate evaluation metric. Moreover, after segmenting the words into subwords, the perplexity of a corpus is not comparable to the perplexity measured with whole words.
    For these reasons, the perplexity values are omitted from this chapter, and the presented results include only the more relevant WER results.

    Statistical significance of differences between two results was assessed using the Wilcoxon Signed Rank \citep{wilcoxon1992individual} for the speaker WER. When stating that a result is or is not statistically significant in this chapter, this metric has been used.

  \subsection{Acoustic modelling experiments}

    \subsubsection{Speech corpora} 

      The acoustic model development and test sets are the same as in \citep{enarvi2017automatic}. The training speech data are 37 hours of spontaneous speech from three different sources. The SPEECON corpus consists of 550 speakers speaking 10 spontaneous sentences each \citep {iskra2002speecon}. The DSPCON\footnote{\url{http://urn.fi/urn:nbn:fi:lb-201708251}} corpus consists of 5281 spontaneous sentences from 218 different male students and 24 female students, totalling 9.8 hours \citep{enarvi2018modeling}. The third source is the FinDialogue part of the FinINTAS corpus \citep{lennes2009segmental}. The development and test sets contain additionally spontaneous utterances from radio shows, referred to as RadioCon. Table \ref{tab:am_data} lists the acoustic modelling data sets.

      \begin{table}[ht!]
          \begin{tabularx}{\columnwidth}{l|l|lll|l|lll|c}
              \toprule
              \textbf{Corpus} & \multicolumn{4}{c|}{\# of speakers}
              &\multicolumn{4}{c|}{\# of utterances} & \# of hours \\ \midrule
                          & total &train&devel&eval&total&train&devel&eval &\\
              SPEECON     & 549   & 549& 0 &0 & 5499 &5499& 0 &0 &  18.8   \\ 
              FinDialogue & 22    &22 &0&0   & 6338& 6338 &0&0  & 10.4  \\ 
              DSPCON      & 243   &192 &40 &11 & 5219 &4129& 863& 227 & 9.7 \\  
              RadioCon    & 11    & 0 &5 &6   & 440& 0& 126& 314 & 0.5 \\ \midrule
              total train & \multicolumn{4}{c|}{763} &  \multicolumn{4}{c|}{15966} & 36.6  \\ 
              total devel & \multicolumn{4}{c|}{45} &  \multicolumn{4}{c|}{989} & 2.0 \\
              total eval  & \multicolumn{4}{c|}{17} &  \multicolumn{4}{c|}{541} & 0.7 \\
              % VoxCeleb (speaker embeddings)         &     &     &     \\ \hline
              \bottomrule
          \end{tabularx}
          \caption{Speech corpora. Training data "train", development data "devel", and evaluation data "eval".}
          \label{tab:am_data}
      \end{table}

      \emph{Speed perturbation} is a data augmentation method in which the speed of the audio is increased or decreased \citep{ko2015audio}. This method is used for the DNN AM training data, augmenting the speech data by changing its speed by a factor of 0.9 and 1.1, increasing the amount of data by a factor of three. As the quality, or domain, of the three different data sets is different, it could be useful to augment only the in-domain data, i.e., DSPCON. A system is trained with the whole dataset augmented, as well as one system augmenting only DSPCON. Augmenting only DSPCON yielded worse results than augmenting the complete data set.

      
    \subsubsection{HMM/GMM acoustic model architecture and training}

      Different features are used in GMMs and DNNs. The GMMs input 13 MFCCs and their $\Delta$ and $\Delta\Delta$ features, amounting to a 39-dimensional feature vector. Cepstral mean and variance normalisation is applied to the features per speaker.

      
      The GMM phoneme model set was trained for the most part by following the Kaldi recipe for the Wall Street Journal corpus\footnote{\url{https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/run.sh}}. The monophone model set is trained with a subset of 2000 shortest utterances. The features are first aligned equally with the states, after which 40 iterations of Viterbi training are performed to estimate the HMM/GMM model set and generate an improved alignment. The number of Gaussians is increased in between iterations, reaching a total of 1000 Gaussians. 

      The triphone model set is initialised from the monophone models and their alignments, and is trained in a three consecutive steps. Subsets of the data set is selected, including 4000 utterances for the first step, 8000 utterances for the second step, and then using the whole ~15000 utterance training data set for the third triphone training step. The first step (\texttt{train\_deltas.sh}) is similar to monophone training, using the delta and delta-delta features in addition to the MFCCs, and increasing the total number of Gaussians to 10k. The second step (\texttt{train\_lda\_mllt.sh}) splices the MFCC features, reduces the dimensionality back to 40 using LDA, and estimates an MLLT transform. The MLLT transform is applied on the features and new estimates of the models are computed, increasing the total number of Gaussians to 15k. The third and final triphone training step (\texttt{train\_sat.sh}) does speaker-adaptive training utilising fMMLR. The total number of Gaussians is increased to 40k.

      After the triphone model set is trained, silence probabilities (see Section \ref{sec:sil_prob}) are estimated from the statistics of the training data alignments. Since the Finnish lexicon is a one-to-one mapping from letters to phonemes, there are no optional pronunciations of words for which to calculate pronunciation probabilities.

      An MMI (see Section \ref{sec:mmi}) system is trained based on the triphone alignments and silence probabilities. The MMI model generates the alignments that are used to train the deep neural network acoustic model.

      The GMM triphone model set was kept fixed throughout the experiments, after first running a couple of experiments to tune the model. These experiments included evaluating the MMI model compared to a fourth triphone training step.
      %  and experimenting on a couple of different 


    \subsubsection{HMM/DNN acoustic model architecture and training}

      The DNN AM inputs higher-resolution features than the GMM AM. 40-dimensional MFCCs are extracted from the data set, which has been augmented using speed perturbation. The MFCCs are concatenated with a speaker embedding vector, either i-vector or x-vector (see Section \ref{sec:emb_expts}), which has 100 or 200 dimensions. 

      The DNN/HMM is a Kaldi chain model, trained on the LF-MMI objective. The DNN is a TDNN with 15 layers that have 1536 dimensions and a bottleneck dimensionality of 160. The time-stride is 1 for the first 3 layers. 0 for the 4th layer, and 3 for the remaining layers. The affine layer before softmax has 256 dimensions. The dropout schedule is set to \texttt{0,0@0.20,0.5@0.50,0}, the xent-regularize factor to 0.1, and the leaky-hmm coefficient to 0.1. The mini-batch size is set to 64. The TDNN is trained for 6 epochs with an initial learning rate of 0.00025 and final learning rate of 0.000025.
      
      % The first layer of the DNN

      % Table \ref{tab:am} lists the TDNN AM layers.

      % \begin{table}[ht!]
      %     \begin{tabularx}{\columnwidth}{|X|l|l|}
      %         \hline
      %         \textbf{layer} & number of neurons & activation function \\ \hline
      %          &   & \\ \hline
      %     \end{tabularx}
      %     \caption{}
      %     \label{tab:am}
      % \end{table}
      

    \subsubsection{Speaker embedding experiments} \label{sec:emb_expts}

      %  spk embs can be trained with out of domain data

      In the speaker embedding experiments, pretrained extractors are used as well as extractors trained on the AM training data. 

      \paragraph*{Pretrained VoxCeleb i-vector and x-vector extractors} 
        Pretrained i-vector and x-vector extractors trained on the VoxCeleb data \citep{nagrani2017voxceleb, chung2018voxceleb2} are used in the experiments. The VoxCeleb1 data contains about 100k utterances from 1251 celebrities and the VoxCeleb2 data contains about 1M utterances from over 6000 speakers. The code that was used for training the extractors is in the Kaldi repository\footnote{i-vectors: \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/voxceleb/v1}}\footnote{x-vectors: \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/voxceleb/v2}} and the pretrained extractors are downloaded online\footnote{\url{https://kaldi-asr.org/models/m7}}.
        
        The VoxCeleb i-vector extractor was trained on 24 MFCCs and their delta and delta-delta coefficients. An energy-based VAD system is used to select the voiced frames for both i-vector and x-vector systems. The i-vector system UBM is a GMM that has 2048 full-covariance component Gaussians. The i-vectors have 400 dimensions, and are subsequently reduced to 200 dimensions using an LDA model.

        The VoxCeleb x-vector extractor is trained on the VoxCeleb speech data that has been augmented in various ways. The MUSAN corpus \citep{snyder2015musan} of music, noise and speech as well as simulated room impulse response \citep{ko2017study} are used to generate noise for the speech data. The noise is added to the speech data and a subset of the noisy audio files is randomly selected and pooled with the clean audio files. This increases the amount of data roughly twofold. 
        The x-vector extractor was trained on 30 MFCCs with their deltas and delta-deltas. The DNN is a TDNN with ReLU non-linearities where the five first layers use frame-level training with a temporal context of a few adjacent frames. The architecure is described by \citet{snyder2017deep}. After the frame-level layers is a pooling layer that aggregates the frame-level outputs, calculating the mean and standard deviation of the whole segment. The last two layers before softmax operate on the segment level statistics. The x-vector is extracted from the penultimate layer (the 6th layer), with 512 dimensions. An LDA model reduces the x-vector dimensionality to 200.


      \paragraph*{Pretrained i-vector extractor trained on Yle and Parliament data}
        A pretrained i-vector extractor was used as a comparison to the models trained in this work. This extractor was trained on the Yle and Parliament speech data. This data set includes Finnish speech from Yle broadcast news and Finnish parliament sessions. In total, there are 782 different speakers and about 800k utterances.

      \paragraph*{i-vector extractor trained on the conversational Finnish data}

        Most of Kaldi recipes for training a HMM/DNN model include i-vectors as the speaker-adaptation method. In this thesis, the extractor for the baseline i-vectors is trained similarly to the Switchboard recipe. The extractor is trained on the same data as the acoustic model for extracting online i-vectors. The speed perturbation data augmentation is used also for the extractor training data. Both online and offline i-vectors are extracted and compared.

      \paragraph*{x-vector extractor trained on the conversational Finnish data}

        An x-vector extractor is trained also on the conversational data. This model uses the same setup and hyperparameters as the Voxceleb x-vector extractor. The data is augmented similarly to the Voxceleb model, increasing the number of utterances up to about 100k.

      \paragraph*{Domain adaptation by finetuning the pretrained extractors}

        Since the VoxCeleb extractor models are trained on English speech, it is possible that the extractors are suboptimal for Finnish. The extractors are trained on task of identifying speakers based on an utterance. There could be differences in the two languages in what kind of acoustic cues are utilised to identify the speaker. As with other modelling tasks, such as acoustic modelling, the extractor model could benefit from finetuning the large pretrained model on a smaller set of in-domain (or closer to the test domain) speech corpus. 

        The Voxceleb x-vector extractor was finetuned with the augmented Finnish speech corpus. The softmax layer was replaced so that the number of target classes (speakers) was compatible with the finetuning data. Apart from the output layer, all the other layers and their parameters were kept from the pretrained model, and the model was trained on the Finnish data for one epoch. This improves the evaluation set WER result a little (Table \ref{tab:spk_emb}), but this was not a statistically significant improvement.

        Replacing the last hidden layer of the extractor model was experimented on, but this did not improve the WER results. Adding a new layer after the 7th layer was also tried, but here too the results were worse than for the pretrained model.


      \paragraph*{Combining i-vectors and x-vectors}

        Since the method of modelling speaker variability is different in the two embedding types (see Section \ref{sec:adaptive_training}), it is possible that they encode different, not completely overlapping information.
        % One of the main differences between the methods is that the x-vector extractor has been explicitly trained to classify speakers, while the i-vector extractor 
        Therefore it might benefit to combine the two speaker embeddings. Simply concatenating the two vectors improved the WER results by a small margin, compared to using only x- or i-vectors. In Table \ref{tab:spk_emb} this method is called the i-x-ensemble vectors. This is a promising result, but the improvement is not statistically significant, based on the Wilcoxon Signed Rank test.
        
       
      \paragraph*{Results}

         % separate the results into different tables with clearer structure
        %   columns about data, offline/online, etc.
        % don't repeat terms

        Table \ref{tab:spk_emb} presents the results of the speaker embedding experiments.
        \begin{table}[ht!]
            \begin{tabularx}{\columnwidth}{l|l|ll}
                \multicolumn{1}{l}{\textbf{Method}} & \multicolumn{1}{l}{\textbf{Data set}} & \multicolumn{1}{l}{\textbf{devel}} & \textbf{eval}  \\   \toprule

                None && 25.1 & 27.3   \\ \midrule

                \multirow{3}{*}{i-vectors} & conv speech  & 23.9 & 26.5 \\ 
                % & conv speech (offline) & 24.0 & 26.5 \\ 
                & VoxCeleb pretrained & 23.9 & 25.5  \\ 
                & Yle, parliament pretrained & \textbf{23.3} & 25.8 \\\midrule

                \multirow{3}{*}{x-vectors} & conv speech & 24.3 & 26.3  \\ 
                & VoxCeleb pretrained & 23.7 & 25.5  \\
                &  VoxCeleb pretrained and conv speech finetuned & 24.2 & 25.3  \\ \midrule

                \multirow{2}{*}{i-x-ensemble} & conv speech & 24.0 & 25.8 \\ 
                & VoxCeleb  & 23.7 & \textbf{25.1}   \\ \bottomrule

            \end{tabularx}
            \caption{The WER results for the word-based system using different speaker embeddings concatenated with the input features. This is the 1st-pass, using a 4-gram language model.}
            \label{tab:spk_emb}
        \end{table}
        The fact that the pretrained speaker embedding extractors get better results than the embeddings trained on the conversational speech data suggest that the domain of the speech is not as important as the quantity of the speech. The domain means the language as well as the type of speech, since the Voxceleb speech is in English.


  \subsection{Language modelling experiments} \label{sec:lm_expts} 

    % This section describes the language modelling experiments and presents the results. The 

    \subsubsection{Text corpora}

              %     \textbf{LM training corpora} & \textbf{\# of word tokens} & 4-gram interpolation weight \\ \midrule
              % DSP conversational speech corpus transcriptions & 61k  & 0.41 \\ 
              % WEB corpus, conversational written text corpus & 75.9M & 0.59 \\ \bottomrule

      Two text corpora were used for training the language models: the DSPCON speech corpus transcriptions and WEBCON corpus collected from Internet forums. Table \ref{tab:lm_corpora} lists the sizes of the LM corpora, in total roughly 76M tokens.

      \begin{table}[htb]
        \begin{tabularx}{\columnwidth}{c|l}
          \toprule
          \textbf{LM training corpus} & \textbf{\# of word tokens} \\ \midrule
          DSPCON conversational speech corpus transcriptions & 61k  \\ 
          WEBCON corpus, conversational written text corpus & 75.9M \\ \bottomrule
        \end{tabularx}
        \caption{The two language modelling corpora. DSPCON is more similar to the development and test corpora than the WEBCON, but it is significantly smaller. The two coprpora are used to generate separate n-gram models, which are interpolated. The interpolation weights are optimised on the development set.}
        \label{tab:lm_corpora}
      \end{table}

      For the subword-based models, the vocabulary was segmented using the morfessor method. The hyperparameters for the morfessor model were adopted from the best model in \citep{enarvi2017automatic}. The vocabulary size decreased from 2.4M words to 42k words after segmentation.


    \subsubsection{Decoding with n-gram LM} \label{sec:ngram_expts} 

      The word-based systems use an n-gram LM with $n=4$ for the first-pass decoding and lattice generation. The SRILM \citep{stolcke2002srilm} is used to train  Kneser-Ney-discounted n-gram (see Section \ref{sec:ngram}) in the ARPA format. The two corpora are interpolated with weights that are optimised on the development data, similarly to \citep{enarvi2017automatic}.

      The subword-based systems use an n-gram LM with $n=4$ for the first-pass decoding and lattice generation. Different constant and variable values of $n$ were evaluated on the development data to find a suitable value. The VariKN \citep{siivola2007morfessor} tool was used to evaluate variable-order pruned n-grams. 

      \begin{table}[ht!]
        \center
        \begin{tabular}{l|l}
          \toprule
          \textbf{Language model} & \textbf{Devel set WER} \\ \midrule
          Constant-order trigram & 25.2 \\ 
          Constant-order 4-gram & \textbf{24.5} \\
          Constant-order 5-gram & 24.6 \\ 
          Variable-order n-gram & 24.9 \\ \bottomrule
        \end{tabular}
        \caption{Subword-based system 1st-pass results on the development data set. The AM uses the VoxCeleb x-i-ensemble vectors, making the results comparable with the last line of Table \ref{tab:spk_emb} which presented the word-based system results.}
        \label{tab:n-gram}
      \end{table}

          %                   ngram 1=42698
          %                   ngram 2=9032698
          % ngram 3=9626856
          % ngram 4=7101982
          % ngram 5=3601421

          % \multirow{2}{*}{\textbf{Order $n$ of n-grams}}
      \begin{table}[ht!]
        \center
          \begin{tabular}{cc|c|c|c|c}
            \toprule
             & \multicolumn{5}{c}{\textbf{\# of n-grams}}  \\ \cmidrule(lr){2-6}
             &\textbf{Word}& \multicolumn{4}{c}{\textbf{Subword}}  \\ \cmidrule(lr){2-6}
                      & \textbf{4-gram}    & \textbf{trigram} & \textbf{4-gram}    & \textbf{5-gram} & \textbf{VariKN} (38-gram) \\ \cmidrule(lr){2-6}
            \textbf{unigrams} & 2,427,251 & 42,698   & 42,698    & 42,698 & 42,699 \\ 
            \textbf{bigrams}  & 22,606,146 & 9,032,698  & 9,032,698 & 9,032,698 & 9,032,513\\
            \textbf{trigrams} & 4,762,607  & 10,288,565 & 9,626,856 & 9,626,856 & 7,591,231\\ 
            \textbf{4-grams} & 2,573,404 &           & 7,788,695 & 7,101,982 & 4,750,328\\ 
            \textbf{5-grams}&  &  &                             & 3,601,421 & 1,539,610\\ \midrule
            \textbf{total} & 32,369,408 & 10,288,565 & 26,490,947 & 29,405,655 & 24,505,381 \\
            \bottomrule
            % \toprule
            % \multirow{4}{*}{Subword vocab}&ngram 1 & 42698 \\ 
            % &ngram 2 & 9032698 \\ 
            % &ngram 3 & 9626856 \\ 
            % &ngram 4 & 7788695 \\ \bottomrule 
        \end{tabular}
        \caption{Number of n-grams per order for the n-gram language models.}
        \label{tab:n-gram-size}
      \end{table}


      % morfessor segment, varikn


    \subsubsection{Rescoring lattices with LSTM LM} \label{sec:lstm_expts} 

      The lattices generated in the first pass are rescored with an LSTM language model (2nd pass). 
      The LSTM language model was mostly the same as in \citep{enarvi2017automatic}. The projection layer has a dimension of 500, after which there is one LSTM layer followed by four highway layers, all with 1500 dimensions. There are dropout layers in between all of the layers, with a dropout rate of 0.2. 

      In order to weight the in-domain DSPCON corpus more, the WEBCON is subsampled with a factor of 0.2 at each epoch. That is, only a random subset of 20\% of the WEBCON corpus is used, resampled at each epoch. The vocabulary is reduced to the 100k most common words to reduce the  memory requirements.

      For the subword models, the cross-entropy objective is used. For the word models, the training objective is noise contrastive estimation (NCE) \citep{gutmann2010noise} to limit the memory requirements of the word models. This objective NCE samples random words and learns to classify words as training words or noise words. The higher the number of noise samples the slower but more stable the training. The number of noise samples is set to 500 per one training word. The noise words are the same for each batch of training words, and the batch size is set to 24. The noise dampening hyperparameter determines whether noise words are sampled uniformly (0) or according to the unigram distribution (1). This is set to 0.5. The Adagrad \citep{duchi2011adaptive} optimisation method is used with a learning rate of 0.1. The models were trained for a maximum of 25 epochs.
      % Noise-contrastive estimation (NCE) samples random words, but instead of optimizing the cross-entropy cost directly, it uses an auxiliary cost that learns to classify a word as a training word or a noise word [13]. This allows it to treat the normalization term as a parameter of the network. BlackOut

      The maximum length of the token sequence that is processed limits the memory of the LSTM network. It determines how far back the LM can learn dependencies. As the subword tokens are shorter, i.e., fewer characters long, they should probably have a longer context than word tokens. A few different sequence lengths were applied to see how this hyperparameter affects the second-pass WER results. The results are presented in Table \ref{tab:lstm_seq}. It was assumed that the word-based models would not benefit from decreasing the sequence length, which is why shorter than 25 sequences were not experimented with.

      \begin{table}[ht!]
        \center
          \begin{tabular}{c|c|c}
              \toprule
              Sequence length & Word LM & Subword LM \\ \midrule
              8 & 24.3 & - \\ 
              15 & \textbf{24.2} & - \\ 
              25 & 24.5 & 23.5 \\ 
              40 & 24.8 & \textbf{23.1} \\ \bottomrule
          \end{tabular}
          \caption{WER results on the evaluation set for word and subword based models with different sequence lengths.}
          \label{tab:lstm_seq}
      \end{table}

      In the decoding, three pruning methods were utilised. Firstly, the standard method of defining a beam width was used: if the logarithmic probability difference between the best token and another token was more than 650, the other token was pruned. Secondly, the maximum number of tokens per a lattice node was set to 62. Finally, if at some time step there are multiple tokens with the same previous words, only the best is ketp. This context length, or recombination order, is set to 22.
      % This latter pruning method highlights the difference between lattices and n-best list.

    \subsubsection{Rescoring $n$ best hypotheses with Transformer-XL LM} \label{sec:transformer_expts}

      The Transformer-Xl models rescored the n-best lists generated from the first or second pass. The use of n-best lists instead of lattices is due to its computational simplicity. n-best lists are suboptimal compared to lattices, but the rescoring of lattices with transformers is not as straight forward as with RNNs, since the state of a position depends not only on the previous state, but on all of the states of other positions, too. This has been done before (e.g., \citep{irie2019language}), but implementing it was beyond the scope of this thesis work.

      
      The Transformer-Xl model hyperparamters were set to the values that were found optimal by \citet{jain2020finnish}.
      % optimised for the development set to some degree. 
      The network embeds the input to 256 dimensions. The model layers have a total dimensionality of 400. The attention layers have 8 heads with 40 dimensions each and the feedforward layers have 1024 dimensions. The total number of layers is 32. 
      The number of tokens to predict is set to 70.  The optimiser is Adam with a learning rate of 0.0001. 

      % \textbf{todo: more details about hyperparams and training}

      \paragraph*{2nd pass results}

      When the n-best lists were generated from the n-gram LM decoding, the rescoring improved the development results about 17\%, from the first pass 29.0\% WER, listed in Table \ref{tab:transformer_n-best}. These numbers are not comparable to those of the Sections \ref{sec:ngram_expts} and \ref{sec:lstm_expts}, since a different acoustic model was used. The table shows the effect of scoring more hypotheses: 50, 200 or 1000.

      \begin{table}[ht!]
        \center
          \begin{tabular}{c|c}
              \toprule
              \textbf{\# of hypotheses} & \textbf{Devel WER} \\ \midrule
              50-best list & 26.2\\ 
              200-best list & 25.1 \\ 
              1000-best list & 24.2 \\ \bottomrule
          \end{tabular}
          \caption{The effect of a larger number of hypotheses on the development set WER results. The n-best lists were generated from a first-pass decoding with an n-gram, which achieved a WER of 29.0 in this experiment.}
          \label{tab:transformer_n-best}
      \end{table}

      Table \ref{tab:transformer_steps} lists the development set WER results with different amounts of training. 
      Iterating the training for more steps brings little improvement after 130k training steps, when rescoring the 50-best list.

      \begin{table}[ht!]
        \center
          \begin{tabular}{c|c}
              \toprule
              \textbf{\# of training steps} & \textbf{Devel WER} \\ \midrule
              130k & 26.2 \\ 
              540k &  26.0  \\ 
              800k &  26.0  \\ \bottomrule
          \end{tabular}
          \caption{The effect of further training on the development set WER results. The n-best lists were generated from a first-pass decoding with an n-gram, which achieved a WER of 29.0 in this experiment.}
          \label{tab:transformer_steps}
      \end{table}


      Table \ref{tab:50-best} presents the results from rescoring the 50-best list generated from n-gram decoding. LSTM LM achieves the best results.
      \begin{table}[ht!]
        \center
          \begin{tabular}{c|c}
              \toprule
              \textbf{LM} & \textbf{Devel WER} \\ \midrule
              n-gram & 29.0 \\
              LSTM &  \textbf{24.8}          \\ 
              LSTM + n-gram & 25.1 \\ 
              Transformer-xl &   26.0 \\ \bottomrule
          \end{tabular}
          \caption{Comparison of the LSTM and transformer models in the 50-best hypothesis rescoring.}
          \label{tab:50-best}
      \end{table}

      \paragraph*{3rd pass results}
      
      The three-pass rescoring scheme utilised all three LM model types. Lattices were generated from the first pass with an n-gram LM, and rescored with an LSTM LM after which an n-best list of best hypotheses  was generated.
      % The list was generated with n-gram lstm interpolated scores  
      The n-best list was rescored with the transformer model to determine the optimal transcriptions.

      % \begin{table}[ht!]
      %   \center
      %     \begin{tabular}{|l|l|}
      %         \hline
      %         LM scoring & Devel set WER \\ \hline
      %         1. n-gram & 29.0 \\ \hline
      %         2. LSTM+n-gram & 23.0  \\ \hline
      %         3. transformer & 22.6  \\ \hline
      %     \end{tabular}
      %     \caption{Three-pass LM scoring.}
      %     \label{tab:3pass}
      % \end{table}

  \newpage
  \subsection{Combined results}

    Table \ref{tab:combined} lists the best results from the acoustic and language modelling experiments. Rescoring the 50-best list generated by LSTM decoding with a Transformer model does not bring a statistically significant improvement with this acoustic model, although in some experiments the improvement was larger.s

    \begin{table}[ht!]
      \center
      \begin{tabular}{l|l|c|l|l}
      
      \multicolumn{3}{c}{}& \multicolumn{2}{c}{\textbf{WER (\%)}} \\

      \toprule

      \textbf{LM scoring} & \textbf{Vocab} & \textbf{Model details} &
      \textbf{devel} & \textbf{eval} \\ \midrule

      \multirow{2}{*}{n-gram scores} & word &
      \multirow{4}{*}{\citet{enarvi2017automatic}}
      & 29.8  & 31.7  \\ \cmidrule{2-2} \cmidrule{4-5} 

      & subword &  & 29.1 & 31.7 \\ \cmidrule{1-2} \cmidrule{4-5} 

      \multirow{2}{*}{NNLM-rescored lattices} & word & & 25.6
      & 27.9 \\ \cmidrule{2-2} \cmidrule{4-5} 

      & subword & & \textbf{25.0} & \textbf{27.1} \\ \bottomrule \toprule

      \multirow{2}{*}{n-gram scores} & word &
      \multirow{2}{*}{\shortstack{AM w/ i-x-vectors,\\4-gram LM}}
      & 23.7 & 25.1  \\ \cmidrule{2-2} \cmidrule{4-5} 

      & subword &  & 24.5 & 25.2  \\ \midrule

      \multirow{2}{*}{NNLM-rescored lattices}   & word
      & LSTM sequence 15 & \textbf{21.1} & 22.4 \\ \cmidrule{2-5} 

      & subword & LSTM sequence 40 & \textbf{21.1} & 22.3 \\ \midrule

      Transformer-rescored 50-best list  & subword  & Transformer-XL
      & \textbf{21.1}  & \textbf{22.2} \\

      \bottomrule

      \end{tabular}
    \caption{The results of the best models.}
    \label{tab:combined}
    \end{table}

    % According to the significance test, the improvement from LSTM rescoring to the transformer rescoring is

  \newpage
  \subsection{Comparison of model sizes and training time} 

    Table \ref{tab:model_size} lists the total number of parameters in the language models. Having a smaller vocabulary makes the input and output layers smaller for the subword-based models, decreasing the number of parameters. 

    \begin{table}[ht!]
        \center
          \begin{tabular}{l|l|l}
              \toprule
              \textbf{Vocab}& \textbf{Model} & \textbf{Number of parameters} \\ \midrule
              % & DNN AM & 17M \\  \midrule
              \multirow{2}{*}{Word} & LSTM LM & 230M \\
              & Transformer-XL & 56M  \\ \midrule
              \multirow{2}{*}{Subword} & LSTM LM  & 115M  \\
              & Transformer-XL & 41M  \\ \bottomrule
          \end{tabular}
          \caption{The number of learnable parameters in the neural network models after hyperparameter tuning.}
          \label{tab:model_size}
    \end{table}


\clearpage
\section{Discussion} \label{sec:discussion}
  
  The experiments described in the last chapter improved the WER results about 4.9 absolute percentages in total, compared to the previous best results. This chapter aims to analyse the experiments, 
  % clarify where the improvements come from,
  highlight possible limitations of the experiments, and propose possible future work on the subject.

    %   The conventional ASR system includes multiple independent components that each contribute to the accuracy of the transcriptions. Using multiple language models 

  \subsection{Acoustic modelling}
    The acoustic modelling data set is quite small, less than 37 hours in total. As a comparison, the Wall Street Journal speech data set is about 80 hours. The size of the speech data set generally correlates with the accuracy of the ASR system. With more data, a more complex neural acoustic model could be trained, which could learn a more accurate mapping from features to phoneme classes. The reason for the small size of the used data set is simply that this is all the available conversational Finnish speech data. However, it is not clear whether the training speech needs to correspond to the conversational evaluation set so closely. It is plausible that using a larger data set would yield better results even if the speech is more formal and therefore different from the evaluation set. Though more data generally yields better results, a small data set and a small model does have the benefit of learning quicker, thus enabling easier hyperparameter tuning and making it feasible to do more experiments.

    A large improvement on the recognition accuracy was achieved by utilising new Kaldi recipes for training the TDNN acoustic model. Kaldi recipes are tuned for a particular data sets. The recipe for training the TDNN used in this thesis was the Switchboard recipe. However, training the GMM AM was mostly done following the Wall Street Journal recipe. Experiments were also done training the TDNN with the WSJ recipe, but this yielded poorer results. The WSJ TDNN is a smaller model because the data set is  smaller. However, using the larger model seemed to be beneficial even though the data set used in this thesis is even smaller than the WSJ data set.  



  \subsection{Speaker embeddings}
    Although i-vectors and x-vectors achieve roughly equal results, the two methods differ in how they are trained or how they embed speakers or utterances in a vector space. i-vectors are meant to encode both channel and speaker variability, which is why they are also called the \emph{total factors}. They are often trained on a data set that has been artificially modified to include more speakers than the data actually includes by changing the speaker labels. More speakers helps the extractor to find the relevant differences across channels and speakers. An x-vector extractor, on the other hand, is trained to classify only speakers. The neural network can benefit from more data, unlike the i-vector GMMs, and the data is therefore augmented with noise for x-vector extractor training in the Kaldi recipes. 
    Because of the differences between the two methods, it is possible that they encode different types of information about the speakers or utterances. To assess whether the methods encode overlapping information, the two vectors were also concatenated and evaluated on the same ASR task. The concatenated i- and x-vectors achieved a small improvement compared to using only x-vectors (about 2\%). However, this improvement was not statistically significant.



  \subsection{Language modelling}
    
     

    \subsubsection{Transformer-XL}
      As noted in Sections \ref{sec:lattice} and \ref{sec:transformer_expts}, the use of n-best list instead of lattices when rescoring with the transformer-XL model limits the hypotheses more. Rescoring the lattices with transformers would be more efficient and probably improve the recognition accuracy, because lattices can generally encode more hypotheses. Furthermore, using multiple neural language models to first rescore lattices with an RNN LM and then rescore n-best lists with a Transformer LM is computationally about twice as costly as rescoring the lattices straight with a Transformer, assuming training the RNN and transformer LMs require equal resources. 
      It would be interesting to see the benefits of rescoring lattices with a transformer LM in future work.


    
    \subsubsection{Modelling conversational language}
      The language modelling methods used in this thesis are not specifically designed for conversational language, although the corpora are selected to correspond to conversational speech. Some conversation-specific methods for ASR have been proposed in recent years. \citet{xiong2018session} proposed a session-level modelling approach, in which the utterances are in kept in the context of the conversation and turns of the speakers are recognised. Because of limitations of the WEBCON corpus, this method was not tried out in this work. The WEBCON corpus does not include complete conversations but only sentences with no context. With a text corpus that includes complete conversations and labelled speaker turns this method could be implemented.

      Another interesting approach to language modelling is include in it some type of topic recognition. For example, \citet{watanabe2011topic} described a topic tracking language model that adapts the language model based on the previously processed speech. 
      This method is analogous to the speaker- and channel-aware acoustic modelling discussed in this thesis, which adapts the acoustic modelling features w.r.t. the speaker.
      Considering how humans recognise speech, topic-aware language model seems an intuitive method to improve the language models and in that way the speech recognition accuracy. However, this method requires labelled text corpus in which the topics are labelled. The WEBCON corpus includes no such labels, so topic-aware language modelling is left for future work.


\clearpage
\section{Conclusion} \label{sec:conclusion}

  % The research questions would be "to evaluate recent acoustic and language modelling methods for recognising conversational Finnish. In particular, to evaluate the effect of different speaker embedding approaches and evaluate transformer LMs compared to LSTM LMs with word and subword vocabularies"

  Spontaneous conversational Finnish is a challenging type of speech to recognise due to many factors, such as frequent dysfluencies in sentence structure and the use of various informal word forms.
  % The previous best results on the conversational Finnish corpus were presented by \citet{enarvi2017automatic}. 
  This thesis work was an effort to improve the ASR accuracy for conversational Finnish, specifically using the DSPCON corpus.
  The aim was to evaluate recent acoustic and language modelling methods on the data.
  The main experiments included evaluating the effect of different speaker embedding approaches and comparing Transformer-XL LMs and LSTM LMs with word and subword vocabularies.

  In the speaker embedding experiments, the use of i-vectors and x-vectors achieved similar results when trained on the same speech corpus. Both methods decreased the evaluation set WER around 3\% (relative improvement) when trained on the small Finnish conversational speech corpus, and roughly 7\% when trained on the significantly larger, English VoxCeleb corpus.
  % This demonstrates that a different language can be utilised for training  speaker embeddings that are useful for ASR.
  % Although i-vectors and x-vectors achieve roughly equal results, the two methods differ in how they are trained or how they embed speakers or utterances in a vector space. It is therefore possible that they encode different types of information about the speakers or utterances. To assess whether the methods encode overlapping information, the two vectors were also concatenated and evaluated on the same ASR task.
  The concatenated i- and x-vectors achieved a small improvement compared to using only x-vectors (about 2\% relative improvement), which is a promising but statistically not a significant improvement.
  Besides the speaker embedding experiments, the acoustic model was tuned for the development data.
  % Since the previous best results presented by \citet{enarvi2017automatic}, the Kaldi recipes have been optimised further.
  Using the new Kaldi recipes also improved the results.
  In total, the ASR accuracy improved 21\% (WER decreased from 31.7\% to 25.1\%), when comparing the results achieved by first-pass decoding in this work and in the work by \citet{enarvi2017automatic}. 

  The language modelling experiments included evaluating different language models with word and subword vocabularies. 
  In the lattice generation phase, evaluating different n-grams showed that a simple 4-gram LM achieved the best WER results for both word-based and subword-based systems. 
  % The LSTM language models were used to rescore the lattices generated using the n-gram language models.
  % The LSTM language models are similar to those in \citep{enarvi2017automatic}.
  % It was hypothesised that the  sequence length of the context ought to be longer for the subword-based systems, since the tokens are shorter. This was shown to be the case: increasing the sequence length from 25 to 40 improved the WER. Perhaps more surprisingly, the word-based systems benefited from decreasing the sequence length from 25 to 15.
  Transformer-XL models were used for rescoring n-best lists generated using n-gram or LSTM LMs. For the word-based system, rescoring the n-best lists with a transformer LM did not improve the results. For the subword-based system, rescoring n-best lists generated after the n-gram scoring did not improve the results, but rescoring LSTM-scored 50-best lists improved the best result by a small margin (0.5\% relative). However, this was not a statistically significant improvement.  The inefficacy of the transformer LMs might be due to the  use of n-best lists instead of lattices as well as the small size of the language modelling corpus. 
  
  Combining the best acoustic and language models built during this thesis work achieved a WER of 22.2\% on the conversational Finnish evaluation set, improving the WER by 4.9 absolute percentages (18\% relative improvement) compared to the previous best result.

% 

\clearpage
\small
% for counting pages
\thesisbibliography

\bibliographystyle{apalike}
\bibliography{references}


\end{document}